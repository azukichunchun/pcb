***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/ALVLM/vit_b16.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'True', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '-1', 'TRAINER.COOPAL.METHOD', 'entropy', 'TRAINER.COOPAL.GAMMA', '0.1']
output_dir: output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1
resume: 
root: /data2/yhiro/data
seed: 1
source_domains: None
target_domains: None
trainer: ALVLM
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: /data2/yhiro/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: True
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
    TRAIN_TYPE: freeze
  COOPAL:
    AEPATH: 
    ASPATH: 
    GAMMA: 0.1
    METHOD: entropy
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MAPLE:
    CTX_INIT: a photo of a
    N_CTX: 2
    PREC: fp16
    PROMPT_DEPTH: 9
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: ALVLM
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.2.2
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.1 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.8.19 (default, Mar 20 2024, 19:58:24)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-60-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 12.0.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA RTX A6000
GPU 1: NVIDIA RTX A6000
GPU 2: NVIDIA RTX A6000
GPU 3: NVIDIA RTX A6000

Nvidia driver version: 525.85.12
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.8.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          32
On-line CPU(s) list:             0-31
Vendor ID:                       GenuineIntel
Model name:                      Intel(R) Xeon(R) Silver 4108 CPU @ 1.80GHz
CPU family:                      6
Model:                           85
Thread(s) per core:              2
Core(s) per socket:              8
Socket(s):                       2
Stepping:                        4
CPU max MHz:                     3000.0000
CPU min MHz:                     800.0000
BogoMIPS:                        3600.00
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       512 KiB (16 instances)
L1i cache:                       512 KiB (16 instances)
L2 cache:                        16 MiB (16 instances)
L3 cache:                        22 MiB (2 instances)
NUMA node(s):                    2
NUMA node0 CPU(s):               0-7,16-23
NUMA node1 CPU(s):               8-15,24-31
Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled
Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Mitigation; IBRS
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable

Versions of relevant libraries:
[pip3] numpy==1.21.6
[pip3] torch==2.2.2
[pip3] torchaudio==2.2.2
[pip3] torchvision==0.17.2
[pip3] triton==2.2.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46344  
[conda] mkl-service               2.4.0            py38h5eee18b_1  
[conda] mkl_fft                   1.3.8            py38h5eee18b_0  
[conda] mkl_random                1.2.4            py38hdb19cb5_0  
[conda] numpy                     1.21.6           py38h5f9d8c6_0  
[conda] numpy-base                1.21.6           py38hb5e798b_0  
[conda] pytorch                   2.2.2           py3.8_cuda12.1_cudnn8.9.2_0    pytorch
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.2.2                py38_cu121    pytorch
[conda] torchtriton               2.2.0                      py38    pytorch
[conda] torchvision               0.17.2               py38_cu121    pytorch
        Pillow (10.2.0)

Loading trainer: ALVLM
Loading dataset: EuroSAT
Reading split from /data2/yhiro/data/eurosat/split_zhou_EuroSAT.json
Saving preprocessed few-shot data to /data2/yhiro/data/eurosat/split_fewshot/shot_-1-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  13,500
# val      5,400
# test     8,100
---------  -------
Loading evaluator: Classification
Loading dataset: EuroSAT
Reading split from /data2/yhiro/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data2/yhiro/data/eurosat/split_fewshot/shot_-1-seed_1.pkl
dataset length: 13500
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
INITIALIZE the prompts weights
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
['X X X X X X X X X X X X X X X X Annual Crop Land.', 'X X X X X X X X X X X X X X X X Forest.', 'X X X X X X X X X X X X X X X X Herbaceous Vegetation Land.', 'X X X X X X X X X X X X X X X X Highway or Road.', 'X X X X X X X X X X X X X X X X Industrial Buildings.', 'X X X X X X X X X X X X X X X X Pasture Land.', 'X X X X X X X X X X X X X X X X Permanent Crop Land.', 'X X X X X X X X X X X X X X X X Residential Buildings.', 'X X X X X X X X X X X X X X X X River.', 'X X X X X X X X X X X X X X X X Sea or Lake.']
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CustomCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
Turning off gradients in both the image and the text encoder
Multiple GPUs detected (n_gpus=2), use all of them!
DataParallel(
  (module): CustomCLIP(
    (prompt_learner): PromptLearner()
    (image_encoder): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
epoch [1/200] batch [1/1] time 6.886 (6.886) data 0.559 (0.559) loss 3.2480 (3.2480) acc 0.0000 (0.0000) lr 2.0000e-03 eta 0:22:50
epoch [2/200] batch [1/1] time 0.938 (0.938) data 0.771 (0.771) loss 3.1133 (3.1133) acc 0.0000 (0.0000) lr 1.9999e-03 eta 0:03:05
epoch [3/200] batch [1/1] time 0.768 (0.768) data 0.568 (0.568) loss 1.6201 (1.6201) acc 40.0000 (40.0000) lr 1.9995e-03 eta 0:02:31
epoch [4/200] batch [1/1] time 0.776 (0.776) data 0.571 (0.571) loss 1.6201 (1.6201) acc 50.0000 (50.0000) lr 1.9989e-03 eta 0:02:32
epoch [5/200] batch [1/1] time 0.862 (0.862) data 0.659 (0.659) loss 0.9512 (0.9512) acc 80.0000 (80.0000) lr 1.9980e-03 eta 0:02:48
epoch [6/200] batch [1/1] time 0.792 (0.792) data 0.586 (0.586) loss 0.9902 (0.9902) acc 70.0000 (70.0000) lr 1.9969e-03 eta 0:02:33
epoch [7/200] batch [1/1] time 0.759 (0.759) data 0.563 (0.563) loss 0.5137 (0.5137) acc 90.0000 (90.0000) lr 1.9956e-03 eta 0:02:26
epoch [8/200] batch [1/1] time 0.783 (0.783) data 0.583 (0.583) loss 0.5054 (0.5054) acc 90.0000 (90.0000) lr 1.9940e-03 eta 0:02:30
epoch [9/200] batch [1/1] time 0.787 (0.787) data 0.578 (0.578) loss 0.5518 (0.5518) acc 90.0000 (90.0000) lr 1.9921e-03 eta 0:02:30
epoch [10/200] batch [1/1] time 0.796 (0.796) data 0.594 (0.594) loss 0.6333 (0.6333) acc 80.0000 (80.0000) lr 1.9900e-03 eta 0:02:31
epoch [11/200] batch [1/1] time 0.744 (0.744) data 0.575 (0.575) loss 0.4768 (0.4768) acc 80.0000 (80.0000) lr 1.9877e-03 eta 0:02:20
epoch [12/200] batch [1/1] time 0.784 (0.784) data 0.583 (0.583) loss 0.4189 (0.4189) acc 90.0000 (90.0000) lr 1.9851e-03 eta 0:02:27
epoch [13/200] batch [1/1] time 0.764 (0.764) data 0.562 (0.562) loss 0.1697 (0.1697) acc 100.0000 (100.0000) lr 1.9823e-03 eta 0:02:22
epoch [14/200] batch [1/1] time 0.770 (0.770) data 0.566 (0.566) loss 0.1980 (0.1980) acc 90.0000 (90.0000) lr 1.9792e-03 eta 0:02:23
epoch [15/200] batch [1/1] time 0.744 (0.744) data 0.574 (0.574) loss 0.2413 (0.2413) acc 90.0000 (90.0000) lr 1.9759e-03 eta 0:02:17
epoch [16/200] batch [1/1] time 0.768 (0.768) data 0.566 (0.566) loss 0.3320 (0.3320) acc 90.0000 (90.0000) lr 1.9724e-03 eta 0:02:21
epoch [17/200] batch [1/1] time 0.781 (0.781) data 0.579 (0.579) loss 0.1366 (0.1366) acc 100.0000 (100.0000) lr 1.9686e-03 eta 0:02:22
epoch [18/200] batch [1/1] time 0.773 (0.773) data 0.575 (0.575) loss 0.4600 (0.4600) acc 90.0000 (90.0000) lr 1.9646e-03 eta 0:02:20
epoch [19/200] batch [1/1] time 0.813 (0.813) data 0.618 (0.618) loss 0.1559 (0.1559) acc 100.0000 (100.0000) lr 1.9603e-03 eta 0:02:27
epoch [20/200] batch [1/1] time 0.855 (0.855) data 0.663 (0.663) loss 0.2830 (0.2830) acc 90.0000 (90.0000) lr 1.9558e-03 eta 0:02:33
epoch [21/200] batch [1/1] time 0.780 (0.780) data 0.574 (0.574) loss 0.1465 (0.1465) acc 100.0000 (100.0000) lr 1.9511e-03 eta 0:02:19
epoch [22/200] batch [1/1] time 0.762 (0.762) data 0.575 (0.575) loss 0.5176 (0.5176) acc 90.0000 (90.0000) lr 1.9461e-03 eta 0:02:15
epoch [23/200] batch [1/1] time 0.789 (0.789) data 0.579 (0.579) loss 0.3838 (0.3838) acc 90.0000 (90.0000) lr 1.9409e-03 eta 0:02:19
epoch [24/200] batch [1/1] time 0.782 (0.782) data 0.579 (0.579) loss 0.1591 (0.1591) acc 90.0000 (90.0000) lr 1.9354e-03 eta 0:02:17
epoch [25/200] batch [1/1] time 0.752 (0.752) data 0.586 (0.586) loss 0.1049 (0.1049) acc 100.0000 (100.0000) lr 1.9298e-03 eta 0:02:11
epoch [26/200] batch [1/1] time 0.866 (0.866) data 0.679 (0.679) loss 0.0783 (0.0783) acc 100.0000 (100.0000) lr 1.9239e-03 eta 0:02:30
epoch [27/200] batch [1/1] time 0.846 (0.846) data 0.644 (0.644) loss 0.0872 (0.0872) acc 100.0000 (100.0000) lr 1.9178e-03 eta 0:02:26
epoch [28/200] batch [1/1] time 0.763 (0.763) data 0.581 (0.581) loss 0.2646 (0.2646) acc 90.0000 (90.0000) lr 1.9114e-03 eta 0:02:11
epoch [29/200] batch [1/1] time 0.767 (0.767) data 0.561 (0.561) loss 0.1022 (0.1022) acc 100.0000 (100.0000) lr 1.9048e-03 eta 0:02:11
epoch [30/200] batch [1/1] time 0.783 (0.783) data 0.580 (0.580) loss 0.4753 (0.4753) acc 80.0000 (80.0000) lr 1.8980e-03 eta 0:02:13
epoch [31/200] batch [1/1] time 0.772 (0.772) data 0.586 (0.586) loss 0.0576 (0.0576) acc 100.0000 (100.0000) lr 1.8910e-03 eta 0:02:10
epoch [32/200] batch [1/1] time 0.757 (0.757) data 0.573 (0.573) loss 0.0626 (0.0626) acc 100.0000 (100.0000) lr 1.8838e-03 eta 0:02:07
epoch [33/200] batch [1/1] time 0.982 (0.982) data 0.586 (0.586) loss 0.0533 (0.0533) acc 100.0000 (100.0000) lr 1.8763e-03 eta 0:02:44
epoch [34/200] batch [1/1] time 0.778 (0.778) data 0.578 (0.578) loss 0.0294 (0.0294) acc 100.0000 (100.0000) lr 1.8686e-03 eta 0:02:09
epoch [35/200] batch [1/1] time 0.769 (0.769) data 0.574 (0.574) loss 0.2305 (0.2305) acc 90.0000 (90.0000) lr 1.8607e-03 eta 0:02:06
epoch [36/200] batch [1/1] time 0.771 (0.771) data 0.580 (0.580) loss 0.1973 (0.1973) acc 90.0000 (90.0000) lr 1.8526e-03 eta 0:02:06
epoch [37/200] batch [1/1] time 0.784 (0.784) data 0.585 (0.585) loss 0.1226 (0.1226) acc 100.0000 (100.0000) lr 1.8443e-03 eta 0:02:07
epoch [38/200] batch [1/1] time 0.777 (0.777) data 0.571 (0.571) loss 0.0410 (0.0410) acc 100.0000 (100.0000) lr 1.8358e-03 eta 0:02:05
epoch [39/200] batch [1/1] time 0.773 (0.773) data 0.579 (0.579) loss 0.0718 (0.0718) acc 100.0000 (100.0000) lr 1.8271e-03 eta 0:02:04
epoch [40/200] batch [1/1] time 0.789 (0.789) data 0.581 (0.581) loss 0.0464 (0.0464) acc 100.0000 (100.0000) lr 1.8181e-03 eta 0:02:06
epoch [41/200] batch [1/1] time 0.749 (0.749) data 0.552 (0.552) loss 0.0459 (0.0459) acc 100.0000 (100.0000) lr 1.8090e-03 eta 0:01:59
epoch [42/200] batch [1/1] time 0.765 (0.765) data 0.581 (0.581) loss 0.0795 (0.0795) acc 100.0000 (100.0000) lr 1.7997e-03 eta 0:02:00
epoch [43/200] batch [1/1] time 0.783 (0.783) data 0.587 (0.587) loss 0.0798 (0.0798) acc 100.0000 (100.0000) lr 1.7902e-03 eta 0:02:02
epoch [44/200] batch [1/1] time 0.876 (0.876) data 0.693 (0.693) loss 0.1062 (0.1062) acc 100.0000 (100.0000) lr 1.7804e-03 eta 0:02:16
epoch [45/200] batch [1/1] time 0.763 (0.763) data 0.580 (0.580) loss 0.3708 (0.3708) acc 90.0000 (90.0000) lr 1.7705e-03 eta 0:01:58
epoch [46/200] batch [1/1] time 0.770 (0.770) data 0.589 (0.589) loss 0.1050 (0.1050) acc 100.0000 (100.0000) lr 1.7604e-03 eta 0:01:58
epoch [47/200] batch [1/1] time 0.801 (0.801) data 0.615 (0.615) loss 0.0461 (0.0461) acc 100.0000 (100.0000) lr 1.7501e-03 eta 0:02:02
epoch [48/200] batch [1/1] time 0.766 (0.766) data 0.564 (0.564) loss 0.0499 (0.0499) acc 100.0000 (100.0000) lr 1.7396e-03 eta 0:01:56
epoch [49/200] batch [1/1] time 0.762 (0.762) data 0.570 (0.570) loss 0.0640 (0.0640) acc 100.0000 (100.0000) lr 1.7290e-03 eta 0:01:55
epoch [50/200] batch [1/1] time 0.781 (0.781) data 0.580 (0.580) loss 0.2285 (0.2285) acc 90.0000 (90.0000) lr 1.7181e-03 eta 0:01:57
epoch [51/200] batch [1/1] time 0.754 (0.754) data 0.566 (0.566) loss 0.0373 (0.0373) acc 100.0000 (100.0000) lr 1.7071e-03 eta 0:01:52
epoch [52/200] batch [1/1] time 0.782 (0.782) data 0.583 (0.583) loss 0.1298 (0.1298) acc 100.0000 (100.0000) lr 1.6959e-03 eta 0:01:55
epoch [53/200] batch [1/1] time 0.764 (0.764) data 0.581 (0.581) loss 0.1646 (0.1646) acc 100.0000 (100.0000) lr 1.6845e-03 eta 0:01:52
epoch [54/200] batch [1/1] time 0.784 (0.784) data 0.577 (0.577) loss 0.0734 (0.0734) acc 100.0000 (100.0000) lr 1.6730e-03 eta 0:01:54
epoch [55/200] batch [1/1] time 0.765 (0.765) data 0.579 (0.579) loss 0.0398 (0.0398) acc 100.0000 (100.0000) lr 1.6613e-03 eta 0:01:50
epoch [56/200] batch [1/1] time 0.793 (0.793) data 0.591 (0.591) loss 0.0642 (0.0642) acc 100.0000 (100.0000) lr 1.6494e-03 eta 0:01:54
epoch [57/200] batch [1/1] time 0.895 (0.895) data 0.717 (0.717) loss 0.2512 (0.2512) acc 90.0000 (90.0000) lr 1.6374e-03 eta 0:02:07
epoch [58/200] batch [1/1] time 0.781 (0.781) data 0.576 (0.576) loss 0.0430 (0.0430) acc 100.0000 (100.0000) lr 1.6252e-03 eta 0:01:50
epoch [59/200] batch [1/1] time 0.760 (0.760) data 0.556 (0.556) loss 0.0065 (0.0065) acc 100.0000 (100.0000) lr 1.6129e-03 eta 0:01:47
epoch [60/200] batch [1/1] time 0.791 (0.791) data 0.587 (0.587) loss 0.0285 (0.0285) acc 100.0000 (100.0000) lr 1.6004e-03 eta 0:01:50
epoch [61/200] batch [1/1] time 0.772 (0.772) data 0.579 (0.579) loss 0.0497 (0.0497) acc 100.0000 (100.0000) lr 1.5878e-03 eta 0:01:47
epoch [62/200] batch [1/1] time 0.756 (0.756) data 0.576 (0.576) loss 0.0716 (0.0716) acc 100.0000 (100.0000) lr 1.5750e-03 eta 0:01:44
epoch [63/200] batch [1/1] time 0.849 (0.849) data 0.662 (0.662) loss 0.0103 (0.0103) acc 100.0000 (100.0000) lr 1.5621e-03 eta 0:01:56
epoch [64/200] batch [1/1] time 0.764 (0.764) data 0.581 (0.581) loss 0.0124 (0.0124) acc 100.0000 (100.0000) lr 1.5490e-03 eta 0:01:43
epoch [65/200] batch [1/1] time 0.773 (0.773) data 0.581 (0.581) loss 0.0085 (0.0085) acc 100.0000 (100.0000) lr 1.5358e-03 eta 0:01:44
epoch [66/200] batch [1/1] time 0.771 (0.771) data 0.571 (0.571) loss 0.0312 (0.0312) acc 100.0000 (100.0000) lr 1.5225e-03 eta 0:01:43
epoch [67/200] batch [1/1] time 0.772 (0.772) data 0.572 (0.572) loss 0.0676 (0.0676) acc 100.0000 (100.0000) lr 1.5090e-03 eta 0:01:42
epoch [68/200] batch [1/1] time 0.872 (0.872) data 0.672 (0.672) loss 0.0482 (0.0482) acc 100.0000 (100.0000) lr 1.4955e-03 eta 0:01:55
epoch [69/200] batch [1/1] time 0.743 (0.743) data 0.561 (0.561) loss 0.0203 (0.0203) acc 100.0000 (100.0000) lr 1.4818e-03 eta 0:01:37
epoch [70/200] batch [1/1] time 0.712 (0.712) data 0.545 (0.545) loss 0.2369 (0.2369) acc 90.0000 (90.0000) lr 1.4679e-03 eta 0:01:32
epoch [71/200] batch [1/1] time 0.762 (0.762) data 0.573 (0.573) loss 0.0510 (0.0510) acc 100.0000 (100.0000) lr 1.4540e-03 eta 0:01:38
epoch [72/200] batch [1/1] time 0.771 (0.771) data 0.562 (0.562) loss 0.0277 (0.0277) acc 100.0000 (100.0000) lr 1.4399e-03 eta 0:01:38
epoch [73/200] batch [1/1] time 0.769 (0.769) data 0.583 (0.583) loss 0.0081 (0.0081) acc 100.0000 (100.0000) lr 1.4258e-03 eta 0:01:37
epoch [74/200] batch [1/1] time 0.762 (0.762) data 0.571 (0.571) loss 0.0409 (0.0409) acc 100.0000 (100.0000) lr 1.4115e-03 eta 0:01:36
epoch [75/200] batch [1/1] time 0.873 (0.873) data 0.681 (0.681) loss 0.0865 (0.0865) acc 100.0000 (100.0000) lr 1.3971e-03 eta 0:01:49
epoch [76/200] batch [1/1] time 0.822 (0.822) data 0.630 (0.630) loss 0.0170 (0.0170) acc 100.0000 (100.0000) lr 1.3827e-03 eta 0:01:41
epoch [77/200] batch [1/1] time 0.905 (0.905) data 0.721 (0.721) loss 0.0158 (0.0158) acc 100.0000 (100.0000) lr 1.3681e-03 eta 0:01:51
epoch [78/200] batch [1/1] time 0.846 (0.846) data 0.647 (0.647) loss 0.0776 (0.0776) acc 100.0000 (100.0000) lr 1.3535e-03 eta 0:01:43
epoch [79/200] batch [1/1] time 0.775 (0.775) data 0.574 (0.574) loss 0.0047 (0.0047) acc 100.0000 (100.0000) lr 1.3387e-03 eta 0:01:33
epoch [80/200] batch [1/1] time 0.781 (0.781) data 0.575 (0.575) loss 0.0124 (0.0124) acc 100.0000 (100.0000) lr 1.3239e-03 eta 0:01:33
epoch [81/200] batch [1/1] time 0.792 (0.792) data 0.588 (0.588) loss 0.1646 (0.1646) acc 90.0000 (90.0000) lr 1.3090e-03 eta 0:01:34
epoch [82/200] batch [1/1] time 0.780 (0.780) data 0.601 (0.601) loss 0.0103 (0.0103) acc 100.0000 (100.0000) lr 1.2940e-03 eta 0:01:31
epoch [83/200] batch [1/1] time 0.779 (0.779) data 0.579 (0.579) loss 0.0328 (0.0328) acc 100.0000 (100.0000) lr 1.2790e-03 eta 0:01:31
epoch [84/200] batch [1/1] time 0.741 (0.741) data 0.547 (0.547) loss 0.0379 (0.0379) acc 100.0000 (100.0000) lr 1.2639e-03 eta 0:01:25
epoch [85/200] batch [1/1] time 0.907 (0.907) data 0.705 (0.705) loss 0.0422 (0.0422) acc 100.0000 (100.0000) lr 1.2487e-03 eta 0:01:44
epoch [86/200] batch [1/1] time 0.772 (0.772) data 0.584 (0.584) loss 0.0101 (0.0101) acc 100.0000 (100.0000) lr 1.2334e-03 eta 0:01:28
epoch [87/200] batch [1/1] time 0.881 (0.881) data 0.679 (0.679) loss 0.1191 (0.1191) acc 100.0000 (100.0000) lr 1.2181e-03 eta 0:01:39
epoch [88/200] batch [1/1] time 0.858 (0.858) data 0.668 (0.668) loss 0.0233 (0.0233) acc 100.0000 (100.0000) lr 1.2028e-03 eta 0:01:36
epoch [89/200] batch [1/1] time 0.773 (0.773) data 0.612 (0.612) loss 0.0197 (0.0197) acc 100.0000 (100.0000) lr 1.1874e-03 eta 0:01:25
epoch [90/200] batch [1/1] time 0.773 (0.773) data 0.576 (0.576) loss 0.0449 (0.0449) acc 100.0000 (100.0000) lr 1.1719e-03 eta 0:01:24
epoch [91/200] batch [1/1] time 0.867 (0.867) data 0.660 (0.660) loss 0.0894 (0.0894) acc 100.0000 (100.0000) lr 1.1564e-03 eta 0:01:34
epoch [92/200] batch [1/1] time 0.818 (0.818) data 0.632 (0.632) loss 0.0110 (0.0110) acc 100.0000 (100.0000) lr 1.1409e-03 eta 0:01:28
epoch [93/200] batch [1/1] time 0.788 (0.788) data 0.578 (0.578) loss 0.0185 (0.0185) acc 100.0000 (100.0000) lr 1.1253e-03 eta 0:01:24
epoch [94/200] batch [1/1] time 0.775 (0.775) data 0.614 (0.614) loss 0.0104 (0.0104) acc 100.0000 (100.0000) lr 1.1097e-03 eta 0:01:22
epoch [95/200] batch [1/1] time 0.778 (0.778) data 0.578 (0.578) loss 0.0080 (0.0080) acc 100.0000 (100.0000) lr 1.0941e-03 eta 0:01:21
epoch [96/200] batch [1/1] time 0.788 (0.788) data 0.580 (0.580) loss 0.0441 (0.0441) acc 100.0000 (100.0000) lr 1.0785e-03 eta 0:01:21
epoch [97/200] batch [1/1] time 0.769 (0.769) data 0.569 (0.569) loss 0.0442 (0.0442) acc 100.0000 (100.0000) lr 1.0628e-03 eta 0:01:19
epoch [98/200] batch [1/1] time 0.760 (0.760) data 0.578 (0.578) loss 0.0991 (0.0991) acc 90.0000 (90.0000) lr 1.0471e-03 eta 0:01:17
epoch [99/200] batch [1/1] time 0.767 (0.767) data 0.579 (0.579) loss 0.1566 (0.1566) acc 90.0000 (90.0000) lr 1.0314e-03 eta 0:01:17
epoch [100/200] batch [1/1] time 0.756 (0.756) data 0.573 (0.573) loss 0.0123 (0.0123) acc 100.0000 (100.0000) lr 1.0157e-03 eta 0:01:15
epoch [101/200] batch [1/1] time 0.769 (0.769) data 0.587 (0.587) loss 0.0555 (0.0555) acc 100.0000 (100.0000) lr 1.0000e-03 eta 0:01:16
epoch [102/200] batch [1/1] time 0.765 (0.765) data 0.575 (0.575) loss 0.0144 (0.0144) acc 100.0000 (100.0000) lr 9.8429e-04 eta 0:01:15
epoch [103/200] batch [1/1] time 0.804 (0.804) data 0.618 (0.618) loss 0.0342 (0.0342) acc 100.0000 (100.0000) lr 9.6859e-04 eta 0:01:17
epoch [104/200] batch [1/1] time 0.856 (0.856) data 0.693 (0.693) loss 0.0994 (0.0994) acc 100.0000 (100.0000) lr 9.5289e-04 eta 0:01:22
epoch [105/200] batch [1/1] time 0.757 (0.757) data 0.575 (0.575) loss 0.0204 (0.0204) acc 100.0000 (100.0000) lr 9.3721e-04 eta 0:01:11
epoch [106/200] batch [1/1] time 0.778 (0.778) data 0.572 (0.572) loss 0.0297 (0.0297) acc 100.0000 (100.0000) lr 9.2154e-04 eta 0:01:13
epoch [107/200] batch [1/1] time 0.855 (0.855) data 0.661 (0.661) loss 0.0718 (0.0718) acc 100.0000 (100.0000) lr 9.0589e-04 eta 0:01:19
epoch [108/200] batch [1/1] time 0.801 (0.801) data 0.597 (0.597) loss 0.0906 (0.0906) acc 100.0000 (100.0000) lr 8.9027e-04 eta 0:01:13
epoch [109/200] batch [1/1] time 0.896 (0.896) data 0.720 (0.720) loss 0.0096 (0.0096) acc 100.0000 (100.0000) lr 8.7467e-04 eta 0:01:21
epoch [110/200] batch [1/1] time 0.751 (0.751) data 0.590 (0.590) loss 0.0140 (0.0140) acc 100.0000 (100.0000) lr 8.5910e-04 eta 0:01:07
epoch [111/200] batch [1/1] time 0.772 (0.772) data 0.575 (0.575) loss 0.0063 (0.0063) acc 100.0000 (100.0000) lr 8.4357e-04 eta 0:01:08
epoch [112/200] batch [1/1] time 0.737 (0.737) data 0.574 (0.574) loss 0.0169 (0.0169) acc 100.0000 (100.0000) lr 8.2807e-04 eta 0:01:04
epoch [113/200] batch [1/1] time 0.764 (0.764) data 0.581 (0.581) loss 0.0124 (0.0124) acc 100.0000 (100.0000) lr 8.1262e-04 eta 0:01:06
epoch [114/200] batch [1/1] time 0.779 (0.779) data 0.581 (0.581) loss 0.0070 (0.0070) acc 100.0000 (100.0000) lr 7.9721e-04 eta 0:01:06
epoch [115/200] batch [1/1] time 0.786 (0.786) data 0.591 (0.591) loss 0.0374 (0.0374) acc 100.0000 (100.0000) lr 7.8186e-04 eta 0:01:06
epoch [116/200] batch [1/1] time 0.786 (0.786) data 0.578 (0.578) loss 0.0034 (0.0034) acc 100.0000 (100.0000) lr 7.6655e-04 eta 0:01:05
epoch [117/200] batch [1/1] time 0.763 (0.763) data 0.573 (0.573) loss 0.0111 (0.0111) acc 100.0000 (100.0000) lr 7.5131e-04 eta 0:01:03
epoch [118/200] batch [1/1] time 0.798 (0.798) data 0.615 (0.615) loss 0.0205 (0.0205) acc 100.0000 (100.0000) lr 7.3613e-04 eta 0:01:05
epoch [119/200] batch [1/1] time 0.798 (0.798) data 0.598 (0.598) loss 0.0120 (0.0120) acc 100.0000 (100.0000) lr 7.2101e-04 eta 0:01:04
epoch [120/200] batch [1/1] time 0.786 (0.786) data 0.574 (0.574) loss 0.0143 (0.0143) acc 100.0000 (100.0000) lr 7.0596e-04 eta 0:01:02
epoch [121/200] batch [1/1] time 0.775 (0.775) data 0.575 (0.575) loss 0.0305 (0.0305) acc 100.0000 (100.0000) lr 6.9098e-04 eta 0:01:01
epoch [122/200] batch [1/1] time 0.789 (0.789) data 0.584 (0.584) loss 0.0424 (0.0424) acc 100.0000 (100.0000) lr 6.7608e-04 eta 0:01:01
epoch [123/200] batch [1/1] time 0.935 (0.935) data 0.729 (0.729) loss 0.0531 (0.0531) acc 100.0000 (100.0000) lr 6.6126e-04 eta 0:01:12
epoch [124/200] batch [1/1] time 0.778 (0.778) data 0.588 (0.588) loss 0.0198 (0.0198) acc 100.0000 (100.0000) lr 6.4653e-04 eta 0:00:59
epoch [125/200] batch [1/1] time 0.760 (0.760) data 0.581 (0.581) loss 0.0148 (0.0148) acc 100.0000 (100.0000) lr 6.3188e-04 eta 0:00:57
epoch [126/200] batch [1/1] time 0.764 (0.764) data 0.575 (0.575) loss 0.0493 (0.0493) acc 100.0000 (100.0000) lr 6.1732e-04 eta 0:00:56
epoch [127/200] batch [1/1] time 0.779 (0.779) data 0.578 (0.578) loss 0.0377 (0.0377) acc 100.0000 (100.0000) lr 6.0285e-04 eta 0:00:56
epoch [128/200] batch [1/1] time 0.775 (0.775) data 0.577 (0.577) loss 0.0039 (0.0039) acc 100.0000 (100.0000) lr 5.8849e-04 eta 0:00:55
epoch [129/200] batch [1/1] time 0.770 (0.770) data 0.579 (0.579) loss 0.0179 (0.0179) acc 100.0000 (100.0000) lr 5.7422e-04 eta 0:00:54
epoch [130/200] batch [1/1] time 0.779 (0.779) data 0.576 (0.576) loss 0.0453 (0.0453) acc 100.0000 (100.0000) lr 5.6006e-04 eta 0:00:54
epoch [131/200] batch [1/1] time 0.773 (0.773) data 0.569 (0.569) loss 0.0073 (0.0073) acc 100.0000 (100.0000) lr 5.4601e-04 eta 0:00:53
epoch [132/200] batch [1/1] time 0.745 (0.745) data 0.580 (0.580) loss 0.0060 (0.0060) acc 100.0000 (100.0000) lr 5.3207e-04 eta 0:00:50
epoch [133/200] batch [1/1] time 0.776 (0.776) data 0.583 (0.583) loss 0.0740 (0.0740) acc 100.0000 (100.0000) lr 5.1825e-04 eta 0:00:52
epoch [134/200] batch [1/1] time 0.778 (0.778) data 0.571 (0.571) loss 0.0191 (0.0191) acc 100.0000 (100.0000) lr 5.0454e-04 eta 0:00:51
epoch [135/200] batch [1/1] time 0.760 (0.760) data 0.572 (0.572) loss 0.0084 (0.0084) acc 100.0000 (100.0000) lr 4.9096e-04 eta 0:00:49
epoch [136/200] batch [1/1] time 0.774 (0.774) data 0.574 (0.574) loss 0.0044 (0.0044) acc 100.0000 (100.0000) lr 4.7750e-04 eta 0:00:49
epoch [137/200] batch [1/1] time 0.786 (0.786) data 0.579 (0.579) loss 0.0116 (0.0116) acc 100.0000 (100.0000) lr 4.6417e-04 eta 0:00:49
epoch [138/200] batch [1/1] time 0.774 (0.774) data 0.586 (0.586) loss 0.0104 (0.0104) acc 100.0000 (100.0000) lr 4.5098e-04 eta 0:00:47
epoch [139/200] batch [1/1] time 0.756 (0.756) data 0.552 (0.552) loss 0.0163 (0.0163) acc 100.0000 (100.0000) lr 4.3792e-04 eta 0:00:46
epoch [140/200] batch [1/1] time 0.746 (0.746) data 0.563 (0.563) loss 0.0072 (0.0072) acc 100.0000 (100.0000) lr 4.2499e-04 eta 0:00:44
epoch [141/200] batch [1/1] time 0.749 (0.749) data 0.566 (0.566) loss 0.0514 (0.0514) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:00:44
epoch [142/200] batch [1/1] time 0.786 (0.786) data 0.578 (0.578) loss 0.0347 (0.0347) acc 100.0000 (100.0000) lr 3.9958e-04 eta 0:00:45
epoch [143/200] batch [1/1] time 0.761 (0.761) data 0.577 (0.577) loss 0.1097 (0.1097) acc 100.0000 (100.0000) lr 3.8709e-04 eta 0:00:43
epoch [144/200] batch [1/1] time 0.753 (0.753) data 0.567 (0.567) loss 0.0254 (0.0254) acc 100.0000 (100.0000) lr 3.7476e-04 eta 0:00:42
epoch [145/200] batch [1/1] time 0.773 (0.773) data 0.572 (0.572) loss 0.0342 (0.0342) acc 100.0000 (100.0000) lr 3.6258e-04 eta 0:00:42
epoch [146/200] batch [1/1] time 0.777 (0.777) data 0.580 (0.580) loss 0.0164 (0.0164) acc 100.0000 (100.0000) lr 3.5055e-04 eta 0:00:41
epoch [147/200] batch [1/1] time 0.788 (0.788) data 0.578 (0.578) loss 0.0264 (0.0264) acc 100.0000 (100.0000) lr 3.3869e-04 eta 0:00:41
epoch [148/200] batch [1/1] time 0.782 (0.782) data 0.580 (0.580) loss 0.0390 (0.0390) acc 100.0000 (100.0000) lr 3.2699e-04 eta 0:00:40
epoch [149/200] batch [1/1] time 0.775 (0.775) data 0.575 (0.575) loss 0.1661 (0.1661) acc 90.0000 (90.0000) lr 3.1545e-04 eta 0:00:39
epoch [150/200] batch [1/1] time 0.859 (0.859) data 0.658 (0.658) loss 0.0086 (0.0086) acc 100.0000 (100.0000) lr 3.0409e-04 eta 0:00:42
epoch [151/200] batch [1/1] time 0.753 (0.753) data 0.571 (0.571) loss 0.0228 (0.0228) acc 100.0000 (100.0000) lr 2.9289e-04 eta 0:00:36
epoch [152/200] batch [1/1] time 0.778 (0.778) data 0.577 (0.577) loss 0.0242 (0.0242) acc 100.0000 (100.0000) lr 2.8187e-04 eta 0:00:37
epoch [153/200] batch [1/1] time 0.769 (0.769) data 0.583 (0.583) loss 0.0069 (0.0069) acc 100.0000 (100.0000) lr 2.7103e-04 eta 0:00:36
epoch [154/200] batch [1/1] time 0.776 (0.776) data 0.572 (0.572) loss 0.1046 (0.1046) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:00:35
epoch [155/200] batch [1/1] time 0.771 (0.771) data 0.580 (0.580) loss 0.0088 (0.0088) acc 100.0000 (100.0000) lr 2.4989e-04 eta 0:00:34
epoch [156/200] batch [1/1] time 0.773 (0.773) data 0.573 (0.573) loss 0.0038 (0.0038) acc 100.0000 (100.0000) lr 2.3959e-04 eta 0:00:34
epoch [157/200] batch [1/1] time 0.772 (0.772) data 0.568 (0.568) loss 0.0057 (0.0057) acc 100.0000 (100.0000) lr 2.2949e-04 eta 0:00:33
epoch [158/200] batch [1/1] time 0.788 (0.788) data 0.583 (0.583) loss 0.0184 (0.0184) acc 100.0000 (100.0000) lr 2.1957e-04 eta 0:00:33
epoch [159/200] batch [1/1] time 0.786 (0.786) data 0.582 (0.582) loss 0.0043 (0.0043) acc 100.0000 (100.0000) lr 2.0984e-04 eta 0:00:32
epoch [160/200] batch [1/1] time 0.792 (0.792) data 0.581 (0.581) loss 0.0161 (0.0161) acc 100.0000 (100.0000) lr 2.0032e-04 eta 0:00:31
epoch [161/200] batch [1/1] time 0.773 (0.773) data 0.578 (0.578) loss 0.1338 (0.1338) acc 90.0000 (90.0000) lr 1.9098e-04 eta 0:00:30
epoch [162/200] batch [1/1] time 1.082 (1.082) data 0.740 (0.740) loss 0.0256 (0.0256) acc 100.0000 (100.0000) lr 1.8185e-04 eta 0:00:41
epoch [163/200] batch [1/1] time 0.772 (0.772) data 0.582 (0.582) loss 0.0056 (0.0056) acc 100.0000 (100.0000) lr 1.7292e-04 eta 0:00:28
epoch [164/200] batch [1/1] time 0.780 (0.780) data 0.577 (0.577) loss 0.0115 (0.0115) acc 100.0000 (100.0000) lr 1.6419e-04 eta 0:00:28
epoch [165/200] batch [1/1] time 0.804 (0.804) data 0.602 (0.602) loss 0.0364 (0.0364) acc 100.0000 (100.0000) lr 1.5567e-04 eta 0:00:28
epoch [166/200] batch [1/1] time 0.832 (0.832) data 0.634 (0.634) loss 0.0064 (0.0064) acc 100.0000 (100.0000) lr 1.4736e-04 eta 0:00:28
epoch [167/200] batch [1/1] time 0.786 (0.786) data 0.578 (0.578) loss 0.0059 (0.0059) acc 100.0000 (100.0000) lr 1.3926e-04 eta 0:00:25
epoch [168/200] batch [1/1] time 0.777 (0.777) data 0.573 (0.573) loss 0.0114 (0.0114) acc 100.0000 (100.0000) lr 1.3137e-04 eta 0:00:24
epoch [169/200] batch [1/1] time 0.794 (0.794) data 0.606 (0.606) loss 0.0450 (0.0450) acc 100.0000 (100.0000) lr 1.2369e-04 eta 0:00:24
epoch [170/200] batch [1/1] time 0.739 (0.739) data 0.573 (0.573) loss 0.0388 (0.0388) acc 100.0000 (100.0000) lr 1.1623e-04 eta 0:00:22
epoch [171/200] batch [1/1] time 0.906 (0.906) data 0.701 (0.701) loss 0.0450 (0.0450) acc 100.0000 (100.0000) lr 1.0899e-04 eta 0:00:26
epoch [172/200] batch [1/1] time 0.757 (0.757) data 0.571 (0.571) loss 0.0070 (0.0070) acc 100.0000 (100.0000) lr 1.0197e-04 eta 0:00:21
epoch [173/200] batch [1/1] time 0.781 (0.781) data 0.574 (0.574) loss 0.0184 (0.0184) acc 100.0000 (100.0000) lr 9.5173e-05 eta 0:00:21
epoch [174/200] batch [1/1] time 0.764 (0.764) data 0.584 (0.584) loss 0.0527 (0.0527) acc 100.0000 (100.0000) lr 8.8597e-05 eta 0:00:19
epoch [175/200] batch [1/1] time 0.764 (0.764) data 0.582 (0.582) loss 0.0083 (0.0083) acc 100.0000 (100.0000) lr 8.2245e-05 eta 0:00:19
epoch [176/200] batch [1/1] time 0.787 (0.787) data 0.585 (0.585) loss 0.0148 (0.0148) acc 100.0000 (100.0000) lr 7.6120e-05 eta 0:00:18
epoch [177/200] batch [1/1] time 0.793 (0.793) data 0.592 (0.592) loss 0.0483 (0.0483) acc 100.0000 (100.0000) lr 7.0224e-05 eta 0:00:18
epoch [178/200] batch [1/1] time 0.766 (0.766) data 0.585 (0.585) loss 0.0264 (0.0264) acc 100.0000 (100.0000) lr 6.4556e-05 eta 0:00:16
epoch [179/200] batch [1/1] time 0.768 (0.768) data 0.570 (0.570) loss 0.0129 (0.0129) acc 100.0000 (100.0000) lr 5.9119e-05 eta 0:00:16
epoch [180/200] batch [1/1] time 0.745 (0.745) data 0.578 (0.578) loss 0.0034 (0.0034) acc 100.0000 (100.0000) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [1/1] time 0.782 (0.782) data 0.592 (0.592) loss 0.0224 (0.0224) acc 100.0000 (100.0000) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [1/1] time 0.777 (0.777) data 0.601 (0.601) loss 0.0090 (0.0090) acc 100.0000 (100.0000) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [1/1] time 0.819 (0.819) data 0.619 (0.619) loss 0.0580 (0.0580) acc 100.0000 (100.0000) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [1/1] time 0.776 (0.776) data 0.569 (0.569) loss 0.0107 (0.0107) acc 100.0000 (100.0000) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [1/1] time 0.785 (0.785) data 0.615 (0.615) loss 0.0324 (0.0324) acc 100.0000 (100.0000) lr 3.1417e-05 eta 0:00:11
epoch [186/200] batch [1/1] time 0.849 (0.849) data 0.679 (0.679) loss 0.0092 (0.0092) acc 100.0000 (100.0000) lr 2.7630e-05 eta 0:00:11
epoch [187/200] batch [1/1] time 0.858 (0.858) data 0.697 (0.697) loss 0.0168 (0.0168) acc 100.0000 (100.0000) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [1/1] time 0.805 (0.805) data 0.607 (0.607) loss 0.0108 (0.0108) acc 100.0000 (100.0000) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [1/1] time 0.791 (0.791) data 0.589 (0.589) loss 0.0486 (0.0486) acc 100.0000 (100.0000) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [1/1] time 0.774 (0.774) data 0.582 (0.582) loss 0.0082 (0.0082) acc 100.0000 (100.0000) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [1/1] time 0.728 (0.728) data 0.568 (0.568) loss 0.0568 (0.0568) acc 100.0000 (100.0000) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [1/1] time 0.764 (0.764) data 0.574 (0.574) loss 0.0592 (0.0592) acc 100.0000 (100.0000) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [1/1] time 0.774 (0.774) data 0.577 (0.577) loss 0.0046 (0.0046) acc 100.0000 (100.0000) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [1/1] time 0.779 (0.779) data 0.578 (0.578) loss 0.0023 (0.0023) acc 100.0000 (100.0000) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [1/1] time 0.828 (0.828) data 0.617 (0.617) loss 0.0244 (0.0244) acc 100.0000 (100.0000) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [1/1] time 0.841 (0.841) data 0.652 (0.652) loss 0.0160 (0.0160) acc 100.0000 (100.0000) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [1/1] time 0.983 (0.983) data 0.778 (0.778) loss 0.0094 (0.0094) acc 100.0000 (100.0000) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [1/1] time 0.895 (0.895) data 0.704 (0.704) loss 0.0033 (0.0033) acc 100.0000 (100.0000) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [1/1] time 0.750 (0.750) data 0.561 (0.561) loss 0.0290 (0.0290) acc 100.0000 (100.0000) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [1/1] time 0.765 (0.765) data 0.575 (0.575) loss 0.0352 (0.0352) acc 100.0000 (100.0000) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,100
* correct: 3,863
* accuracy: 47.7%
* error: 52.3%
* macro_f1: 38.7%
training time for 0-th round: 219.24 seconds
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
| Calculating uncertainty of Unlabeled set
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
INITIALIZE the prompts weights
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
['X X X X X X X X X X X X X X X X Annual Crop Land.', 'X X X X X X X X X X X X X X X X Forest.', 'X X X X X X X X X X X X X X X X Herbaceous Vegetation Land.', 'X X X X X X X X X X X X X X X X Highway or Road.', 'X X X X X X X X X X X X X X X X Industrial Buildings.', 'X X X X X X X X X X X X X X X X Pasture Land.', 'X X X X X X X X X X X X X X X X Permanent Crop Land.', 'X X X X X X X X X X X X X X X X Residential Buildings.', 'X X X X X X X X X X X X X X X X River.', 'X X X X X X X X X X X X X X X X Sea or Lake.']
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CustomCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
Turning off gradients in both the image and the text encoder
Multiple GPUs detected (n_gpus=2), use all of them!
DataParallel(
  (module): CustomCLIP(
    (prompt_learner): PromptLearner()
    (image_encoder): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
epoch [1/200] batch [1/1] time 0.877 (0.877) data 0.686 (0.686) loss 2.8223 (2.8223) acc 15.0000 (15.0000) lr 2.0000e-03 eta 0:02:54
epoch [2/200] batch [1/1] time 1.395 (1.395) data 1.002 (1.002) loss 2.8418 (2.8418) acc 10.0000 (10.0000) lr 1.9999e-03 eta 0:04:36
epoch [3/200] batch [1/1] time 1.002 (1.002) data 0.816 (0.816) loss 2.1289 (2.1289) acc 20.0000 (20.0000) lr 1.9995e-03 eta 0:03:17
epoch [4/200] batch [1/1] time 0.881 (0.881) data 0.690 (0.690) loss 4.0625 (4.0625) acc 30.0000 (30.0000) lr 1.9989e-03 eta 0:02:52
epoch [5/200] batch [1/1] time 0.875 (0.875) data 0.679 (0.679) loss 2.7520 (2.7520) acc 55.0000 (55.0000) lr 1.9980e-03 eta 0:02:50
epoch [6/200] batch [1/1] time 0.879 (0.879) data 0.676 (0.676) loss 2.1836 (2.1836) acc 55.0000 (55.0000) lr 1.9969e-03 eta 0:02:50
epoch [7/200] batch [1/1] time 0.893 (0.893) data 0.687 (0.687) loss 1.5498 (1.5498) acc 40.0000 (40.0000) lr 1.9956e-03 eta 0:02:52
epoch [8/200] batch [1/1] time 0.874 (0.874) data 0.693 (0.693) loss 2.8223 (2.8223) acc 65.0000 (65.0000) lr 1.9940e-03 eta 0:02:47
epoch [9/200] batch [1/1] time 0.879 (0.879) data 0.692 (0.692) loss 1.4756 (1.4756) acc 60.0000 (60.0000) lr 1.9921e-03 eta 0:02:47
epoch [10/200] batch [1/1] time 0.844 (0.844) data 0.676 (0.676) loss 1.1582 (1.1582) acc 65.0000 (65.0000) lr 1.9900e-03 eta 0:02:40
epoch [11/200] batch [1/1] time 0.891 (0.891) data 0.697 (0.697) loss 0.9614 (0.9614) acc 70.0000 (70.0000) lr 1.9877e-03 eta 0:02:48
epoch [12/200] batch [1/1] time 0.874 (0.874) data 0.689 (0.689) loss 1.0098 (1.0098) acc 70.0000 (70.0000) lr 1.9851e-03 eta 0:02:44
epoch [13/200] batch [1/1] time 0.904 (0.904) data 0.701 (0.701) loss 1.1943 (1.1943) acc 40.0000 (40.0000) lr 1.9823e-03 eta 0:02:48
epoch [14/200] batch [1/1] time 0.884 (0.884) data 0.679 (0.679) loss 1.1738 (1.1738) acc 70.0000 (70.0000) lr 1.9792e-03 eta 0:02:44
epoch [15/200] batch [1/1] time 0.898 (0.898) data 0.701 (0.701) loss 0.7490 (0.7490) acc 80.0000 (80.0000) lr 1.9759e-03 eta 0:02:46
epoch [16/200] batch [1/1] time 0.951 (0.951) data 0.750 (0.750) loss 0.5698 (0.5698) acc 85.0000 (85.0000) lr 1.9724e-03 eta 0:02:54
epoch [17/200] batch [1/1] time 0.903 (0.903) data 0.698 (0.698) loss 0.8716 (0.8716) acc 75.0000 (75.0000) lr 1.9686e-03 eta 0:02:45
epoch [18/200] batch [1/1] time 0.885 (0.885) data 0.680 (0.680) loss 0.4463 (0.4463) acc 90.0000 (90.0000) lr 1.9646e-03 eta 0:02:41
epoch [19/200] batch [1/1] time 0.927 (0.927) data 0.724 (0.724) loss 0.6201 (0.6201) acc 80.0000 (80.0000) lr 1.9603e-03 eta 0:02:47
epoch [20/200] batch [1/1] time 0.960 (0.960) data 0.779 (0.779) loss 0.5928 (0.5928) acc 75.0000 (75.0000) lr 1.9558e-03 eta 0:02:52
epoch [21/200] batch [1/1] time 0.936 (0.936) data 0.719 (0.719) loss 0.3892 (0.3892) acc 85.0000 (85.0000) lr 1.9511e-03 eta 0:02:47
epoch [22/200] batch [1/1] time 0.901 (0.901) data 0.693 (0.693) loss 0.3689 (0.3689) acc 95.0000 (95.0000) lr 1.9461e-03 eta 0:02:40
epoch [23/200] batch [1/1] time 0.896 (0.896) data 0.690 (0.690) loss 0.7739 (0.7739) acc 80.0000 (80.0000) lr 1.9409e-03 eta 0:02:38
epoch [24/200] batch [1/1] time 0.910 (0.910) data 0.704 (0.704) loss 0.7090 (0.7090) acc 75.0000 (75.0000) lr 1.9354e-03 eta 0:02:40
epoch [25/200] batch [1/1] time 0.881 (0.881) data 0.680 (0.680) loss 0.3740 (0.3740) acc 90.0000 (90.0000) lr 1.9298e-03 eta 0:02:34
epoch [26/200] batch [1/1] time 0.897 (0.897) data 0.694 (0.694) loss 0.3325 (0.3325) acc 95.0000 (95.0000) lr 1.9239e-03 eta 0:02:36
epoch [27/200] batch [1/1] time 0.899 (0.899) data 0.696 (0.696) loss 0.3162 (0.3162) acc 85.0000 (85.0000) lr 1.9178e-03 eta 0:02:35
epoch [28/200] batch [1/1] time 0.895 (0.895) data 0.689 (0.689) loss 0.2808 (0.2808) acc 95.0000 (95.0000) lr 1.9114e-03 eta 0:02:33
epoch [29/200] batch [1/1] time 0.901 (0.901) data 0.728 (0.728) loss 0.2935 (0.2935) acc 100.0000 (100.0000) lr 1.9048e-03 eta 0:02:34
epoch [30/200] batch [1/1] time 0.890 (0.890) data 0.687 (0.687) loss 0.1573 (0.1573) acc 100.0000 (100.0000) lr 1.8980e-03 eta 0:02:31
epoch [31/200] batch [1/1] time 0.886 (0.886) data 0.684 (0.684) loss 0.2810 (0.2810) acc 95.0000 (95.0000) lr 1.8910e-03 eta 0:02:29
epoch [32/200] batch [1/1] time 0.971 (0.971) data 0.759 (0.759) loss 0.4485 (0.4485) acc 85.0000 (85.0000) lr 1.8838e-03 eta 0:02:43
epoch [33/200] batch [1/1] time 0.925 (0.925) data 0.717 (0.717) loss 0.2494 (0.2494) acc 90.0000 (90.0000) lr 1.8763e-03 eta 0:02:34
epoch [34/200] batch [1/1] time 0.853 (0.853) data 0.691 (0.691) loss 0.2449 (0.2449) acc 95.0000 (95.0000) lr 1.8686e-03 eta 0:02:21
epoch [35/200] batch [1/1] time 0.879 (0.879) data 0.681 (0.681) loss 0.1065 (0.1065) acc 100.0000 (100.0000) lr 1.8607e-03 eta 0:02:25
epoch [36/200] batch [1/1] time 0.896 (0.896) data 0.695 (0.695) loss 0.4810 (0.4810) acc 85.0000 (85.0000) lr 1.8526e-03 eta 0:02:26
epoch [37/200] batch [1/1] time 0.979 (0.979) data 0.775 (0.775) loss 0.1505 (0.1505) acc 100.0000 (100.0000) lr 1.8443e-03 eta 0:02:39
epoch [38/200] batch [1/1] time 0.920 (0.920) data 0.711 (0.711) loss 0.2448 (0.2448) acc 90.0000 (90.0000) lr 1.8358e-03 eta 0:02:29
epoch [39/200] batch [1/1] time 0.892 (0.892) data 0.698 (0.698) loss 0.1826 (0.1826) acc 95.0000 (95.0000) lr 1.8271e-03 eta 0:02:23
epoch [40/200] batch [1/1] time 0.879 (0.879) data 0.686 (0.686) loss 0.3735 (0.3735) acc 85.0000 (85.0000) lr 1.8181e-03 eta 0:02:20
epoch [41/200] batch [1/1] time 0.971 (0.971) data 0.775 (0.775) loss 0.2166 (0.2166) acc 95.0000 (95.0000) lr 1.8090e-03 eta 0:02:34
epoch [42/200] batch [1/1] time 0.931 (0.931) data 0.736 (0.736) loss 0.1200 (0.1200) acc 100.0000 (100.0000) lr 1.7997e-03 eta 0:02:27
epoch [43/200] batch [1/1] time 0.892 (0.892) data 0.689 (0.689) loss 0.2365 (0.2365) acc 95.0000 (95.0000) lr 1.7902e-03 eta 0:02:19
epoch [44/200] batch [1/1] time 0.882 (0.882) data 0.691 (0.691) loss 0.4373 (0.4373) acc 90.0000 (90.0000) lr 1.7804e-03 eta 0:02:17
epoch [45/200] batch [1/1] time 0.901 (0.901) data 0.696 (0.696) loss 0.2357 (0.2357) acc 95.0000 (95.0000) lr 1.7705e-03 eta 0:02:19
epoch [46/200] batch [1/1] time 0.897 (0.897) data 0.691 (0.691) loss 0.2676 (0.2676) acc 95.0000 (95.0000) lr 1.7604e-03 eta 0:02:18
epoch [47/200] batch [1/1] time 0.887 (0.887) data 0.695 (0.695) loss 0.3547 (0.3547) acc 85.0000 (85.0000) lr 1.7501e-03 eta 0:02:15
epoch [48/200] batch [1/1] time 0.877 (0.877) data 0.688 (0.688) loss 0.1331 (0.1331) acc 100.0000 (100.0000) lr 1.7396e-03 eta 0:02:13
epoch [49/200] batch [1/1] time 0.890 (0.890) data 0.695 (0.695) loss 0.1738 (0.1738) acc 95.0000 (95.0000) lr 1.7290e-03 eta 0:02:14
epoch [50/200] batch [1/1] time 0.903 (0.903) data 0.697 (0.697) loss 0.4402 (0.4402) acc 80.0000 (80.0000) lr 1.7181e-03 eta 0:02:15
epoch [51/200] batch [1/1] time 0.859 (0.859) data 0.656 (0.656) loss 0.2141 (0.2141) acc 90.0000 (90.0000) lr 1.7071e-03 eta 0:02:07
epoch [52/200] batch [1/1] time 0.909 (0.909) data 0.716 (0.716) loss 0.2220 (0.2220) acc 90.0000 (90.0000) lr 1.6959e-03 eta 0:02:14
epoch [53/200] batch [1/1] time 0.890 (0.890) data 0.689 (0.689) loss 0.3730 (0.3730) acc 90.0000 (90.0000) lr 1.6845e-03 eta 0:02:10
epoch [54/200] batch [1/1] time 0.970 (0.970) data 0.782 (0.782) loss 0.1266 (0.1266) acc 95.0000 (95.0000) lr 1.6730e-03 eta 0:02:21
epoch [55/200] batch [1/1] time 0.900 (0.900) data 0.695 (0.695) loss 0.0880 (0.0880) acc 95.0000 (95.0000) lr 1.6613e-03 eta 0:02:10
epoch [56/200] batch [1/1] time 0.878 (0.878) data 0.696 (0.696) loss 0.1155 (0.1155) acc 100.0000 (100.0000) lr 1.6494e-03 eta 0:02:06
epoch [57/200] batch [1/1] time 0.872 (0.872) data 0.691 (0.691) loss 0.3899 (0.3899) acc 90.0000 (90.0000) lr 1.6374e-03 eta 0:02:04
epoch [58/200] batch [1/1] time 0.932 (0.932) data 0.759 (0.759) loss 0.1716 (0.1716) acc 95.0000 (95.0000) lr 1.6252e-03 eta 0:02:12
epoch [59/200] batch [1/1] time 0.869 (0.869) data 0.686 (0.686) loss 0.1517 (0.1517) acc 95.0000 (95.0000) lr 1.6129e-03 eta 0:02:02
epoch [60/200] batch [1/1] time 0.905 (0.905) data 0.711 (0.711) loss 0.1487 (0.1487) acc 95.0000 (95.0000) lr 1.6004e-03 eta 0:02:06
epoch [61/200] batch [1/1] time 0.883 (0.883) data 0.689 (0.689) loss 0.0720 (0.0720) acc 100.0000 (100.0000) lr 1.5878e-03 eta 0:02:02
epoch [62/200] batch [1/1] time 0.888 (0.888) data 0.694 (0.694) loss 0.1943 (0.1943) acc 90.0000 (90.0000) lr 1.5750e-03 eta 0:02:02
epoch [63/200] batch [1/1] time 0.858 (0.858) data 0.679 (0.679) loss 0.4355 (0.4355) acc 85.0000 (85.0000) lr 1.5621e-03 eta 0:01:57
epoch [64/200] batch [1/1] time 0.859 (0.859) data 0.691 (0.691) loss 0.1384 (0.1384) acc 100.0000 (100.0000) lr 1.5490e-03 eta 0:01:56
epoch [65/200] batch [1/1] time 0.886 (0.886) data 0.686 (0.686) loss 0.1290 (0.1290) acc 100.0000 (100.0000) lr 1.5358e-03 eta 0:01:59
epoch [66/200] batch [1/1] time 0.859 (0.859) data 0.688 (0.688) loss 0.1296 (0.1296) acc 100.0000 (100.0000) lr 1.5225e-03 eta 0:01:55
epoch [67/200] batch [1/1] time 0.882 (0.882) data 0.693 (0.693) loss 0.2438 (0.2438) acc 95.0000 (95.0000) lr 1.5090e-03 eta 0:01:57
epoch [68/200] batch [1/1] time 0.882 (0.882) data 0.677 (0.677) loss 0.1005 (0.1005) acc 100.0000 (100.0000) lr 1.4955e-03 eta 0:01:56
epoch [69/200] batch [1/1] time 1.052 (1.052) data 0.849 (0.849) loss 0.1431 (0.1431) acc 95.0000 (95.0000) lr 1.4818e-03 eta 0:02:17
epoch [70/200] batch [1/1] time 0.872 (0.872) data 0.687 (0.687) loss 0.0925 (0.0925) acc 100.0000 (100.0000) lr 1.4679e-03 eta 0:01:53
epoch [71/200] batch [1/1] time 0.852 (0.852) data 0.660 (0.660) loss 0.1794 (0.1794) acc 95.0000 (95.0000) lr 1.4540e-03 eta 0:01:49
epoch [72/200] batch [1/1] time 0.854 (0.854) data 0.689 (0.689) loss 0.1372 (0.1372) acc 100.0000 (100.0000) lr 1.4399e-03 eta 0:01:49
epoch [73/200] batch [1/1] time 0.872 (0.872) data 0.684 (0.684) loss 0.0762 (0.0762) acc 100.0000 (100.0000) lr 1.4258e-03 eta 0:01:50
epoch [74/200] batch [1/1] time 0.898 (0.898) data 0.695 (0.695) loss 0.3513 (0.3513) acc 95.0000 (95.0000) lr 1.4115e-03 eta 0:01:53
epoch [75/200] batch [1/1] time 0.876 (0.876) data 0.692 (0.692) loss 0.1050 (0.1050) acc 95.0000 (95.0000) lr 1.3971e-03 eta 0:01:49
epoch [76/200] batch [1/1] time 0.875 (0.875) data 0.681 (0.681) loss 0.3962 (0.3962) acc 90.0000 (90.0000) lr 1.3827e-03 eta 0:01:48
epoch [77/200] batch [1/1] time 0.893 (0.893) data 0.689 (0.689) loss 0.0985 (0.0985) acc 100.0000 (100.0000) lr 1.3681e-03 eta 0:01:49
epoch [78/200] batch [1/1] time 0.886 (0.886) data 0.680 (0.680) loss 0.0886 (0.0886) acc 100.0000 (100.0000) lr 1.3535e-03 eta 0:01:48
epoch [79/200] batch [1/1] time 0.867 (0.867) data 0.666 (0.666) loss 0.1104 (0.1104) acc 100.0000 (100.0000) lr 1.3387e-03 eta 0:01:44
epoch [80/200] batch [1/1] time 0.877 (0.877) data 0.661 (0.661) loss 0.1115 (0.1115) acc 100.0000 (100.0000) lr 1.3239e-03 eta 0:01:45
epoch [81/200] batch [1/1] time 0.969 (0.969) data 0.773 (0.773) loss 0.1304 (0.1304) acc 100.0000 (100.0000) lr 1.3090e-03 eta 0:01:55
epoch [82/200] batch [1/1] time 0.881 (0.881) data 0.685 (0.685) loss 0.3000 (0.3000) acc 90.0000 (90.0000) lr 1.2940e-03 eta 0:01:43
epoch [83/200] batch [1/1] time 0.897 (0.897) data 0.686 (0.686) loss 0.0834 (0.0834) acc 100.0000 (100.0000) lr 1.2790e-03 eta 0:01:44
epoch [84/200] batch [1/1] time 0.886 (0.886) data 0.691 (0.691) loss 0.2295 (0.2295) acc 90.0000 (90.0000) lr 1.2639e-03 eta 0:01:42
epoch [85/200] batch [1/1] time 0.882 (0.882) data 0.689 (0.689) loss 0.2267 (0.2267) acc 90.0000 (90.0000) lr 1.2487e-03 eta 0:01:41
epoch [86/200] batch [1/1] time 0.877 (0.877) data 0.684 (0.684) loss 0.0613 (0.0613) acc 100.0000 (100.0000) lr 1.2334e-03 eta 0:01:40
epoch [87/200] batch [1/1] time 0.882 (0.882) data 0.690 (0.690) loss 0.0375 (0.0375) acc 100.0000 (100.0000) lr 1.2181e-03 eta 0:01:39
epoch [88/200] batch [1/1] time 0.890 (0.890) data 0.682 (0.682) loss 0.0959 (0.0959) acc 100.0000 (100.0000) lr 1.2028e-03 eta 0:01:39
epoch [89/200] batch [1/1] time 0.877 (0.877) data 0.696 (0.696) loss 0.2462 (0.2462) acc 90.0000 (90.0000) lr 1.1874e-03 eta 0:01:37
epoch [90/200] batch [1/1] time 0.970 (0.970) data 0.781 (0.781) loss 0.1816 (0.1816) acc 95.0000 (95.0000) lr 1.1719e-03 eta 0:01:46
epoch [91/200] batch [1/1] time 0.881 (0.881) data 0.687 (0.687) loss 0.0834 (0.0834) acc 100.0000 (100.0000) lr 1.1564e-03 eta 0:01:36
epoch [92/200] batch [1/1] time 0.849 (0.849) data 0.661 (0.661) loss 0.2218 (0.2218) acc 95.0000 (95.0000) lr 1.1409e-03 eta 0:01:31
epoch [93/200] batch [1/1] time 0.872 (0.872) data 0.687 (0.687) loss 0.0902 (0.0902) acc 100.0000 (100.0000) lr 1.1253e-03 eta 0:01:33
epoch [94/200] batch [1/1] time 0.935 (0.935) data 0.732 (0.732) loss 0.0854 (0.0854) acc 100.0000 (100.0000) lr 1.1097e-03 eta 0:01:39
epoch [95/200] batch [1/1] time 0.905 (0.905) data 0.700 (0.700) loss 0.2019 (0.2019) acc 90.0000 (90.0000) lr 1.0941e-03 eta 0:01:35
epoch [96/200] batch [1/1] time 0.885 (0.885) data 0.685 (0.685) loss 0.1661 (0.1661) acc 90.0000 (90.0000) lr 1.0785e-03 eta 0:01:32
epoch [97/200] batch [1/1] time 0.877 (0.877) data 0.691 (0.691) loss 0.1114 (0.1114) acc 100.0000 (100.0000) lr 1.0628e-03 eta 0:01:30
epoch [98/200] batch [1/1] time 0.897 (0.897) data 0.685 (0.685) loss 0.2256 (0.2256) acc 95.0000 (95.0000) lr 1.0471e-03 eta 0:01:31
epoch [99/200] batch [1/1] time 0.979 (0.979) data 0.778 (0.778) loss 0.1277 (0.1277) acc 95.0000 (95.0000) lr 1.0314e-03 eta 0:01:38
epoch [100/200] batch [1/1] time 0.898 (0.898) data 0.696 (0.696) loss 0.1423 (0.1423) acc 100.0000 (100.0000) lr 1.0157e-03 eta 0:01:29
epoch [101/200] batch [1/1] time 0.900 (0.900) data 0.689 (0.689) loss 0.0933 (0.0933) acc 100.0000 (100.0000) lr 1.0000e-03 eta 0:01:29
epoch [102/200] batch [1/1] time 0.888 (0.888) data 0.699 (0.699) loss 0.1962 (0.1962) acc 95.0000 (95.0000) lr 9.8429e-04 eta 0:01:27
epoch [103/200] batch [1/1] time 0.894 (0.894) data 0.698 (0.698) loss 0.0758 (0.0758) acc 100.0000 (100.0000) lr 9.6859e-04 eta 0:01:26
epoch [104/200] batch [1/1] time 0.981 (0.981) data 0.793 (0.793) loss 0.0792 (0.0792) acc 100.0000 (100.0000) lr 9.5289e-04 eta 0:01:34
epoch [105/200] batch [1/1] time 0.889 (0.889) data 0.693 (0.693) loss 0.2593 (0.2593) acc 90.0000 (90.0000) lr 9.3721e-04 eta 0:01:24
epoch [106/200] batch [1/1] time 0.874 (0.874) data 0.684 (0.684) loss 0.1272 (0.1272) acc 95.0000 (95.0000) lr 9.2154e-04 eta 0:01:22
epoch [107/200] batch [1/1] time 0.966 (0.966) data 0.759 (0.759) loss 0.1566 (0.1566) acc 95.0000 (95.0000) lr 9.0589e-04 eta 0:01:29
epoch [108/200] batch [1/1] time 0.879 (0.879) data 0.692 (0.692) loss 0.2927 (0.2927) acc 90.0000 (90.0000) lr 8.9027e-04 eta 0:01:20
epoch [109/200] batch [1/1] time 0.893 (0.893) data 0.686 (0.686) loss 0.1195 (0.1195) acc 100.0000 (100.0000) lr 8.7467e-04 eta 0:01:21
epoch [110/200] batch [1/1] time 0.901 (0.901) data 0.689 (0.689) loss 0.0857 (0.0857) acc 100.0000 (100.0000) lr 8.5910e-04 eta 0:01:21
epoch [111/200] batch [1/1] time 0.891 (0.891) data 0.700 (0.700) loss 0.0649 (0.0649) acc 100.0000 (100.0000) lr 8.4357e-04 eta 0:01:19
epoch [112/200] batch [1/1] time 0.881 (0.881) data 0.682 (0.682) loss 0.0903 (0.0903) acc 100.0000 (100.0000) lr 8.2807e-04 eta 0:01:17
epoch [113/200] batch [1/1] time 0.888 (0.888) data 0.685 (0.685) loss 0.1410 (0.1410) acc 95.0000 (95.0000) lr 8.1262e-04 eta 0:01:17
epoch [114/200] batch [1/1] time 0.877 (0.877) data 0.673 (0.673) loss 0.0574 (0.0574) acc 100.0000 (100.0000) lr 7.9721e-04 eta 0:01:15
epoch [115/200] batch [1/1] time 0.876 (0.876) data 0.684 (0.684) loss 0.1304 (0.1304) acc 95.0000 (95.0000) lr 7.8186e-04 eta 0:01:14
epoch [116/200] batch [1/1] time 0.887 (0.887) data 0.700 (0.700) loss 0.1566 (0.1566) acc 95.0000 (95.0000) lr 7.6655e-04 eta 0:01:14
epoch [117/200] batch [1/1] time 0.880 (0.880) data 0.686 (0.686) loss 0.1119 (0.1119) acc 95.0000 (95.0000) lr 7.5131e-04 eta 0:01:13
epoch [118/200] batch [1/1] time 1.208 (1.208) data 1.005 (1.005) loss 0.1499 (0.1499) acc 95.0000 (95.0000) lr 7.3613e-04 eta 0:01:39
epoch [119/200] batch [1/1] time 1.108 (1.108) data 0.899 (0.899) loss 0.0312 (0.0312) acc 100.0000 (100.0000) lr 7.2101e-04 eta 0:01:29
epoch [120/200] batch [1/1] time 0.881 (0.881) data 0.690 (0.690) loss 0.1644 (0.1644) acc 95.0000 (95.0000) lr 7.0596e-04 eta 0:01:10
epoch [121/200] batch [1/1] time 0.885 (0.885) data 0.683 (0.683) loss 0.0630 (0.0630) acc 100.0000 (100.0000) lr 6.9098e-04 eta 0:01:09
epoch [122/200] batch [1/1] time 0.890 (0.890) data 0.679 (0.679) loss 0.0625 (0.0625) acc 100.0000 (100.0000) lr 6.7608e-04 eta 0:01:09
epoch [123/200] batch [1/1] time 0.895 (0.895) data 0.686 (0.686) loss 0.1117 (0.1117) acc 95.0000 (95.0000) lr 6.6126e-04 eta 0:01:08
epoch [124/200] batch [1/1] time 0.873 (0.873) data 0.662 (0.662) loss 0.2170 (0.2170) acc 95.0000 (95.0000) lr 6.4653e-04 eta 0:01:06
epoch [125/200] batch [1/1] time 0.960 (0.960) data 0.764 (0.764) loss 0.0854 (0.0854) acc 100.0000 (100.0000) lr 6.3188e-04 eta 0:01:11
epoch [126/200] batch [1/1] time 0.916 (0.916) data 0.752 (0.752) loss 0.1886 (0.1886) acc 90.0000 (90.0000) lr 6.1732e-04 eta 0:01:07
epoch [127/200] batch [1/1] time 0.904 (0.904) data 0.695 (0.695) loss 0.0421 (0.0421) acc 100.0000 (100.0000) lr 6.0285e-04 eta 0:01:05
epoch [128/200] batch [1/1] time 0.895 (0.895) data 0.695 (0.695) loss 0.0443 (0.0443) acc 100.0000 (100.0000) lr 5.8849e-04 eta 0:01:04
epoch [129/200] batch [1/1] time 0.968 (0.968) data 0.776 (0.776) loss 0.2253 (0.2253) acc 90.0000 (90.0000) lr 5.7422e-04 eta 0:01:08
epoch [130/200] batch [1/1] time 1.040 (1.040) data 0.834 (0.834) loss 0.1243 (0.1243) acc 95.0000 (95.0000) lr 5.6006e-04 eta 0:01:12
epoch [131/200] batch [1/1] time 0.852 (0.852) data 0.691 (0.691) loss 0.0684 (0.0684) acc 100.0000 (100.0000) lr 5.4601e-04 eta 0:00:58
epoch [132/200] batch [1/1] time 0.878 (0.878) data 0.709 (0.709) loss 0.1186 (0.1186) acc 100.0000 (100.0000) lr 5.3207e-04 eta 0:00:59
epoch [133/200] batch [1/1] time 0.905 (0.905) data 0.700 (0.700) loss 0.0567 (0.0567) acc 100.0000 (100.0000) lr 5.1825e-04 eta 0:01:00
epoch [134/200] batch [1/1] time 0.905 (0.905) data 0.696 (0.696) loss 0.1041 (0.1041) acc 100.0000 (100.0000) lr 5.0454e-04 eta 0:00:59
epoch [135/200] batch [1/1] time 0.973 (0.973) data 0.771 (0.771) loss 0.0930 (0.0930) acc 100.0000 (100.0000) lr 4.9096e-04 eta 0:01:03
epoch [136/200] batch [1/1] time 0.901 (0.901) data 0.691 (0.691) loss 0.0282 (0.0282) acc 100.0000 (100.0000) lr 4.7750e-04 eta 0:00:57
epoch [137/200] batch [1/1] time 0.900 (0.900) data 0.691 (0.691) loss 0.1573 (0.1573) acc 95.0000 (95.0000) lr 4.6417e-04 eta 0:00:56
epoch [138/200] batch [1/1] time 0.951 (0.951) data 0.780 (0.780) loss 0.1740 (0.1740) acc 95.0000 (95.0000) lr 4.5098e-04 eta 0:00:58
epoch [139/200] batch [1/1] time 0.887 (0.887) data 0.696 (0.696) loss 0.0646 (0.0646) acc 100.0000 (100.0000) lr 4.3792e-04 eta 0:00:54
epoch [140/200] batch [1/1] time 0.895 (0.895) data 0.704 (0.704) loss 0.0318 (0.0318) acc 100.0000 (100.0000) lr 4.2499e-04 eta 0:00:53
epoch [141/200] batch [1/1] time 0.899 (0.899) data 0.704 (0.704) loss 0.1096 (0.1096) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:00:53
epoch [142/200] batch [1/1] time 0.906 (0.906) data 0.698 (0.698) loss 0.1913 (0.1913) acc 95.0000 (95.0000) lr 3.9958e-04 eta 0:00:52
epoch [143/200] batch [1/1] time 0.889 (0.889) data 0.679 (0.679) loss 0.0454 (0.0454) acc 100.0000 (100.0000) lr 3.8709e-04 eta 0:00:50
epoch [144/200] batch [1/1] time 1.045 (1.045) data 0.847 (0.847) loss 0.0925 (0.0925) acc 95.0000 (95.0000) lr 3.7476e-04 eta 0:00:58
epoch [145/200] batch [1/1] time 0.884 (0.884) data 0.691 (0.691) loss 0.1793 (0.1793) acc 95.0000 (95.0000) lr 3.6258e-04 eta 0:00:48
epoch [146/200] batch [1/1] time 0.950 (0.950) data 0.768 (0.768) loss 0.0522 (0.0522) acc 100.0000 (100.0000) lr 3.5055e-04 eta 0:00:51
epoch [147/200] batch [1/1] time 0.913 (0.913) data 0.712 (0.712) loss 0.1927 (0.1927) acc 95.0000 (95.0000) lr 3.3869e-04 eta 0:00:48
epoch [148/200] batch [1/1] time 0.891 (0.891) data 0.688 (0.688) loss 0.0396 (0.0396) acc 100.0000 (100.0000) lr 3.2699e-04 eta 0:00:46
epoch [149/200] batch [1/1] time 0.899 (0.899) data 0.695 (0.695) loss 0.1048 (0.1048) acc 100.0000 (100.0000) lr 3.1545e-04 eta 0:00:45
epoch [150/200] batch [1/1] time 0.893 (0.893) data 0.694 (0.694) loss 0.0566 (0.0566) acc 100.0000 (100.0000) lr 3.0409e-04 eta 0:00:44
epoch [151/200] batch [1/1] time 0.890 (0.890) data 0.693 (0.693) loss 0.0902 (0.0902) acc 100.0000 (100.0000) lr 2.9289e-04 eta 0:00:43
epoch [152/200] batch [1/1] time 0.904 (0.904) data 0.696 (0.696) loss 0.0465 (0.0465) acc 100.0000 (100.0000) lr 2.8187e-04 eta 0:00:43
epoch [153/200] batch [1/1] time 0.886 (0.886) data 0.690 (0.690) loss 0.0520 (0.0520) acc 100.0000 (100.0000) lr 2.7103e-04 eta 0:00:41
epoch [154/200] batch [1/1] time 0.935 (0.935) data 0.768 (0.768) loss 0.2142 (0.2142) acc 95.0000 (95.0000) lr 2.6037e-04 eta 0:00:43
epoch [155/200] batch [1/1] time 0.843 (0.843) data 0.656 (0.656) loss 0.0923 (0.0923) acc 100.0000 (100.0000) lr 2.4989e-04 eta 0:00:37
epoch [156/200] batch [1/1] time 0.906 (0.906) data 0.700 (0.700) loss 0.0777 (0.0777) acc 100.0000 (100.0000) lr 2.3959e-04 eta 0:00:39
epoch [157/200] batch [1/1] time 0.860 (0.860) data 0.672 (0.672) loss 0.0371 (0.0371) acc 100.0000 (100.0000) lr 2.2949e-04 eta 0:00:36
epoch [158/200] batch [1/1] time 0.955 (0.955) data 0.770 (0.770) loss 0.1606 (0.1606) acc 95.0000 (95.0000) lr 2.1957e-04 eta 0:00:40
epoch [159/200] batch [1/1] time 0.898 (0.898) data 0.697 (0.697) loss 0.1135 (0.1135) acc 95.0000 (95.0000) lr 2.0984e-04 eta 0:00:36
epoch [160/200] batch [1/1] time 0.897 (0.897) data 0.691 (0.691) loss 0.0851 (0.0851) acc 100.0000 (100.0000) lr 2.0032e-04 eta 0:00:35
epoch [161/200] batch [1/1] time 0.903 (0.903) data 0.707 (0.707) loss 0.0406 (0.0406) acc 100.0000 (100.0000) lr 1.9098e-04 eta 0:00:35
epoch [162/200] batch [1/1] time 0.901 (0.901) data 0.695 (0.695) loss 0.2192 (0.2192) acc 90.0000 (90.0000) lr 1.8185e-04 eta 0:00:34
epoch [163/200] batch [1/1] time 0.892 (0.892) data 0.695 (0.695) loss 0.1355 (0.1355) acc 95.0000 (95.0000) lr 1.7292e-04 eta 0:00:33
epoch [164/200] batch [1/1] time 0.904 (0.904) data 0.698 (0.698) loss 0.2620 (0.2620) acc 85.0000 (85.0000) lr 1.6419e-04 eta 0:00:32
epoch [165/200] batch [1/1] time 0.892 (0.892) data 0.692 (0.692) loss 0.0647 (0.0647) acc 100.0000 (100.0000) lr 1.5567e-04 eta 0:00:31
epoch [166/200] batch [1/1] time 0.906 (0.906) data 0.701 (0.701) loss 0.0860 (0.0860) acc 100.0000 (100.0000) lr 1.4736e-04 eta 0:00:30
epoch [167/200] batch [1/1] time 0.887 (0.887) data 0.695 (0.695) loss 0.2583 (0.2583) acc 90.0000 (90.0000) lr 1.3926e-04 eta 0:00:29
epoch [168/200] batch [1/1] time 0.892 (0.892) data 0.687 (0.687) loss 0.1842 (0.1842) acc 95.0000 (95.0000) lr 1.3137e-04 eta 0:00:28
epoch [169/200] batch [1/1] time 0.899 (0.899) data 0.694 (0.694) loss 0.0296 (0.0296) acc 100.0000 (100.0000) lr 1.2369e-04 eta 0:00:27
epoch [170/200] batch [1/1] time 1.002 (1.002) data 0.801 (0.801) loss 0.0455 (0.0455) acc 100.0000 (100.0000) lr 1.1623e-04 eta 0:00:30
epoch [171/200] batch [1/1] time 0.875 (0.875) data 0.683 (0.683) loss 0.0541 (0.0541) acc 100.0000 (100.0000) lr 1.0899e-04 eta 0:00:25
epoch [172/200] batch [1/1] time 0.963 (0.963) data 0.761 (0.761) loss 0.1458 (0.1458) acc 95.0000 (95.0000) lr 1.0197e-04 eta 0:00:26
epoch [173/200] batch [1/1] time 0.877 (0.877) data 0.684 (0.684) loss 0.2042 (0.2042) acc 90.0000 (90.0000) lr 9.5173e-05 eta 0:00:23
epoch [174/200] batch [1/1] time 0.946 (0.946) data 0.751 (0.751) loss 0.1768 (0.1768) acc 95.0000 (95.0000) lr 8.8597e-05 eta 0:00:24
epoch [175/200] batch [1/1] time 0.890 (0.890) data 0.695 (0.695) loss 0.0431 (0.0431) acc 100.0000 (100.0000) lr 8.2245e-05 eta 0:00:22
epoch [176/200] batch [1/1] time 0.892 (0.892) data 0.686 (0.686) loss 0.1737 (0.1737) acc 95.0000 (95.0000) lr 7.6120e-05 eta 0:00:21
epoch [177/200] batch [1/1] time 0.876 (0.876) data 0.682 (0.682) loss 0.1152 (0.1152) acc 95.0000 (95.0000) lr 7.0224e-05 eta 0:00:20
epoch [178/200] batch [1/1] time 0.899 (0.899) data 0.693 (0.693) loss 0.0404 (0.0404) acc 100.0000 (100.0000) lr 6.4556e-05 eta 0:00:19
epoch [179/200] batch [1/1] time 0.892 (0.892) data 0.689 (0.689) loss 0.0580 (0.0580) acc 100.0000 (100.0000) lr 5.9119e-05 eta 0:00:18
epoch [180/200] batch [1/1] time 0.857 (0.857) data 0.690 (0.690) loss 0.0431 (0.0431) acc 100.0000 (100.0000) lr 5.3915e-05 eta 0:00:17
epoch [181/200] batch [1/1] time 0.849 (0.849) data 0.682 (0.682) loss 0.1074 (0.1074) acc 95.0000 (95.0000) lr 4.8943e-05 eta 0:00:16
epoch [182/200] batch [1/1] time 1.028 (1.028) data 0.690 (0.690) loss 0.1670 (0.1670) acc 95.0000 (95.0000) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [1/1] time 0.909 (0.909) data 0.704 (0.704) loss 0.0246 (0.0246) acc 100.0000 (100.0000) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [1/1] time 0.862 (0.862) data 0.693 (0.693) loss 0.0632 (0.0632) acc 100.0000 (100.0000) lr 3.5443e-05 eta 0:00:13
epoch [185/200] batch [1/1] time 0.882 (0.882) data 0.693 (0.693) loss 0.1465 (0.1465) acc 95.0000 (95.0000) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [1/1] time 0.886 (0.886) data 0.695 (0.695) loss 0.0257 (0.0257) acc 100.0000 (100.0000) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [1/1] time 0.861 (0.861) data 0.671 (0.671) loss 0.0523 (0.0523) acc 100.0000 (100.0000) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [1/1] time 0.864 (0.864) data 0.700 (0.700) loss 0.0755 (0.0755) acc 100.0000 (100.0000) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [1/1] time 0.868 (0.868) data 0.686 (0.686) loss 0.0594 (0.0594) acc 100.0000 (100.0000) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [1/1] time 0.912 (0.912) data 0.702 (0.702) loss 0.0646 (0.0646) acc 100.0000 (100.0000) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [1/1] time 0.899 (0.899) data 0.690 (0.690) loss 0.1162 (0.1162) acc 95.0000 (95.0000) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [1/1] time 0.894 (0.894) data 0.687 (0.687) loss 0.2240 (0.2240) acc 95.0000 (95.0000) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [1/1] time 0.876 (0.876) data 0.687 (0.687) loss 0.0291 (0.0291) acc 100.0000 (100.0000) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [1/1] time 0.952 (0.952) data 0.762 (0.762) loss 0.2673 (0.2673) acc 95.0000 (95.0000) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [1/1] time 0.898 (0.898) data 0.688 (0.688) loss 0.0635 (0.0635) acc 100.0000 (100.0000) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [1/1] time 0.926 (0.926) data 0.763 (0.763) loss 0.0511 (0.0511) acc 100.0000 (100.0000) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [1/1] time 0.885 (0.885) data 0.700 (0.700) loss 0.0661 (0.0661) acc 100.0000 (100.0000) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [1/1] time 0.912 (0.912) data 0.730 (0.730) loss 0.0810 (0.0810) acc 100.0000 (100.0000) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [1/1] time 0.903 (0.903) data 0.701 (0.701) loss 0.0859 (0.0859) acc 95.0000 (95.0000) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [1/1] time 0.900 (0.900) data 0.692 (0.692) loss 0.0495 (0.0495) acc 100.0000 (100.0000) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,100
* correct: 5,393
* accuracy: 66.6%
* error: 33.4%
* macro_f1: 65.2%
training time for 1-th round: 289.21 seconds
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
| Calculating uncertainty of Unlabeled set
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
INITIALIZE the prompts weights
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
['X X X X X X X X X X X X X X X X Annual Crop Land.', 'X X X X X X X X X X X X X X X X Forest.', 'X X X X X X X X X X X X X X X X Herbaceous Vegetation Land.', 'X X X X X X X X X X X X X X X X Highway or Road.', 'X X X X X X X X X X X X X X X X Industrial Buildings.', 'X X X X X X X X X X X X X X X X Pasture Land.', 'X X X X X X X X X X X X X X X X Permanent Crop Land.', 'X X X X X X X X X X X X X X X X Residential Buildings.', 'X X X X X X X X X X X X X X X X River.', 'X X X X X X X X X X X X X X X X Sea or Lake.']
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CustomCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
Turning off gradients in both the image and the text encoder
Multiple GPUs detected (n_gpus=2), use all of them!
DataParallel(
  (module): CustomCLIP(
    (prompt_learner): PromptLearner()
    (image_encoder): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
epoch [1/200] batch [1/1] time 0.975 (0.975) data 0.784 (0.784) loss 2.1270 (2.1270) acc 26.6667 (26.6667) lr 2.0000e-03 eta 0:03:13
epoch [2/200] batch [1/1] time 0.938 (0.938) data 0.727 (0.727) loss 2.3652 (2.3652) acc 13.3333 (13.3333) lr 1.9999e-03 eta 0:03:05
epoch [3/200] batch [1/1] time 0.943 (0.943) data 0.740 (0.740) loss 2.4727 (2.4727) acc 23.3333 (23.3333) lr 1.9995e-03 eta 0:03:05
epoch [4/200] batch [1/1] time 0.954 (0.954) data 0.761 (0.761) loss 2.0156 (2.0156) acc 33.3333 (33.3333) lr 1.9989e-03 eta 0:03:07
epoch [5/200] batch [1/1] time 0.965 (0.965) data 0.763 (0.763) loss 1.7471 (1.7471) acc 36.6667 (36.6667) lr 1.9980e-03 eta 0:03:08
epoch [6/200] batch [1/1] time 0.917 (0.917) data 0.741 (0.741) loss 1.8047 (1.8047) acc 50.0000 (50.0000) lr 1.9969e-03 eta 0:02:57
epoch [7/200] batch [1/1] time 0.935 (0.935) data 0.752 (0.752) loss 1.4463 (1.4463) acc 53.3333 (53.3333) lr 1.9956e-03 eta 0:03:00
epoch [8/200] batch [1/1] time 0.978 (0.978) data 0.762 (0.762) loss 1.4404 (1.4404) acc 50.0000 (50.0000) lr 1.9940e-03 eta 0:03:07
epoch [9/200] batch [1/1] time 0.950 (0.950) data 0.752 (0.752) loss 1.0586 (1.0586) acc 70.0000 (70.0000) lr 1.9921e-03 eta 0:03:01
epoch [10/200] batch [1/1] time 0.955 (0.955) data 0.760 (0.760) loss 1.0625 (1.0625) acc 63.3333 (63.3333) lr 1.9900e-03 eta 0:03:01
epoch [11/200] batch [1/1] time 0.960 (0.960) data 0.749 (0.749) loss 1.0186 (1.0186) acc 70.0000 (70.0000) lr 1.9877e-03 eta 0:03:01
epoch [12/200] batch [1/1] time 0.948 (0.948) data 0.746 (0.746) loss 0.9067 (0.9067) acc 76.6667 (76.6667) lr 1.9851e-03 eta 0:02:58
epoch [13/200] batch [1/1] time 0.948 (0.948) data 0.762 (0.762) loss 0.8926 (0.8926) acc 66.6667 (66.6667) lr 1.9823e-03 eta 0:02:57
epoch [14/200] batch [1/1] time 0.948 (0.948) data 0.745 (0.745) loss 0.8740 (0.8740) acc 66.6667 (66.6667) lr 1.9792e-03 eta 0:02:56
epoch [15/200] batch [1/1] time 0.958 (0.958) data 0.757 (0.757) loss 0.8594 (0.8594) acc 80.0000 (80.0000) lr 1.9759e-03 eta 0:02:57
epoch [16/200] batch [1/1] time 0.965 (0.965) data 0.766 (0.766) loss 0.8760 (0.8760) acc 76.6667 (76.6667) lr 1.9724e-03 eta 0:02:57
epoch [17/200] batch [1/1] time 0.966 (0.966) data 0.756 (0.756) loss 0.5117 (0.5117) acc 90.0000 (90.0000) lr 1.9686e-03 eta 0:02:56
epoch [18/200] batch [1/1] time 0.973 (0.973) data 0.767 (0.767) loss 0.6572 (0.6572) acc 73.3333 (73.3333) lr 1.9646e-03 eta 0:02:57
epoch [19/200] batch [1/1] time 0.939 (0.939) data 0.744 (0.744) loss 0.7461 (0.7461) acc 80.0000 (80.0000) lr 1.9603e-03 eta 0:02:49
epoch [20/200] batch [1/1] time 0.958 (0.958) data 0.752 (0.752) loss 0.4131 (0.4131) acc 86.6667 (86.6667) lr 1.9558e-03 eta 0:02:52
epoch [21/200] batch [1/1] time 0.958 (0.958) data 0.769 (0.769) loss 0.6836 (0.6836) acc 80.0000 (80.0000) lr 1.9511e-03 eta 0:02:51
epoch [22/200] batch [1/1] time 0.953 (0.953) data 0.751 (0.751) loss 0.5796 (0.5796) acc 76.6667 (76.6667) lr 1.9461e-03 eta 0:02:49
epoch [23/200] batch [1/1] time 0.955 (0.955) data 0.746 (0.746) loss 0.6382 (0.6382) acc 76.6667 (76.6667) lr 1.9409e-03 eta 0:02:49
epoch [24/200] batch [1/1] time 0.960 (0.960) data 0.750 (0.750) loss 0.5449 (0.5449) acc 83.3333 (83.3333) lr 1.9354e-03 eta 0:02:48
epoch [25/200] batch [1/1] time 0.916 (0.916) data 0.746 (0.746) loss 0.4316 (0.4316) acc 90.0000 (90.0000) lr 1.9298e-03 eta 0:02:40
epoch [26/200] batch [1/1] time 0.961 (0.961) data 0.757 (0.757) loss 0.4050 (0.4050) acc 90.0000 (90.0000) lr 1.9239e-03 eta 0:02:47
epoch [27/200] batch [1/1] time 0.972 (0.972) data 0.762 (0.762) loss 0.3687 (0.3687) acc 83.3333 (83.3333) lr 1.9178e-03 eta 0:02:48
epoch [28/200] batch [1/1] time 0.961 (0.961) data 0.765 (0.765) loss 0.2288 (0.2288) acc 96.6667 (96.6667) lr 1.9114e-03 eta 0:02:45
epoch [29/200] batch [1/1] time 0.950 (0.950) data 0.747 (0.747) loss 0.3745 (0.3745) acc 93.3333 (93.3333) lr 1.9048e-03 eta 0:02:42
epoch [30/200] batch [1/1] time 0.958 (0.958) data 0.749 (0.749) loss 0.3516 (0.3516) acc 86.6667 (86.6667) lr 1.8980e-03 eta 0:02:42
epoch [31/200] batch [1/1] time 0.977 (0.977) data 0.760 (0.760) loss 0.3411 (0.3411) acc 90.0000 (90.0000) lr 1.8910e-03 eta 0:02:45
epoch [32/200] batch [1/1] time 0.955 (0.955) data 0.746 (0.746) loss 0.3835 (0.3835) acc 93.3333 (93.3333) lr 1.8838e-03 eta 0:02:40
epoch [33/200] batch [1/1] time 0.915 (0.915) data 0.723 (0.723) loss 0.4211 (0.4211) acc 86.6667 (86.6667) lr 1.8763e-03 eta 0:02:32
epoch [34/200] batch [1/1] time 0.955 (0.955) data 0.756 (0.756) loss 0.3472 (0.3472) acc 90.0000 (90.0000) lr 1.8686e-03 eta 0:02:38
epoch [35/200] batch [1/1] time 0.964 (0.964) data 0.751 (0.751) loss 0.2671 (0.2671) acc 100.0000 (100.0000) lr 1.8607e-03 eta 0:02:39
epoch [36/200] batch [1/1] time 1.021 (1.021) data 0.847 (0.847) loss 0.3145 (0.3145) acc 96.6667 (96.6667) lr 1.8526e-03 eta 0:02:47
epoch [37/200] batch [1/1] time 0.964 (0.964) data 0.750 (0.750) loss 0.4087 (0.4087) acc 83.3333 (83.3333) lr 1.8443e-03 eta 0:02:37
epoch [38/200] batch [1/1] time 0.934 (0.934) data 0.757 (0.757) loss 0.3989 (0.3989) acc 83.3333 (83.3333) lr 1.8358e-03 eta 0:02:31
epoch [39/200] batch [1/1] time 0.967 (0.967) data 0.756 (0.756) loss 0.2145 (0.2145) acc 96.6667 (96.6667) lr 1.8271e-03 eta 0:02:35
epoch [40/200] batch [1/1] time 0.971 (0.971) data 0.760 (0.760) loss 0.1832 (0.1832) acc 100.0000 (100.0000) lr 1.8181e-03 eta 0:02:35
epoch [41/200] batch [1/1] time 0.956 (0.956) data 0.754 (0.754) loss 0.3694 (0.3694) acc 93.3333 (93.3333) lr 1.8090e-03 eta 0:02:31
epoch [42/200] batch [1/1] time 0.943 (0.943) data 0.737 (0.737) loss 0.3108 (0.3108) acc 93.3333 (93.3333) lr 1.7997e-03 eta 0:02:28
epoch [43/200] batch [1/1] time 0.969 (0.969) data 0.765 (0.765) loss 0.2600 (0.2600) acc 96.6667 (96.6667) lr 1.7902e-03 eta 0:02:32
epoch [44/200] batch [1/1] time 1.003 (1.003) data 0.784 (0.784) loss 0.4507 (0.4507) acc 83.3333 (83.3333) lr 1.7804e-03 eta 0:02:36
epoch [45/200] batch [1/1] time 0.963 (0.963) data 0.757 (0.757) loss 0.2729 (0.2729) acc 93.3333 (93.3333) lr 1.7705e-03 eta 0:02:29
epoch [46/200] batch [1/1] time 0.950 (0.950) data 0.742 (0.742) loss 0.2629 (0.2629) acc 93.3333 (93.3333) lr 1.7604e-03 eta 0:02:26
epoch [47/200] batch [1/1] time 0.966 (0.966) data 0.774 (0.774) loss 0.2415 (0.2415) acc 93.3333 (93.3333) lr 1.7501e-03 eta 0:02:27
epoch [48/200] batch [1/1] time 1.038 (1.038) data 0.848 (0.848) loss 0.2671 (0.2671) acc 96.6667 (96.6667) lr 1.7396e-03 eta 0:02:37
epoch [49/200] batch [1/1] time 0.952 (0.952) data 0.766 (0.766) loss 0.1636 (0.1636) acc 96.6667 (96.6667) lr 1.7290e-03 eta 0:02:23
epoch [50/200] batch [1/1] time 0.962 (0.962) data 0.753 (0.753) loss 0.1437 (0.1437) acc 100.0000 (100.0000) lr 1.7181e-03 eta 0:02:24
epoch [51/200] batch [1/1] time 0.974 (0.974) data 0.768 (0.768) loss 0.2678 (0.2678) acc 93.3333 (93.3333) lr 1.7071e-03 eta 0:02:25
epoch [52/200] batch [1/1] time 0.984 (0.984) data 0.778 (0.778) loss 0.3379 (0.3379) acc 86.6667 (86.6667) lr 1.6959e-03 eta 0:02:25
epoch [53/200] batch [1/1] time 1.018 (1.018) data 0.825 (0.825) loss 0.2114 (0.2114) acc 96.6667 (96.6667) lr 1.6845e-03 eta 0:02:29
epoch [54/200] batch [1/1] time 0.965 (0.965) data 0.756 (0.756) loss 0.1897 (0.1897) acc 100.0000 (100.0000) lr 1.6730e-03 eta 0:02:20
epoch [55/200] batch [1/1] time 0.979 (0.979) data 0.768 (0.768) loss 0.1868 (0.1868) acc 96.6667 (96.6667) lr 1.6613e-03 eta 0:02:21
epoch [56/200] batch [1/1] time 0.973 (0.973) data 0.761 (0.761) loss 0.2101 (0.2101) acc 96.6667 (96.6667) lr 1.6494e-03 eta 0:02:20
epoch [57/200] batch [1/1] time 0.969 (0.969) data 0.755 (0.755) loss 0.2717 (0.2717) acc 90.0000 (90.0000) lr 1.6374e-03 eta 0:02:18
epoch [58/200] batch [1/1] time 1.007 (1.007) data 0.792 (0.792) loss 0.2202 (0.2202) acc 96.6667 (96.6667) lr 1.6252e-03 eta 0:02:22
epoch [59/200] batch [1/1] time 0.999 (0.999) data 0.790 (0.790) loss 0.0695 (0.0695) acc 100.0000 (100.0000) lr 1.6129e-03 eta 0:02:20
epoch [60/200] batch [1/1] time 0.962 (0.962) data 0.753 (0.753) loss 0.2593 (0.2593) acc 93.3333 (93.3333) lr 1.6004e-03 eta 0:02:14
epoch [61/200] batch [1/1] time 0.942 (0.942) data 0.753 (0.753) loss 0.1274 (0.1274) acc 100.0000 (100.0000) lr 1.5878e-03 eta 0:02:10
epoch [62/200] batch [1/1] time 0.966 (0.966) data 0.754 (0.754) loss 0.3274 (0.3274) acc 90.0000 (90.0000) lr 1.5750e-03 eta 0:02:13
epoch [63/200] batch [1/1] time 0.974 (0.974) data 0.775 (0.775) loss 0.1575 (0.1575) acc 100.0000 (100.0000) lr 1.5621e-03 eta 0:02:13
epoch [64/200] batch [1/1] time 0.968 (0.968) data 0.757 (0.757) loss 0.2244 (0.2244) acc 93.3333 (93.3333) lr 1.5490e-03 eta 0:02:11
epoch [65/200] batch [1/1] time 0.976 (0.976) data 0.768 (0.768) loss 0.1758 (0.1758) acc 96.6667 (96.6667) lr 1.5358e-03 eta 0:02:11
epoch [66/200] batch [1/1] time 1.037 (1.037) data 0.837 (0.837) loss 0.2225 (0.2225) acc 96.6667 (96.6667) lr 1.5225e-03 eta 0:02:18
epoch [67/200] batch [1/1] time 0.962 (0.962) data 0.750 (0.750) loss 0.1456 (0.1456) acc 96.6667 (96.6667) lr 1.5090e-03 eta 0:02:07
epoch [68/200] batch [1/1] time 1.043 (1.043) data 0.835 (0.835) loss 0.2355 (0.2355) acc 96.6667 (96.6667) lr 1.4955e-03 eta 0:02:17
epoch [69/200] batch [1/1] time 0.969 (0.969) data 0.760 (0.760) loss 0.1088 (0.1088) acc 100.0000 (100.0000) lr 1.4818e-03 eta 0:02:06
epoch [70/200] batch [1/1] time 1.018 (1.018) data 0.825 (0.825) loss 0.1658 (0.1658) acc 96.6667 (96.6667) lr 1.4679e-03 eta 0:02:12
epoch [71/200] batch [1/1] time 1.006 (1.006) data 0.820 (0.820) loss 0.1337 (0.1337) acc 96.6667 (96.6667) lr 1.4540e-03 eta 0:02:09
epoch [72/200] batch [1/1] time 0.939 (0.939) data 0.751 (0.751) loss 0.2279 (0.2279) acc 93.3333 (93.3333) lr 1.4399e-03 eta 0:02:00
epoch [73/200] batch [1/1] time 0.911 (0.911) data 0.723 (0.723) loss 0.1664 (0.1664) acc 96.6667 (96.6667) lr 1.4258e-03 eta 0:01:55
epoch [74/200] batch [1/1] time 1.048 (1.048) data 0.841 (0.841) loss 0.1812 (0.1812) acc 96.6667 (96.6667) lr 1.4115e-03 eta 0:02:11
epoch [75/200] batch [1/1] time 0.950 (0.950) data 0.760 (0.760) loss 0.1967 (0.1967) acc 96.6667 (96.6667) lr 1.3971e-03 eta 0:01:58
epoch [76/200] batch [1/1] time 0.960 (0.960) data 0.756 (0.756) loss 0.3472 (0.3472) acc 90.0000 (90.0000) lr 1.3827e-03 eta 0:01:58
epoch [77/200] batch [1/1] time 0.964 (0.964) data 0.759 (0.759) loss 0.1913 (0.1913) acc 96.6667 (96.6667) lr 1.3681e-03 eta 0:01:58
epoch [78/200] batch [1/1] time 1.066 (1.066) data 0.864 (0.864) loss 0.1107 (0.1107) acc 96.6667 (96.6667) lr 1.3535e-03 eta 0:02:10
epoch [79/200] batch [1/1] time 0.957 (0.957) data 0.784 (0.784) loss 0.1761 (0.1761) acc 93.3333 (93.3333) lr 1.3387e-03 eta 0:01:55
epoch [80/200] batch [1/1] time 0.919 (0.919) data 0.749 (0.749) loss 0.2209 (0.2209) acc 96.6667 (96.6667) lr 1.3239e-03 eta 0:01:50
epoch [81/200] batch [1/1] time 0.932 (0.932) data 0.762 (0.762) loss 0.2864 (0.2864) acc 96.6667 (96.6667) lr 1.3090e-03 eta 0:01:50
epoch [82/200] batch [1/1] time 0.954 (0.954) data 0.760 (0.760) loss 0.2191 (0.2191) acc 90.0000 (90.0000) lr 1.2940e-03 eta 0:01:52
epoch [83/200] batch [1/1] time 0.969 (0.969) data 0.753 (0.753) loss 0.0911 (0.0911) acc 100.0000 (100.0000) lr 1.2790e-03 eta 0:01:53
epoch [84/200] batch [1/1] time 0.926 (0.926) data 0.758 (0.758) loss 0.1305 (0.1305) acc 96.6667 (96.6667) lr 1.2639e-03 eta 0:01:47
epoch [85/200] batch [1/1] time 0.982 (0.982) data 0.767 (0.767) loss 0.2739 (0.2739) acc 96.6667 (96.6667) lr 1.2487e-03 eta 0:01:52
epoch [86/200] batch [1/1] time 1.163 (1.163) data 0.761 (0.761) loss 0.0973 (0.0973) acc 96.6667 (96.6667) lr 1.2334e-03 eta 0:02:12
epoch [87/200] batch [1/1] time 1.021 (1.021) data 0.824 (0.824) loss 0.1152 (0.1152) acc 96.6667 (96.6667) lr 1.2181e-03 eta 0:01:55
epoch [88/200] batch [1/1] time 0.991 (0.991) data 0.774 (0.774) loss 0.1664 (0.1664) acc 96.6667 (96.6667) lr 1.2028e-03 eta 0:01:50
epoch [89/200] batch [1/1] time 0.984 (0.984) data 0.774 (0.774) loss 0.3105 (0.3105) acc 90.0000 (90.0000) lr 1.1874e-03 eta 0:01:49
epoch [90/200] batch [1/1] time 0.982 (0.982) data 0.764 (0.764) loss 0.1844 (0.1844) acc 93.3333 (93.3333) lr 1.1719e-03 eta 0:01:47
epoch [91/200] batch [1/1] time 0.977 (0.977) data 0.763 (0.763) loss 0.1415 (0.1415) acc 93.3333 (93.3333) lr 1.1564e-03 eta 0:01:46
epoch [92/200] batch [1/1] time 0.953 (0.953) data 0.753 (0.753) loss 0.1003 (0.1003) acc 96.6667 (96.6667) lr 1.1409e-03 eta 0:01:42
epoch [93/200] batch [1/1] time 0.960 (0.960) data 0.752 (0.752) loss 0.3357 (0.3357) acc 90.0000 (90.0000) lr 1.1253e-03 eta 0:01:42
epoch [94/200] batch [1/1] time 0.959 (0.959) data 0.757 (0.757) loss 0.3474 (0.3474) acc 90.0000 (90.0000) lr 1.1097e-03 eta 0:01:41
epoch [95/200] batch [1/1] time 0.959 (0.959) data 0.751 (0.751) loss 0.1771 (0.1771) acc 96.6667 (96.6667) lr 1.0941e-03 eta 0:01:40
epoch [96/200] batch [1/1] time 0.965 (0.965) data 0.760 (0.760) loss 0.2666 (0.2666) acc 93.3333 (93.3333) lr 1.0785e-03 eta 0:01:40
epoch [97/200] batch [1/1] time 0.968 (0.968) data 0.759 (0.759) loss 0.1323 (0.1323) acc 100.0000 (100.0000) lr 1.0628e-03 eta 0:01:39
epoch [98/200] batch [1/1] time 0.954 (0.954) data 0.749 (0.749) loss 0.2305 (0.2305) acc 93.3333 (93.3333) lr 1.0471e-03 eta 0:01:37
epoch [99/200] batch [1/1] time 0.971 (0.971) data 0.755 (0.755) loss 0.1475 (0.1475) acc 93.3333 (93.3333) lr 1.0314e-03 eta 0:01:38
epoch [100/200] batch [1/1] time 0.972 (0.972) data 0.752 (0.752) loss 0.1875 (0.1875) acc 93.3333 (93.3333) lr 1.0157e-03 eta 0:01:37
epoch [101/200] batch [1/1] time 0.974 (0.974) data 0.756 (0.756) loss 0.1448 (0.1448) acc 96.6667 (96.6667) lr 1.0000e-03 eta 0:01:36
epoch [102/200] batch [1/1] time 0.964 (0.964) data 0.764 (0.764) loss 0.0821 (0.0821) acc 100.0000 (100.0000) lr 9.8429e-04 eta 0:01:34
epoch [103/200] batch [1/1] time 0.962 (0.962) data 0.764 (0.764) loss 0.1659 (0.1659) acc 96.6667 (96.6667) lr 9.6859e-04 eta 0:01:33
epoch [104/200] batch [1/1] time 0.961 (0.961) data 0.745 (0.745) loss 0.1326 (0.1326) acc 96.6667 (96.6667) lr 9.5289e-04 eta 0:01:32
epoch [105/200] batch [1/1] time 1.013 (1.013) data 0.804 (0.804) loss 0.0880 (0.0880) acc 100.0000 (100.0000) lr 9.3721e-04 eta 0:01:36
epoch [106/200] batch [1/1] time 0.962 (0.962) data 0.755 (0.755) loss 0.2600 (0.2600) acc 93.3333 (93.3333) lr 9.2154e-04 eta 0:01:30
epoch [107/200] batch [1/1] time 0.957 (0.957) data 0.748 (0.748) loss 0.1493 (0.1493) acc 96.6667 (96.6667) lr 9.0589e-04 eta 0:01:28
epoch [108/200] batch [1/1] time 0.959 (0.959) data 0.744 (0.744) loss 0.1119 (0.1119) acc 96.6667 (96.6667) lr 8.9027e-04 eta 0:01:28
epoch [109/200] batch [1/1] time 0.938 (0.938) data 0.763 (0.763) loss 0.1731 (0.1731) acc 96.6667 (96.6667) lr 8.7467e-04 eta 0:01:25
epoch [110/200] batch [1/1] time 0.941 (0.941) data 0.732 (0.732) loss 0.1532 (0.1532) acc 96.6667 (96.6667) lr 8.5910e-04 eta 0:01:24
epoch [111/200] batch [1/1] time 1.041 (1.041) data 0.831 (0.831) loss 0.1043 (0.1043) acc 96.6667 (96.6667) lr 8.4357e-04 eta 0:01:32
epoch [112/200] batch [1/1] time 0.962 (0.962) data 0.756 (0.756) loss 0.0570 (0.0570) acc 100.0000 (100.0000) lr 8.2807e-04 eta 0:01:24
epoch [113/200] batch [1/1] time 0.927 (0.927) data 0.736 (0.736) loss 0.1802 (0.1802) acc 100.0000 (100.0000) lr 8.1262e-04 eta 0:01:20
epoch [114/200] batch [1/1] time 0.929 (0.929) data 0.759 (0.759) loss 0.1616 (0.1616) acc 93.3333 (93.3333) lr 7.9721e-04 eta 0:01:19
epoch [115/200] batch [1/1] time 0.931 (0.931) data 0.756 (0.756) loss 0.1981 (0.1981) acc 90.0000 (90.0000) lr 7.8186e-04 eta 0:01:19
epoch [116/200] batch [1/1] time 0.968 (0.968) data 0.762 (0.762) loss 0.1840 (0.1840) acc 96.6667 (96.6667) lr 7.6655e-04 eta 0:01:21
epoch [117/200] batch [1/1] time 0.970 (0.970) data 0.759 (0.759) loss 0.1104 (0.1104) acc 96.6667 (96.6667) lr 7.5131e-04 eta 0:01:20
epoch [118/200] batch [1/1] time 0.961 (0.961) data 0.759 (0.759) loss 0.1494 (0.1494) acc 90.0000 (90.0000) lr 7.3613e-04 eta 0:01:18
epoch [119/200] batch [1/1] time 0.941 (0.941) data 0.765 (0.765) loss 0.0907 (0.0907) acc 100.0000 (100.0000) lr 7.2101e-04 eta 0:01:16
epoch [120/200] batch [1/1] time 0.930 (0.930) data 0.737 (0.737) loss 0.1129 (0.1129) acc 96.6667 (96.6667) lr 7.0596e-04 eta 0:01:14
epoch [121/200] batch [1/1] time 1.012 (1.012) data 0.841 (0.841) loss 0.0642 (0.0642) acc 100.0000 (100.0000) lr 6.9098e-04 eta 0:01:19
epoch [122/200] batch [1/1] time 0.963 (0.963) data 0.754 (0.754) loss 0.0744 (0.0744) acc 100.0000 (100.0000) lr 6.7608e-04 eta 0:01:15
epoch [123/200] batch [1/1] time 0.927 (0.927) data 0.746 (0.746) loss 0.1216 (0.1216) acc 96.6667 (96.6667) lr 6.6126e-04 eta 0:01:11
epoch [124/200] batch [1/1] time 0.968 (0.968) data 0.757 (0.757) loss 0.0974 (0.0974) acc 100.0000 (100.0000) lr 6.4653e-04 eta 0:01:13
epoch [125/200] batch [1/1] time 0.965 (0.965) data 0.754 (0.754) loss 0.0650 (0.0650) acc 100.0000 (100.0000) lr 6.3188e-04 eta 0:01:12
epoch [126/200] batch [1/1] time 0.958 (0.958) data 0.754 (0.754) loss 0.1761 (0.1761) acc 93.3333 (93.3333) lr 6.1732e-04 eta 0:01:10
epoch [127/200] batch [1/1] time 0.951 (0.951) data 0.756 (0.756) loss 0.1131 (0.1131) acc 96.6667 (96.6667) lr 6.0285e-04 eta 0:01:09
epoch [128/200] batch [1/1] time 0.968 (0.968) data 0.769 (0.769) loss 0.1959 (0.1959) acc 93.3333 (93.3333) lr 5.8849e-04 eta 0:01:09
epoch [129/200] batch [1/1] time 0.965 (0.965) data 0.753 (0.753) loss 0.1971 (0.1971) acc 93.3333 (93.3333) lr 5.7422e-04 eta 0:01:08
epoch [130/200] batch [1/1] time 1.081 (1.081) data 0.902 (0.902) loss 0.0720 (0.0720) acc 100.0000 (100.0000) lr 5.6006e-04 eta 0:01:15
epoch [131/200] batch [1/1] time 1.034 (1.034) data 0.837 (0.837) loss 0.1703 (0.1703) acc 93.3333 (93.3333) lr 5.4601e-04 eta 0:01:11
epoch [132/200] batch [1/1] time 0.967 (0.967) data 0.757 (0.757) loss 0.2119 (0.2119) acc 93.3333 (93.3333) lr 5.3207e-04 eta 0:01:05
epoch [133/200] batch [1/1] time 1.001 (1.001) data 0.789 (0.789) loss 0.0610 (0.0610) acc 100.0000 (100.0000) lr 5.1825e-04 eta 0:01:07
epoch [134/200] batch [1/1] time 0.915 (0.915) data 0.721 (0.721) loss 0.0654 (0.0654) acc 100.0000 (100.0000) lr 5.0454e-04 eta 0:01:00
epoch [135/200] batch [1/1] time 0.968 (0.968) data 0.752 (0.752) loss 0.0922 (0.0922) acc 96.6667 (96.6667) lr 4.9096e-04 eta 0:01:02
epoch [136/200] batch [1/1] time 0.951 (0.951) data 0.754 (0.754) loss 0.1377 (0.1377) acc 96.6667 (96.6667) lr 4.7750e-04 eta 0:01:00
epoch [137/200] batch [1/1] time 0.967 (0.967) data 0.757 (0.757) loss 0.1418 (0.1418) acc 100.0000 (100.0000) lr 4.6417e-04 eta 0:01:00
epoch [138/200] batch [1/1] time 0.968 (0.968) data 0.758 (0.758) loss 0.1118 (0.1118) acc 100.0000 (100.0000) lr 4.5098e-04 eta 0:01:00
epoch [139/200] batch [1/1] time 0.944 (0.944) data 0.755 (0.755) loss 0.0770 (0.0770) acc 100.0000 (100.0000) lr 4.3792e-04 eta 0:00:57
epoch [140/200] batch [1/1] time 0.956 (0.956) data 0.761 (0.761) loss 0.1636 (0.1636) acc 93.3333 (93.3333) lr 4.2499e-04 eta 0:00:57
epoch [141/200] batch [1/1] time 0.939 (0.939) data 0.769 (0.769) loss 0.2251 (0.2251) acc 90.0000 (90.0000) lr 4.1221e-04 eta 0:00:55
epoch [142/200] batch [1/1] time 0.970 (0.970) data 0.754 (0.754) loss 0.1037 (0.1037) acc 96.6667 (96.6667) lr 3.9958e-04 eta 0:00:56
epoch [143/200] batch [1/1] time 0.967 (0.967) data 0.757 (0.757) loss 0.0892 (0.0892) acc 100.0000 (100.0000) lr 3.8709e-04 eta 0:00:55
epoch [144/200] batch [1/1] time 0.978 (0.978) data 0.766 (0.766) loss 0.1162 (0.1162) acc 96.6667 (96.6667) lr 3.7476e-04 eta 0:00:54
epoch [145/200] batch [1/1] time 0.950 (0.950) data 0.758 (0.758) loss 0.0347 (0.0347) acc 100.0000 (100.0000) lr 3.6258e-04 eta 0:00:52
epoch [146/200] batch [1/1] time 0.972 (0.972) data 0.764 (0.764) loss 0.1848 (0.1848) acc 96.6667 (96.6667) lr 3.5055e-04 eta 0:00:52
epoch [147/200] batch [1/1] time 0.955 (0.955) data 0.746 (0.746) loss 0.1791 (0.1791) acc 93.3333 (93.3333) lr 3.3869e-04 eta 0:00:50
epoch [148/200] batch [1/1] time 0.974 (0.974) data 0.761 (0.761) loss 0.1497 (0.1497) acc 96.6667 (96.6667) lr 3.2699e-04 eta 0:00:50
epoch [149/200] batch [1/1] time 0.953 (0.953) data 0.762 (0.762) loss 0.0739 (0.0739) acc 96.6667 (96.6667) lr 3.1545e-04 eta 0:00:48
epoch [150/200] batch [1/1] time 0.954 (0.954) data 0.759 (0.759) loss 0.0828 (0.0828) acc 100.0000 (100.0000) lr 3.0409e-04 eta 0:00:47
epoch [151/200] batch [1/1] time 1.113 (1.113) data 0.909 (0.909) loss 0.1814 (0.1814) acc 93.3333 (93.3333) lr 2.9289e-04 eta 0:00:54
epoch [152/200] batch [1/1] time 1.137 (1.137) data 0.919 (0.919) loss 0.0917 (0.0917) acc 100.0000 (100.0000) lr 2.8187e-04 eta 0:00:54
epoch [153/200] batch [1/1] time 0.952 (0.952) data 0.757 (0.757) loss 0.2194 (0.2194) acc 90.0000 (90.0000) lr 2.7103e-04 eta 0:00:44
epoch [154/200] batch [1/1] time 0.971 (0.971) data 0.767 (0.767) loss 0.1862 (0.1862) acc 93.3333 (93.3333) lr 2.6037e-04 eta 0:00:44
epoch [155/200] batch [1/1] time 0.950 (0.950) data 0.744 (0.744) loss 0.1213 (0.1213) acc 96.6667 (96.6667) lr 2.4989e-04 eta 0:00:42
epoch [156/200] batch [1/1] time 0.960 (0.960) data 0.762 (0.762) loss 0.0842 (0.0842) acc 96.6667 (96.6667) lr 2.3959e-04 eta 0:00:42
epoch [157/200] batch [1/1] time 1.040 (1.040) data 0.840 (0.840) loss 0.0840 (0.0840) acc 100.0000 (100.0000) lr 2.2949e-04 eta 0:00:44
epoch [158/200] batch [1/1] time 1.113 (1.113) data 0.761 (0.761) loss 0.0903 (0.0903) acc 100.0000 (100.0000) lr 2.1957e-04 eta 0:00:46
epoch [159/200] batch [1/1] time 0.978 (0.978) data 0.778 (0.778) loss 0.2384 (0.2384) acc 93.3333 (93.3333) lr 2.0984e-04 eta 0:00:40
epoch [160/200] batch [1/1] time 1.019 (1.019) data 0.842 (0.842) loss 0.1293 (0.1293) acc 96.6667 (96.6667) lr 2.0032e-04 eta 0:00:40
epoch [161/200] batch [1/1] time 0.965 (0.965) data 0.752 (0.752) loss 0.0534 (0.0534) acc 100.0000 (100.0000) lr 1.9098e-04 eta 0:00:37
epoch [162/200] batch [1/1] time 1.013 (1.013) data 0.819 (0.819) loss 0.0986 (0.0986) acc 96.6667 (96.6667) lr 1.8185e-04 eta 0:00:38
epoch [163/200] batch [1/1] time 0.889 (0.889) data 0.720 (0.720) loss 0.0677 (0.0677) acc 100.0000 (100.0000) lr 1.7292e-04 eta 0:00:32
epoch [164/200] batch [1/1] time 0.950 (0.950) data 0.750 (0.750) loss 0.1128 (0.1128) acc 96.6667 (96.6667) lr 1.6419e-04 eta 0:00:34
epoch [165/200] batch [1/1] time 0.978 (0.978) data 0.767 (0.767) loss 0.1730 (0.1730) acc 93.3333 (93.3333) lr 1.5567e-04 eta 0:00:34
epoch [166/200] batch [1/1] time 0.955 (0.955) data 0.755 (0.755) loss 0.1549 (0.1549) acc 96.6667 (96.6667) lr 1.4736e-04 eta 0:00:32
epoch [167/200] batch [1/1] time 0.962 (0.962) data 0.754 (0.754) loss 0.1031 (0.1031) acc 100.0000 (100.0000) lr 1.3926e-04 eta 0:00:31
epoch [168/200] batch [1/1] time 0.963 (0.963) data 0.752 (0.752) loss 0.2015 (0.2015) acc 93.3333 (93.3333) lr 1.3137e-04 eta 0:00:30
epoch [169/200] batch [1/1] time 0.961 (0.961) data 0.761 (0.761) loss 0.0624 (0.0624) acc 100.0000 (100.0000) lr 1.2369e-04 eta 0:00:29
epoch [170/200] batch [1/1] time 0.951 (0.951) data 0.753 (0.753) loss 0.1530 (0.1530) acc 96.6667 (96.6667) lr 1.1623e-04 eta 0:00:28
epoch [171/200] batch [1/1] time 1.000 (1.000) data 0.786 (0.786) loss 0.0573 (0.0573) acc 100.0000 (100.0000) lr 1.0899e-04 eta 0:00:28
epoch [172/200] batch [1/1] time 0.984 (0.984) data 0.775 (0.775) loss 0.1808 (0.1808) acc 96.6667 (96.6667) lr 1.0197e-04 eta 0:00:27
epoch [173/200] batch [1/1] time 0.906 (0.906) data 0.735 (0.735) loss 0.0894 (0.0894) acc 100.0000 (100.0000) lr 9.5173e-05 eta 0:00:24
epoch [174/200] batch [1/1] time 1.026 (1.026) data 0.850 (0.850) loss 0.1223 (0.1223) acc 100.0000 (100.0000) lr 8.8597e-05 eta 0:00:26
epoch [175/200] batch [1/1] time 1.030 (1.030) data 0.825 (0.825) loss 0.1123 (0.1123) acc 96.6667 (96.6667) lr 8.2245e-05 eta 0:00:25
epoch [176/200] batch [1/1] time 0.973 (0.973) data 0.761 (0.761) loss 0.0647 (0.0647) acc 100.0000 (100.0000) lr 7.6120e-05 eta 0:00:23
epoch [177/200] batch [1/1] time 0.978 (0.978) data 0.765 (0.765) loss 0.1354 (0.1354) acc 96.6667 (96.6667) lr 7.0224e-05 eta 0:00:22
epoch [178/200] batch [1/1] time 0.982 (0.982) data 0.781 (0.781) loss 0.1348 (0.1348) acc 93.3333 (93.3333) lr 6.4556e-05 eta 0:00:21
epoch [179/200] batch [1/1] time 0.971 (0.971) data 0.758 (0.758) loss 0.1621 (0.1621) acc 93.3333 (93.3333) lr 5.9119e-05 eta 0:00:20
epoch [180/200] batch [1/1] time 1.007 (1.007) data 0.835 (0.835) loss 0.0781 (0.0781) acc 96.6667 (96.6667) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [1/1] time 1.062 (1.062) data 0.853 (0.853) loss 0.0499 (0.0499) acc 100.0000 (100.0000) lr 4.8943e-05 eta 0:00:20
epoch [182/200] batch [1/1] time 0.955 (0.955) data 0.738 (0.738) loss 0.1022 (0.1022) acc 100.0000 (100.0000) lr 4.4207e-05 eta 0:00:17
epoch [183/200] batch [1/1] time 0.920 (0.920) data 0.719 (0.719) loss 0.1758 (0.1758) acc 93.3333 (93.3333) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [1/1] time 0.927 (0.927) data 0.716 (0.716) loss 0.0946 (0.0946) acc 100.0000 (100.0000) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [1/1] time 0.966 (0.966) data 0.763 (0.763) loss 0.1572 (0.1572) acc 96.6667 (96.6667) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [1/1] time 0.970 (0.970) data 0.773 (0.773) loss 0.1587 (0.1587) acc 96.6667 (96.6667) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [1/1] time 0.987 (0.987) data 0.772 (0.772) loss 0.0761 (0.0761) acc 100.0000 (100.0000) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [1/1] time 0.981 (0.981) data 0.766 (0.766) loss 0.0577 (0.0577) acc 100.0000 (100.0000) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [1/1] time 0.978 (0.978) data 0.765 (0.765) loss 0.0964 (0.0964) acc 96.6667 (96.6667) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [1/1] time 1.030 (1.030) data 0.835 (0.835) loss 0.0818 (0.0818) acc 100.0000 (100.0000) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [1/1] time 0.955 (0.955) data 0.757 (0.757) loss 0.0450 (0.0450) acc 100.0000 (100.0000) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [1/1] time 0.939 (0.939) data 0.725 (0.725) loss 0.0718 (0.0718) acc 100.0000 (100.0000) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [1/1] time 0.956 (0.956) data 0.751 (0.751) loss 0.0787 (0.0787) acc 100.0000 (100.0000) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [1/1] time 0.970 (0.970) data 0.759 (0.759) loss 0.0470 (0.0470) acc 100.0000 (100.0000) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [1/1] time 0.945 (0.945) data 0.756 (0.756) loss 0.0832 (0.0832) acc 100.0000 (100.0000) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [1/1] time 0.954 (0.954) data 0.759 (0.759) loss 0.0913 (0.0913) acc 100.0000 (100.0000) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [1/1] time 0.977 (0.977) data 0.761 (0.761) loss 0.1606 (0.1606) acc 93.3333 (93.3333) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [1/1] time 0.899 (0.899) data 0.724 (0.724) loss 0.1658 (0.1658) acc 93.3333 (93.3333) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [1/1] time 0.958 (0.958) data 0.761 (0.761) loss 0.2164 (0.2164) acc 93.3333 (93.3333) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [1/1] time 0.992 (0.992) data 0.786 (0.786) loss 0.1805 (0.1805) acc 96.6667 (96.6667) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,100
* correct: 5,925
* accuracy: 73.1%
* error: 26.9%
* macro_f1: 72.6%
training time for 2-th round: 303.77 seconds
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
| Calculating uncertainty of Unlabeled set
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
INITIALIZE the prompts weights
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
['X X X X X X X X X X X X X X X X Annual Crop Land.', 'X X X X X X X X X X X X X X X X Forest.', 'X X X X X X X X X X X X X X X X Herbaceous Vegetation Land.', 'X X X X X X X X X X X X X X X X Highway or Road.', 'X X X X X X X X X X X X X X X X Industrial Buildings.', 'X X X X X X X X X X X X X X X X Pasture Land.', 'X X X X X X X X X X X X X X X X Permanent Crop Land.', 'X X X X X X X X X X X X X X X X Residential Buildings.', 'X X X X X X X X X X X X X X X X River.', 'X X X X X X X X X X X X X X X X Sea or Lake.']
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CustomCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
Turning off gradients in both the image and the text encoder
Multiple GPUs detected (n_gpus=2), use all of them!
DataParallel(
  (module): CustomCLIP(
    (prompt_learner): PromptLearner()
    (image_encoder): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
epoch [1/200] batch [1/1] time 1.272 (1.272) data 0.952 (0.952) loss 2.7109 (2.7109) acc 15.6250 (15.6250) lr 2.0000e-03 eta 0:04:13
epoch [2/200] batch [1/1] time 0.972 (0.972) data 0.758 (0.758) loss 2.3730 (2.3730) acc 9.3750 (9.3750) lr 1.9999e-03 eta 0:03:12
epoch [3/200] batch [1/1] time 1.014 (1.014) data 0.820 (0.820) loss 2.2949 (2.2949) acc 37.5000 (37.5000) lr 1.9995e-03 eta 0:03:19
epoch [4/200] batch [1/1] time 0.988 (0.988) data 0.795 (0.795) loss 2.3926 (2.3926) acc 18.7500 (18.7500) lr 1.9989e-03 eta 0:03:13
epoch [5/200] batch [1/1] time 1.077 (1.077) data 0.868 (0.868) loss 1.7178 (1.7178) acc 46.8750 (46.8750) lr 1.9980e-03 eta 0:03:29
epoch [6/200] batch [1/1] time 0.987 (0.987) data 0.784 (0.784) loss 1.4961 (1.4961) acc 43.7500 (43.7500) lr 1.9969e-03 eta 0:03:11
epoch [7/200] batch [1/1] time 0.956 (0.956) data 0.767 (0.767) loss 1.5791 (1.5791) acc 50.0000 (50.0000) lr 1.9956e-03 eta 0:03:04
epoch [8/200] batch [1/1] time 0.982 (0.982) data 0.780 (0.780) loss 1.6357 (1.6357) acc 43.7500 (43.7500) lr 1.9940e-03 eta 0:03:08
epoch [9/200] batch [1/1] time 1.007 (1.007) data 0.796 (0.796) loss 1.4238 (1.4238) acc 56.2500 (56.2500) lr 1.9921e-03 eta 0:03:12
epoch [10/200] batch [1/1] time 0.957 (0.957) data 0.761 (0.761) loss 1.0195 (1.0195) acc 78.1250 (78.1250) lr 1.9900e-03 eta 0:03:01
epoch [11/200] batch [1/1] time 0.955 (0.955) data 0.782 (0.782) loss 1.0615 (1.0615) acc 65.6250 (65.6250) lr 1.9877e-03 eta 0:03:00
epoch [12/200] batch [1/1] time 1.153 (1.153) data 0.942 (0.942) loss 1.0020 (1.0020) acc 68.7500 (68.7500) lr 1.9851e-03 eta 0:03:36
epoch [13/200] batch [1/1] time 1.040 (1.040) data 0.866 (0.866) loss 0.9189 (0.9189) acc 68.7500 (68.7500) lr 1.9823e-03 eta 0:03:14
epoch [14/200] batch [1/1] time 0.985 (0.985) data 0.787 (0.787) loss 0.7549 (0.7549) acc 84.3750 (84.3750) lr 1.9792e-03 eta 0:03:03
epoch [15/200] batch [1/1] time 0.926 (0.926) data 0.736 (0.736) loss 1.0361 (1.0361) acc 68.7500 (68.7500) lr 1.9759e-03 eta 0:02:51
epoch [16/200] batch [1/1] time 1.007 (1.007) data 0.794 (0.794) loss 0.8340 (0.8340) acc 65.6250 (65.6250) lr 1.9724e-03 eta 0:03:05
epoch [17/200] batch [1/1] time 0.990 (0.990) data 0.780 (0.780) loss 0.9082 (0.9082) acc 68.7500 (68.7500) lr 1.9686e-03 eta 0:03:01
epoch [18/200] batch [1/1] time 1.004 (1.004) data 0.799 (0.799) loss 0.6138 (0.6138) acc 84.3750 (84.3750) lr 1.9646e-03 eta 0:03:02
epoch [19/200] batch [1/1] time 1.055 (1.055) data 0.855 (0.855) loss 0.7734 (0.7734) acc 75.0000 (75.0000) lr 1.9603e-03 eta 0:03:10
epoch [20/200] batch [1/1] time 1.021 (1.021) data 0.819 (0.819) loss 0.7363 (0.7363) acc 78.1250 (78.1250) lr 1.9558e-03 eta 0:03:03
epoch [21/200] batch [1/1] time 0.998 (0.998) data 0.791 (0.791) loss 0.4810 (0.4810) acc 90.6250 (90.6250) lr 1.9511e-03 eta 0:02:58
epoch [22/200] batch [1/1] time 0.987 (0.987) data 0.788 (0.788) loss 0.8389 (0.8389) acc 78.1250 (78.1250) lr 1.9461e-03 eta 0:02:55
epoch [23/200] batch [1/1] time 0.993 (0.993) data 0.786 (0.786) loss 0.7764 (0.7764) acc 81.2500 (81.2500) lr 1.9409e-03 eta 0:02:55
epoch [24/200] batch [1/1] time 0.970 (0.970) data 0.780 (0.780) loss 0.6660 (0.6660) acc 81.2500 (81.2500) lr 1.9354e-03 eta 0:02:50
epoch [25/200] batch [1/1] time 0.976 (0.976) data 0.784 (0.784) loss 0.5229 (0.5229) acc 90.6250 (90.6250) lr 1.9298e-03 eta 0:02:50
epoch [26/200] batch [1/1] time 0.968 (0.968) data 0.781 (0.781) loss 0.4021 (0.4021) acc 87.5000 (87.5000) lr 1.9239e-03 eta 0:02:48
epoch [27/200] batch [1/1] time 0.994 (0.994) data 0.794 (0.794) loss 0.5625 (0.5625) acc 87.5000 (87.5000) lr 1.9178e-03 eta 0:02:52
epoch [28/200] batch [1/1] time 0.976 (0.976) data 0.764 (0.764) loss 0.5103 (0.5103) acc 87.5000 (87.5000) lr 1.9114e-03 eta 0:02:47
epoch [29/200] batch [1/1] time 1.034 (1.034) data 0.818 (0.818) loss 0.3689 (0.3689) acc 90.6250 (90.6250) lr 1.9048e-03 eta 0:02:56
epoch [30/200] batch [1/1] time 0.996 (0.996) data 0.788 (0.788) loss 0.4773 (0.4773) acc 84.3750 (84.3750) lr 1.8980e-03 eta 0:02:49
epoch [31/200] batch [1/1] time 0.964 (0.964) data 0.795 (0.795) loss 0.5439 (0.5439) acc 84.3750 (84.3750) lr 1.8910e-03 eta 0:02:42
epoch [32/200] batch [1/1] time 1.005 (1.005) data 0.801 (0.801) loss 0.3518 (0.3518) acc 87.5000 (87.5000) lr 1.8838e-03 eta 0:02:48
epoch [33/200] batch [1/1] time 1.071 (1.071) data 0.880 (0.880) loss 0.3220 (0.3220) acc 93.7500 (93.7500) lr 1.8763e-03 eta 0:02:58
epoch [34/200] batch [1/1] time 0.967 (0.967) data 0.789 (0.789) loss 0.5898 (0.5898) acc 78.1250 (78.1250) lr 1.8686e-03 eta 0:02:40
epoch [35/200] batch [1/1] time 0.954 (0.954) data 0.780 (0.780) loss 0.7466 (0.7466) acc 84.3750 (84.3750) lr 1.8607e-03 eta 0:02:37
epoch [36/200] batch [1/1] time 1.015 (1.015) data 0.797 (0.797) loss 0.4600 (0.4600) acc 81.2500 (81.2500) lr 1.8526e-03 eta 0:02:46
epoch [37/200] batch [1/1] time 1.012 (1.012) data 0.793 (0.793) loss 0.4709 (0.4709) acc 87.5000 (87.5000) lr 1.8443e-03 eta 0:02:44
epoch [38/200] batch [1/1] time 0.998 (0.998) data 0.792 (0.792) loss 0.3157 (0.3157) acc 90.6250 (90.6250) lr 1.8358e-03 eta 0:02:41
epoch [39/200] batch [1/1] time 1.014 (1.014) data 0.801 (0.801) loss 0.3096 (0.3096) acc 90.6250 (90.6250) lr 1.8271e-03 eta 0:02:43
epoch [40/200] batch [1/1] time 0.983 (0.983) data 0.783 (0.783) loss 0.3042 (0.3042) acc 90.6250 (90.6250) lr 1.8181e-03 eta 0:02:37
epoch [41/200] batch [1/1] time 0.996 (0.996) data 0.799 (0.799) loss 0.3147 (0.3147) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:02:38
epoch [42/200] batch [1/1] time 0.983 (0.983) data 0.795 (0.795) loss 0.3857 (0.3857) acc 90.6250 (90.6250) lr 1.7997e-03 eta 0:02:35
epoch [43/200] batch [1/1] time 0.995 (0.995) data 0.781 (0.781) loss 0.2397 (0.2397) acc 90.6250 (90.6250) lr 1.7902e-03 eta 0:02:36
epoch [44/200] batch [1/1] time 1.069 (1.069) data 0.863 (0.863) loss 0.4902 (0.4902) acc 84.3750 (84.3750) lr 1.7804e-03 eta 0:02:46
epoch [45/200] batch [1/1] time 0.989 (0.989) data 0.778 (0.778) loss 0.3516 (0.3516) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:02:33
epoch [46/200] batch [1/1] time 1.057 (1.057) data 0.850 (0.850) loss 0.4214 (0.4214) acc 84.3750 (84.3750) lr 1.7604e-03 eta 0:02:42
epoch [47/200] batch [1/1] time 0.999 (0.999) data 0.786 (0.786) loss 0.3462 (0.3462) acc 90.6250 (90.6250) lr 1.7501e-03 eta 0:02:32
epoch [48/200] batch [1/1] time 0.985 (0.985) data 0.794 (0.794) loss 0.2070 (0.2070) acc 93.7500 (93.7500) lr 1.7396e-03 eta 0:02:29
epoch [49/200] batch [1/1] time 1.017 (1.017) data 0.819 (0.819) loss 0.3579 (0.3579) acc 90.6250 (90.6250) lr 1.7290e-03 eta 0:02:33
epoch [50/200] batch [1/1] time 1.040 (1.040) data 0.827 (0.827) loss 0.3367 (0.3367) acc 93.7500 (93.7500) lr 1.7181e-03 eta 0:02:35
epoch [51/200] batch [1/1] time 1.003 (1.003) data 0.787 (0.787) loss 0.1980 (0.1980) acc 96.8750 (96.8750) lr 1.7071e-03 eta 0:02:29
epoch [52/200] batch [1/1] time 1.055 (1.055) data 0.854 (0.854) loss 0.3860 (0.3860) acc 90.6250 (90.6250) lr 1.6959e-03 eta 0:02:36
epoch [53/200] batch [1/1] time 0.974 (0.974) data 0.781 (0.781) loss 0.3784 (0.3784) acc 90.6250 (90.6250) lr 1.6845e-03 eta 0:02:23
epoch [54/200] batch [1/1] time 1.014 (1.014) data 0.796 (0.796) loss 0.3704 (0.3704) acc 84.3750 (84.3750) lr 1.6730e-03 eta 0:02:28
epoch [55/200] batch [1/1] time 0.984 (0.984) data 0.793 (0.793) loss 0.2476 (0.2476) acc 96.8750 (96.8750) lr 1.6613e-03 eta 0:02:22
epoch [56/200] batch [1/1] time 0.983 (0.983) data 0.797 (0.797) loss 0.2404 (0.2404) acc 96.8750 (96.8750) lr 1.6494e-03 eta 0:02:21
epoch [57/200] batch [1/1] time 0.983 (0.983) data 0.790 (0.790) loss 0.2581 (0.2581) acc 93.7500 (93.7500) lr 1.6374e-03 eta 0:02:20
epoch [58/200] batch [1/1] time 0.961 (0.961) data 0.791 (0.791) loss 0.3767 (0.3767) acc 87.5000 (87.5000) lr 1.6252e-03 eta 0:02:16
epoch [59/200] batch [1/1] time 0.961 (0.961) data 0.793 (0.793) loss 0.2954 (0.2954) acc 90.6250 (90.6250) lr 1.6129e-03 eta 0:02:15
epoch [60/200] batch [1/1] time 1.102 (1.102) data 0.911 (0.911) loss 0.3203 (0.3203) acc 93.7500 (93.7500) lr 1.6004e-03 eta 0:02:34
epoch [61/200] batch [1/1] time 0.967 (0.967) data 0.792 (0.792) loss 0.4153 (0.4153) acc 84.3750 (84.3750) lr 1.5878e-03 eta 0:02:14
epoch [62/200] batch [1/1] time 0.948 (0.948) data 0.780 (0.780) loss 0.2041 (0.2041) acc 96.8750 (96.8750) lr 1.5750e-03 eta 0:02:10
epoch [63/200] batch [1/1] time 0.996 (0.996) data 0.799 (0.799) loss 0.2900 (0.2900) acc 93.7500 (93.7500) lr 1.5621e-03 eta 0:02:16
epoch [64/200] batch [1/1] time 1.007 (1.007) data 0.791 (0.791) loss 0.3806 (0.3806) acc 90.6250 (90.6250) lr 1.5490e-03 eta 0:02:17
epoch [65/200] batch [1/1] time 1.008 (1.008) data 0.787 (0.787) loss 0.1666 (0.1666) acc 96.8750 (96.8750) lr 1.5358e-03 eta 0:02:16
epoch [66/200] batch [1/1] time 0.997 (0.997) data 0.794 (0.794) loss 0.3811 (0.3811) acc 84.3750 (84.3750) lr 1.5225e-03 eta 0:02:13
epoch [67/200] batch [1/1] time 1.006 (1.006) data 0.794 (0.794) loss 0.1181 (0.1181) acc 100.0000 (100.0000) lr 1.5090e-03 eta 0:02:13
epoch [68/200] batch [1/1] time 0.951 (0.951) data 0.753 (0.753) loss 0.3274 (0.3274) acc 93.7500 (93.7500) lr 1.4955e-03 eta 0:02:05
epoch [69/200] batch [1/1] time 1.139 (1.139) data 0.923 (0.923) loss 0.2986 (0.2986) acc 90.6250 (90.6250) lr 1.4818e-03 eta 0:02:29
epoch [70/200] batch [1/1] time 1.025 (1.025) data 0.822 (0.822) loss 0.2017 (0.2017) acc 93.7500 (93.7500) lr 1.4679e-03 eta 0:02:13
epoch [71/200] batch [1/1] time 0.984 (0.984) data 0.786 (0.786) loss 0.1852 (0.1852) acc 96.8750 (96.8750) lr 1.4540e-03 eta 0:02:06
epoch [72/200] batch [1/1] time 0.990 (0.990) data 0.797 (0.797) loss 0.2423 (0.2423) acc 93.7500 (93.7500) lr 1.4399e-03 eta 0:02:06
epoch [73/200] batch [1/1] time 1.010 (1.010) data 0.799 (0.799) loss 0.3853 (0.3853) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:02:08
epoch [74/200] batch [1/1] time 1.004 (1.004) data 0.814 (0.814) loss 0.4133 (0.4133) acc 90.6250 (90.6250) lr 1.4115e-03 eta 0:02:06
epoch [75/200] batch [1/1] time 0.985 (0.985) data 0.782 (0.782) loss 0.2379 (0.2379) acc 93.7500 (93.7500) lr 1.3971e-03 eta 0:02:03
epoch [76/200] batch [1/1] time 0.999 (0.999) data 0.791 (0.791) loss 0.3201 (0.3201) acc 90.6250 (90.6250) lr 1.3827e-03 eta 0:02:03
epoch [77/200] batch [1/1] time 1.005 (1.005) data 0.802 (0.802) loss 0.4343 (0.4343) acc 93.7500 (93.7500) lr 1.3681e-03 eta 0:02:03
epoch [78/200] batch [1/1] time 1.006 (1.006) data 0.791 (0.791) loss 0.1892 (0.1892) acc 100.0000 (100.0000) lr 1.3535e-03 eta 0:02:02
epoch [79/200] batch [1/1] time 1.004 (1.004) data 0.786 (0.786) loss 0.4160 (0.4160) acc 81.2500 (81.2500) lr 1.3387e-03 eta 0:02:01
epoch [80/200] batch [1/1] time 1.003 (1.003) data 0.789 (0.789) loss 0.1357 (0.1357) acc 93.7500 (93.7500) lr 1.3239e-03 eta 0:02:00
epoch [81/200] batch [1/1] time 0.987 (0.987) data 0.801 (0.801) loss 0.2935 (0.2935) acc 93.7500 (93.7500) lr 1.3090e-03 eta 0:01:57
epoch [82/200] batch [1/1] time 1.016 (1.016) data 0.816 (0.816) loss 0.3123 (0.3123) acc 87.5000 (87.5000) lr 1.2940e-03 eta 0:01:59
epoch [83/200] batch [1/1] time 1.006 (1.006) data 0.793 (0.793) loss 0.2615 (0.2615) acc 90.6250 (90.6250) lr 1.2790e-03 eta 0:01:57
epoch [84/200] batch [1/1] time 0.972 (0.972) data 0.759 (0.759) loss 0.3022 (0.3022) acc 93.7500 (93.7500) lr 1.2639e-03 eta 0:01:52
epoch [85/200] batch [1/1] time 1.027 (1.027) data 0.812 (0.812) loss 0.2062 (0.2062) acc 96.8750 (96.8750) lr 1.2487e-03 eta 0:01:58
epoch [86/200] batch [1/1] time 1.002 (1.002) data 0.794 (0.794) loss 0.3835 (0.3835) acc 87.5000 (87.5000) lr 1.2334e-03 eta 0:01:54
epoch [87/200] batch [1/1] time 1.011 (1.011) data 0.797 (0.797) loss 0.1544 (0.1544) acc 96.8750 (96.8750) lr 1.2181e-03 eta 0:01:54
epoch [88/200] batch [1/1] time 0.989 (0.989) data 0.812 (0.812) loss 0.2445 (0.2445) acc 90.6250 (90.6250) lr 1.2028e-03 eta 0:01:50
epoch [89/200] batch [1/1] time 0.986 (0.986) data 0.794 (0.794) loss 0.3489 (0.3489) acc 87.5000 (87.5000) lr 1.1874e-03 eta 0:01:49
epoch [90/200] batch [1/1] time 1.075 (1.075) data 0.883 (0.883) loss 0.1489 (0.1489) acc 96.8750 (96.8750) lr 1.1719e-03 eta 0:01:58
epoch [91/200] batch [1/1] time 1.007 (1.007) data 0.806 (0.806) loss 0.2034 (0.2034) acc 93.7500 (93.7500) lr 1.1564e-03 eta 0:01:49
epoch [92/200] batch [1/1] time 1.010 (1.010) data 0.798 (0.798) loss 0.2045 (0.2045) acc 96.8750 (96.8750) lr 1.1409e-03 eta 0:01:49
epoch [93/200] batch [1/1] time 1.095 (1.095) data 0.892 (0.892) loss 0.3369 (0.3369) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:01:57
epoch [94/200] batch [1/1] time 0.953 (0.953) data 0.778 (0.778) loss 0.2510 (0.2510) acc 90.6250 (90.6250) lr 1.1097e-03 eta 0:01:41
epoch [95/200] batch [1/1] time 0.970 (0.970) data 0.792 (0.792) loss 0.1239 (0.1239) acc 100.0000 (100.0000) lr 1.0941e-03 eta 0:01:41
epoch [96/200] batch [1/1] time 1.086 (1.086) data 0.892 (0.892) loss 0.1171 (0.1171) acc 100.0000 (100.0000) lr 1.0785e-03 eta 0:01:52
epoch [97/200] batch [1/1] time 1.008 (1.008) data 0.795 (0.795) loss 0.3271 (0.3271) acc 90.6250 (90.6250) lr 1.0628e-03 eta 0:01:43
epoch [98/200] batch [1/1] time 1.074 (1.074) data 0.863 (0.863) loss 0.1428 (0.1428) acc 93.7500 (93.7500) lr 1.0471e-03 eta 0:01:49
epoch [99/200] batch [1/1] time 1.126 (1.126) data 0.927 (0.927) loss 0.1663 (0.1663) acc 96.8750 (96.8750) lr 1.0314e-03 eta 0:01:53
epoch [100/200] batch [1/1] time 0.978 (0.978) data 0.799 (0.799) loss 0.2456 (0.2456) acc 96.8750 (96.8750) lr 1.0157e-03 eta 0:01:37
epoch [101/200] batch [1/1] time 0.995 (0.995) data 0.794 (0.794) loss 0.2896 (0.2896) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:01:38
epoch [102/200] batch [1/1] time 1.118 (1.118) data 0.925 (0.925) loss 0.1422 (0.1422) acc 93.7500 (93.7500) lr 9.8429e-04 eta 0:01:49
epoch [103/200] batch [1/1] time 1.047 (1.047) data 0.870 (0.870) loss 0.1086 (0.1086) acc 96.8750 (96.8750) lr 9.6859e-04 eta 0:01:41
epoch [104/200] batch [1/1] time 0.976 (0.976) data 0.769 (0.769) loss 0.1809 (0.1809) acc 93.7500 (93.7500) lr 9.5289e-04 eta 0:01:33
epoch [105/200] batch [1/1] time 1.006 (1.006) data 0.799 (0.799) loss 0.0953 (0.0953) acc 100.0000 (100.0000) lr 9.3721e-04 eta 0:01:35
epoch [106/200] batch [1/1] time 0.996 (0.996) data 0.788 (0.788) loss 0.2573 (0.2573) acc 96.8750 (96.8750) lr 9.2154e-04 eta 0:01:33
epoch [107/200] batch [1/1] time 0.978 (0.978) data 0.777 (0.777) loss 0.0858 (0.0858) acc 100.0000 (100.0000) lr 9.0589e-04 eta 0:01:30
epoch [108/200] batch [1/1] time 0.991 (0.991) data 0.791 (0.791) loss 0.1176 (0.1176) acc 96.8750 (96.8750) lr 8.9027e-04 eta 0:01:31
epoch [109/200] batch [1/1] time 1.003 (1.003) data 0.792 (0.792) loss 0.2046 (0.2046) acc 96.8750 (96.8750) lr 8.7467e-04 eta 0:01:31
epoch [110/200] batch [1/1] time 0.997 (0.997) data 0.787 (0.787) loss 0.2966 (0.2966) acc 90.6250 (90.6250) lr 8.5910e-04 eta 0:01:29
epoch [111/200] batch [1/1] time 0.991 (0.991) data 0.775 (0.775) loss 0.1200 (0.1200) acc 100.0000 (100.0000) lr 8.4357e-04 eta 0:01:28
epoch [112/200] batch [1/1] time 1.005 (1.005) data 0.792 (0.792) loss 0.1608 (0.1608) acc 93.7500 (93.7500) lr 8.2807e-04 eta 0:01:28
epoch [113/200] batch [1/1] time 0.980 (0.980) data 0.785 (0.785) loss 0.0742 (0.0742) acc 100.0000 (100.0000) lr 8.1262e-04 eta 0:01:25
epoch [114/200] batch [1/1] time 1.140 (1.140) data 0.927 (0.927) loss 0.3022 (0.3022) acc 84.3750 (84.3750) lr 7.9721e-04 eta 0:01:38
epoch [115/200] batch [1/1] time 0.972 (0.972) data 0.776 (0.776) loss 0.1672 (0.1672) acc 96.8750 (96.8750) lr 7.8186e-04 eta 0:01:22
epoch [116/200] batch [1/1] time 0.995 (0.995) data 0.784 (0.784) loss 0.1234 (0.1234) acc 96.8750 (96.8750) lr 7.6655e-04 eta 0:01:23
epoch [117/200] batch [1/1] time 1.014 (1.014) data 0.795 (0.795) loss 0.3276 (0.3276) acc 90.6250 (90.6250) lr 7.5131e-04 eta 0:01:24
epoch [118/200] batch [1/1] time 1.018 (1.018) data 0.804 (0.804) loss 0.1991 (0.1991) acc 93.7500 (93.7500) lr 7.3613e-04 eta 0:01:23
epoch [119/200] batch [1/1] time 1.082 (1.082) data 0.870 (0.870) loss 0.2252 (0.2252) acc 93.7500 (93.7500) lr 7.2101e-04 eta 0:01:27
epoch [120/200] batch [1/1] time 1.012 (1.012) data 0.802 (0.802) loss 0.1190 (0.1190) acc 100.0000 (100.0000) lr 7.0596e-04 eta 0:01:20
epoch [121/200] batch [1/1] time 1.006 (1.006) data 0.790 (0.790) loss 0.0966 (0.0966) acc 100.0000 (100.0000) lr 6.9098e-04 eta 0:01:19
epoch [122/200] batch [1/1] time 0.962 (0.962) data 0.788 (0.788) loss 0.1604 (0.1604) acc 100.0000 (100.0000) lr 6.7608e-04 eta 0:01:15
epoch [123/200] batch [1/1] time 1.000 (1.000) data 0.791 (0.791) loss 0.0575 (0.0575) acc 100.0000 (100.0000) lr 6.6126e-04 eta 0:01:17
epoch [124/200] batch [1/1] time 0.925 (0.925) data 0.754 (0.754) loss 0.0883 (0.0883) acc 100.0000 (100.0000) lr 6.4653e-04 eta 0:01:10
epoch [125/200] batch [1/1] time 0.957 (0.957) data 0.752 (0.752) loss 0.2351 (0.2351) acc 96.8750 (96.8750) lr 6.3188e-04 eta 0:01:11
epoch [126/200] batch [1/1] time 1.020 (1.020) data 0.808 (0.808) loss 0.2656 (0.2656) acc 90.6250 (90.6250) lr 6.1732e-04 eta 0:01:15
epoch [127/200] batch [1/1] time 1.032 (1.032) data 0.817 (0.817) loss 0.1316 (0.1316) acc 100.0000 (100.0000) lr 6.0285e-04 eta 0:01:15
epoch [128/200] batch [1/1] time 1.002 (1.002) data 0.787 (0.787) loss 0.1099 (0.1099) acc 100.0000 (100.0000) lr 5.8849e-04 eta 0:01:12
epoch [129/200] batch [1/1] time 1.042 (1.042) data 0.866 (0.866) loss 0.1047 (0.1047) acc 100.0000 (100.0000) lr 5.7422e-04 eta 0:01:13
epoch [130/200] batch [1/1] time 1.019 (1.019) data 0.801 (0.801) loss 0.1234 (0.1234) acc 100.0000 (100.0000) lr 5.6006e-04 eta 0:01:11
epoch [131/200] batch [1/1] time 1.315 (1.315) data 0.949 (0.949) loss 0.2147 (0.2147) acc 93.7500 (93.7500) lr 5.4601e-04 eta 0:01:30
epoch [132/200] batch [1/1] time 0.969 (0.969) data 0.792 (0.792) loss 0.2524 (0.2524) acc 96.8750 (96.8750) lr 5.3207e-04 eta 0:01:05
epoch [133/200] batch [1/1] time 1.035 (1.035) data 0.818 (0.818) loss 0.2024 (0.2024) acc 93.7500 (93.7500) lr 5.1825e-04 eta 0:01:09
epoch [134/200] batch [1/1] time 1.009 (1.009) data 0.798 (0.798) loss 0.2896 (0.2896) acc 96.8750 (96.8750) lr 5.0454e-04 eta 0:01:06
epoch [135/200] batch [1/1] time 0.985 (0.985) data 0.794 (0.794) loss 0.1816 (0.1816) acc 93.7500 (93.7500) lr 4.9096e-04 eta 0:01:04
epoch [136/200] batch [1/1] time 1.019 (1.019) data 0.818 (0.818) loss 0.1215 (0.1215) acc 96.8750 (96.8750) lr 4.7750e-04 eta 0:01:05
epoch [137/200] batch [1/1] time 0.985 (0.985) data 0.785 (0.785) loss 0.2069 (0.2069) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:01:02
epoch [138/200] batch [1/1] time 1.083 (1.083) data 0.870 (0.870) loss 0.2666 (0.2666) acc 90.6250 (90.6250) lr 4.5098e-04 eta 0:01:07
epoch [139/200] batch [1/1] time 1.067 (1.067) data 0.894 (0.894) loss 0.1550 (0.1550) acc 100.0000 (100.0000) lr 4.3792e-04 eta 0:01:05
epoch [140/200] batch [1/1] time 0.976 (0.976) data 0.778 (0.778) loss 0.1315 (0.1315) acc 96.8750 (96.8750) lr 4.2499e-04 eta 0:00:58
epoch [141/200] batch [1/1] time 0.972 (0.972) data 0.789 (0.789) loss 0.0941 (0.0941) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:00:57
epoch [142/200] batch [1/1] time 1.009 (1.009) data 0.803 (0.803) loss 0.2059 (0.2059) acc 93.7500 (93.7500) lr 3.9958e-04 eta 0:00:58
epoch [143/200] batch [1/1] time 1.060 (1.060) data 0.867 (0.867) loss 0.0562 (0.0562) acc 100.0000 (100.0000) lr 3.8709e-04 eta 0:01:00
epoch [144/200] batch [1/1] time 0.958 (0.958) data 0.783 (0.783) loss 0.1825 (0.1825) acc 96.8750 (96.8750) lr 3.7476e-04 eta 0:00:53
epoch [145/200] batch [1/1] time 1.008 (1.008) data 0.795 (0.795) loss 0.2847 (0.2847) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:00:55
epoch [146/200] batch [1/1] time 0.981 (0.981) data 0.783 (0.783) loss 0.0892 (0.0892) acc 100.0000 (100.0000) lr 3.5055e-04 eta 0:00:52
epoch [147/200] batch [1/1] time 1.010 (1.010) data 0.790 (0.790) loss 0.1566 (0.1566) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:00:53
epoch [148/200] batch [1/1] time 1.000 (1.000) data 0.785 (0.785) loss 0.0865 (0.0865) acc 100.0000 (100.0000) lr 3.2699e-04 eta 0:00:51
epoch [149/200] batch [1/1] time 0.967 (0.967) data 0.795 (0.795) loss 0.1586 (0.1586) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:00:49
epoch [150/200] batch [1/1] time 0.985 (0.985) data 0.813 (0.813) loss 0.1056 (0.1056) acc 96.8750 (96.8750) lr 3.0409e-04 eta 0:00:49
epoch [151/200] batch [1/1] time 1.002 (1.002) data 0.800 (0.800) loss 0.2249 (0.2249) acc 90.6250 (90.6250) lr 2.9289e-04 eta 0:00:49
epoch [152/200] batch [1/1] time 0.974 (0.974) data 0.762 (0.762) loss 0.1041 (0.1041) acc 96.8750 (96.8750) lr 2.8187e-04 eta 0:00:46
epoch [153/200] batch [1/1] time 0.990 (0.990) data 0.789 (0.789) loss 0.1245 (0.1245) acc 100.0000 (100.0000) lr 2.7103e-04 eta 0:00:46
epoch [154/200] batch [1/1] time 1.012 (1.012) data 0.799 (0.799) loss 0.1499 (0.1499) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:00:46
epoch [155/200] batch [1/1] time 1.130 (1.130) data 0.933 (0.933) loss 0.1847 (0.1847) acc 90.6250 (90.6250) lr 2.4989e-04 eta 0:00:50
epoch [156/200] batch [1/1] time 1.006 (1.006) data 0.794 (0.794) loss 0.2842 (0.2842) acc 87.5000 (87.5000) lr 2.3959e-04 eta 0:00:44
epoch [157/200] batch [1/1] time 0.974 (0.974) data 0.799 (0.799) loss 0.0654 (0.0654) acc 100.0000 (100.0000) lr 2.2949e-04 eta 0:00:41
epoch [158/200] batch [1/1] time 0.981 (0.981) data 0.787 (0.787) loss 0.3486 (0.3486) acc 93.7500 (93.7500) lr 2.1957e-04 eta 0:00:41
epoch [159/200] batch [1/1] time 0.980 (0.980) data 0.777 (0.777) loss 0.1830 (0.1830) acc 93.7500 (93.7500) lr 2.0984e-04 eta 0:00:40
epoch [160/200] batch [1/1] time 0.997 (0.997) data 0.787 (0.787) loss 0.1042 (0.1042) acc 96.8750 (96.8750) lr 2.0032e-04 eta 0:00:39
epoch [161/200] batch [1/1] time 0.988 (0.988) data 0.781 (0.781) loss 0.2426 (0.2426) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:00:38
epoch [162/200] batch [1/1] time 1.009 (1.009) data 0.799 (0.799) loss 0.0864 (0.0864) acc 100.0000 (100.0000) lr 1.8185e-04 eta 0:00:38
epoch [163/200] batch [1/1] time 0.965 (0.965) data 0.775 (0.775) loss 0.2837 (0.2837) acc 93.7500 (93.7500) lr 1.7292e-04 eta 0:00:35
epoch [164/200] batch [1/1] time 0.937 (0.937) data 0.747 (0.747) loss 0.1907 (0.1907) acc 90.6250 (90.6250) lr 1.6419e-04 eta 0:00:33
epoch [165/200] batch [1/1] time 1.004 (1.004) data 0.795 (0.795) loss 0.1349 (0.1349) acc 96.8750 (96.8750) lr 1.5567e-04 eta 0:00:35
epoch [166/200] batch [1/1] time 0.987 (0.987) data 0.782 (0.782) loss 0.1554 (0.1554) acc 96.8750 (96.8750) lr 1.4736e-04 eta 0:00:33
epoch [167/200] batch [1/1] time 0.954 (0.954) data 0.781 (0.781) loss 0.0883 (0.0883) acc 100.0000 (100.0000) lr 1.3926e-04 eta 0:00:31
epoch [168/200] batch [1/1] time 1.113 (1.113) data 0.922 (0.922) loss 0.1464 (0.1464) acc 90.6250 (90.6250) lr 1.3137e-04 eta 0:00:35
epoch [169/200] batch [1/1] time 1.016 (1.016) data 0.817 (0.817) loss 0.1698 (0.1698) acc 96.8750 (96.8750) lr 1.2369e-04 eta 0:00:31
epoch [170/200] batch [1/1] time 1.074 (1.074) data 0.871 (0.871) loss 0.1478 (0.1478) acc 96.8750 (96.8750) lr 1.1623e-04 eta 0:00:32
epoch [171/200] batch [1/1] time 1.017 (1.017) data 0.803 (0.803) loss 0.1353 (0.1353) acc 100.0000 (100.0000) lr 1.0899e-04 eta 0:00:29
epoch [172/200] batch [1/1] time 1.036 (1.036) data 0.833 (0.833) loss 0.0710 (0.0710) acc 100.0000 (100.0000) lr 1.0197e-04 eta 0:00:28
epoch [173/200] batch [1/1] time 0.999 (0.999) data 0.790 (0.790) loss 0.1081 (0.1081) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:00:26
epoch [174/200] batch [1/1] time 0.993 (0.993) data 0.781 (0.781) loss 0.1708 (0.1708) acc 96.8750 (96.8750) lr 8.8597e-05 eta 0:00:25
epoch [175/200] batch [1/1] time 1.008 (1.008) data 0.832 (0.832) loss 0.1995 (0.1995) acc 93.7500 (93.7500) lr 8.2245e-05 eta 0:00:25
epoch [176/200] batch [1/1] time 0.991 (0.991) data 0.798 (0.798) loss 0.0527 (0.0527) acc 100.0000 (100.0000) lr 7.6120e-05 eta 0:00:23
epoch [177/200] batch [1/1] time 1.127 (1.127) data 0.912 (0.912) loss 0.1426 (0.1426) acc 100.0000 (100.0000) lr 7.0224e-05 eta 0:00:25
epoch [178/200] batch [1/1] time 0.958 (0.958) data 0.787 (0.787) loss 0.2854 (0.2854) acc 84.3750 (84.3750) lr 6.4556e-05 eta 0:00:21
epoch [179/200] batch [1/1] time 1.043 (1.043) data 0.831 (0.831) loss 0.1075 (0.1075) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:00:21
epoch [180/200] batch [1/1] time 0.981 (0.981) data 0.772 (0.772) loss 0.1454 (0.1454) acc 93.7500 (93.7500) lr 5.3915e-05 eta 0:00:19
epoch [181/200] batch [1/1] time 1.009 (1.009) data 0.791 (0.791) loss 0.1825 (0.1825) acc 100.0000 (100.0000) lr 4.8943e-05 eta 0:00:19
epoch [182/200] batch [1/1] time 1.052 (1.052) data 0.874 (0.874) loss 0.1815 (0.1815) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [1/1] time 0.977 (0.977) data 0.781 (0.781) loss 0.1007 (0.1007) acc 100.0000 (100.0000) lr 3.9706e-05 eta 0:00:16
epoch [184/200] batch [1/1] time 0.987 (0.987) data 0.788 (0.788) loss 0.1270 (0.1270) acc 100.0000 (100.0000) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [1/1] time 0.979 (0.979) data 0.789 (0.789) loss 0.1111 (0.1111) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [1/1] time 1.001 (1.001) data 0.788 (0.788) loss 0.1599 (0.1599) acc 96.8750 (96.8750) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [1/1] time 1.007 (1.007) data 0.805 (0.805) loss 0.2042 (0.2042) acc 96.8750 (96.8750) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [1/1] time 1.030 (1.030) data 0.823 (0.823) loss 0.0858 (0.0858) acc 100.0000 (100.0000) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [1/1] time 0.987 (0.987) data 0.785 (0.785) loss 0.1033 (0.1033) acc 96.8750 (96.8750) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [1/1] time 0.962 (0.962) data 0.778 (0.778) loss 0.1731 (0.1731) acc 93.7500 (93.7500) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [1/1] time 1.007 (1.007) data 0.796 (0.796) loss 0.2279 (0.2279) acc 93.7500 (93.7500) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [1/1] time 1.003 (1.003) data 0.790 (0.790) loss 0.1721 (0.1721) acc 93.7500 (93.7500) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [1/1] time 0.985 (0.985) data 0.783 (0.783) loss 0.1057 (0.1057) acc 96.8750 (96.8750) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [1/1] time 0.965 (0.965) data 0.791 (0.791) loss 0.3105 (0.3105) acc 90.6250 (90.6250) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [1/1] time 0.950 (0.950) data 0.779 (0.779) loss 0.1346 (0.1346) acc 93.7500 (93.7500) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [1/1] time 0.979 (0.979) data 0.790 (0.790) loss 0.1587 (0.1587) acc 96.8750 (96.8750) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [1/1] time 0.981 (0.981) data 0.788 (0.788) loss 0.3093 (0.3093) acc 90.6250 (90.6250) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [1/1] time 0.981 (0.981) data 0.771 (0.771) loss 0.1816 (0.1816) acc 93.7500 (93.7500) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [1/1] time 1.012 (1.012) data 0.802 (0.802) loss 0.0900 (0.0900) acc 100.0000 (100.0000) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [1/1] time 0.960 (0.960) data 0.773 (0.773) loss 0.1715 (0.1715) acc 96.8750 (96.8750) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,100
* correct: 6,560
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 80.7%
training time for 3-th round: 310.87 seconds
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
| Calculating uncertainty of Unlabeled set
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
INITIALIZE the prompts weights
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
['X X X X X X X X X X X X X X X X Annual Crop Land.', 'X X X X X X X X X X X X X X X X Forest.', 'X X X X X X X X X X X X X X X X Herbaceous Vegetation Land.', 'X X X X X X X X X X X X X X X X Highway or Road.', 'X X X X X X X X X X X X X X X X Industrial Buildings.', 'X X X X X X X X X X X X X X X X Pasture Land.', 'X X X X X X X X X X X X X X X X Permanent Crop Land.', 'X X X X X X X X X X X X X X X X Residential Buildings.', 'X X X X X X X X X X X X X X X X River.', 'X X X X X X X X X X X X X X X X Sea or Lake.']
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CustomCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
Turning off gradients in both the image and the text encoder
Multiple GPUs detected (n_gpus=2), use all of them!
DataParallel(
  (module): CustomCLIP(
    (prompt_learner): PromptLearner()
    (image_encoder): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
epoch [1/200] batch [1/1] time 0.984 (0.984) data 0.799 (0.799) loss 2.5410 (2.5410) acc 15.6250 (15.6250) lr 2.0000e-03 eta 0:03:15
epoch [2/200] batch [1/1] time 1.103 (1.103) data 0.886 (0.886) loss 2.1973 (2.1973) acc 21.8750 (21.8750) lr 1.9999e-03 eta 0:03:38
epoch [3/200] batch [1/1] time 1.131 (1.131) data 0.934 (0.934) loss 2.6172 (2.6172) acc 25.0000 (25.0000) lr 1.9995e-03 eta 0:03:42
epoch [4/200] batch [1/1] time 1.014 (1.014) data 0.798 (0.798) loss 2.0605 (2.0605) acc 25.0000 (25.0000) lr 1.9989e-03 eta 0:03:18
epoch [5/200] batch [1/1] time 0.990 (0.990) data 0.779 (0.779) loss 1.6924 (1.6924) acc 40.6250 (40.6250) lr 1.9980e-03 eta 0:03:13
epoch [6/200] batch [1/1] time 0.971 (0.971) data 0.775 (0.775) loss 2.0449 (2.0449) acc 37.5000 (37.5000) lr 1.9969e-03 eta 0:03:08
epoch [7/200] batch [1/1] time 1.007 (1.007) data 0.809 (0.809) loss 1.4824 (1.4824) acc 53.1250 (53.1250) lr 1.9956e-03 eta 0:03:14
epoch [8/200] batch [1/1] time 1.139 (1.139) data 0.919 (0.919) loss 1.4443 (1.4443) acc 46.8750 (46.8750) lr 1.9940e-03 eta 0:03:38
epoch [9/200] batch [1/1] time 0.999 (0.999) data 0.784 (0.784) loss 1.2393 (1.2393) acc 62.5000 (62.5000) lr 1.9921e-03 eta 0:03:10
epoch [10/200] batch [1/1] time 1.000 (1.000) data 0.803 (0.803) loss 0.9448 (0.9448) acc 81.2500 (81.2500) lr 1.9900e-03 eta 0:03:10
epoch [11/200] batch [1/1] time 1.015 (1.015) data 0.802 (0.802) loss 1.3770 (1.3770) acc 46.8750 (46.8750) lr 1.9877e-03 eta 0:03:11
epoch [12/200] batch [1/1] time 0.991 (0.991) data 0.799 (0.799) loss 1.0596 (1.0596) acc 68.7500 (68.7500) lr 1.9851e-03 eta 0:03:06
epoch [13/200] batch [1/1] time 1.011 (1.011) data 0.810 (0.810) loss 1.2510 (1.2510) acc 75.0000 (75.0000) lr 1.9823e-03 eta 0:03:09
epoch [14/200] batch [1/1] time 0.992 (0.992) data 0.796 (0.796) loss 1.0400 (1.0400) acc 56.2500 (56.2500) lr 1.9792e-03 eta 0:03:04
epoch [15/200] batch [1/1] time 1.035 (1.035) data 0.823 (0.823) loss 0.9478 (0.9478) acc 75.0000 (75.0000) lr 1.9759e-03 eta 0:03:11
epoch [16/200] batch [1/1] time 0.997 (0.997) data 0.790 (0.790) loss 0.6328 (0.6328) acc 75.0000 (75.0000) lr 1.9724e-03 eta 0:03:03
epoch [17/200] batch [1/1] time 1.001 (1.001) data 0.800 (0.800) loss 0.8613 (0.8613) acc 84.3750 (84.3750) lr 1.9686e-03 eta 0:03:03
epoch [18/200] batch [1/1] time 0.993 (0.993) data 0.789 (0.789) loss 0.6411 (0.6411) acc 87.5000 (87.5000) lr 1.9646e-03 eta 0:03:00
epoch [19/200] batch [1/1] time 1.012 (1.012) data 0.809 (0.809) loss 1.0332 (1.0332) acc 75.0000 (75.0000) lr 1.9603e-03 eta 0:03:03
epoch [20/200] batch [1/1] time 1.096 (1.096) data 0.888 (0.888) loss 0.6606 (0.6606) acc 84.3750 (84.3750) lr 1.9558e-03 eta 0:03:17
epoch [21/200] batch [1/1] time 1.067 (1.067) data 0.871 (0.871) loss 0.6089 (0.6089) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:03:11
epoch [22/200] batch [1/1] time 0.985 (0.985) data 0.790 (0.790) loss 0.5439 (0.5439) acc 90.6250 (90.6250) lr 1.9461e-03 eta 0:02:55
epoch [23/200] batch [1/1] time 1.004 (1.004) data 0.794 (0.794) loss 0.7246 (0.7246) acc 81.2500 (81.2500) lr 1.9409e-03 eta 0:02:57
epoch [24/200] batch [1/1] time 1.020 (1.020) data 0.821 (0.821) loss 0.6211 (0.6211) acc 81.2500 (81.2500) lr 1.9354e-03 eta 0:02:59
epoch [25/200] batch [1/1] time 1.036 (1.036) data 0.819 (0.819) loss 0.5903 (0.5903) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:03:01
epoch [26/200] batch [1/1] time 1.010 (1.010) data 0.792 (0.792) loss 0.4395 (0.4395) acc 84.3750 (84.3750) lr 1.9239e-03 eta 0:02:55
epoch [27/200] batch [1/1] time 1.017 (1.017) data 0.806 (0.806) loss 0.5952 (0.5952) acc 78.1250 (78.1250) lr 1.9178e-03 eta 0:02:55
epoch [28/200] batch [1/1] time 1.015 (1.015) data 0.808 (0.808) loss 0.5850 (0.5850) acc 81.2500 (81.2500) lr 1.9114e-03 eta 0:02:54
epoch [29/200] batch [1/1] time 1.175 (1.175) data 0.794 (0.794) loss 0.8462 (0.8462) acc 81.2500 (81.2500) lr 1.9048e-03 eta 0:03:20
epoch [30/200] batch [1/1] time 0.995 (0.995) data 0.778 (0.778) loss 0.5820 (0.5820) acc 87.5000 (87.5000) lr 1.8980e-03 eta 0:02:49
epoch [31/200] batch [1/1] time 1.011 (1.011) data 0.810 (0.810) loss 0.4241 (0.4241) acc 90.6250 (90.6250) lr 1.8910e-03 eta 0:02:50
epoch [32/200] batch [1/1] time 1.004 (1.004) data 0.795 (0.795) loss 0.4236 (0.4236) acc 90.6250 (90.6250) lr 1.8838e-03 eta 0:02:48
epoch [33/200] batch [1/1] time 1.024 (1.024) data 0.825 (0.825) loss 0.4744 (0.4744) acc 90.6250 (90.6250) lr 1.8763e-03 eta 0:02:51
epoch [34/200] batch [1/1] time 1.011 (1.011) data 0.790 (0.790) loss 0.5244 (0.5244) acc 81.2500 (81.2500) lr 1.8686e-03 eta 0:02:47
epoch [35/200] batch [1/1] time 0.964 (0.964) data 0.789 (0.789) loss 0.9219 (0.9219) acc 75.0000 (75.0000) lr 1.8607e-03 eta 0:02:39
epoch [36/200] batch [1/1] time 0.987 (0.987) data 0.772 (0.772) loss 0.4998 (0.4998) acc 81.2500 (81.2500) lr 1.8526e-03 eta 0:02:41
epoch [37/200] batch [1/1] time 1.030 (1.030) data 0.815 (0.815) loss 0.4138 (0.4138) acc 87.5000 (87.5000) lr 1.8443e-03 eta 0:02:47
epoch [38/200] batch [1/1] time 1.008 (1.008) data 0.792 (0.792) loss 0.6270 (0.6270) acc 78.1250 (78.1250) lr 1.8358e-03 eta 0:02:43
epoch [39/200] batch [1/1] time 1.004 (1.004) data 0.799 (0.799) loss 0.5474 (0.5474) acc 84.3750 (84.3750) lr 1.8271e-03 eta 0:02:41
epoch [40/200] batch [1/1] time 1.002 (1.002) data 0.801 (0.801) loss 0.3477 (0.3477) acc 96.8750 (96.8750) lr 1.8181e-03 eta 0:02:40
epoch [41/200] batch [1/1] time 0.993 (0.993) data 0.790 (0.790) loss 0.6211 (0.6211) acc 84.3750 (84.3750) lr 1.8090e-03 eta 0:02:37
epoch [42/200] batch [1/1] time 0.987 (0.987) data 0.794 (0.794) loss 0.3398 (0.3398) acc 87.5000 (87.5000) lr 1.7997e-03 eta 0:02:35
epoch [43/200] batch [1/1] time 0.990 (0.990) data 0.787 (0.787) loss 0.3464 (0.3464) acc 93.7500 (93.7500) lr 1.7902e-03 eta 0:02:35
epoch [44/200] batch [1/1] time 1.067 (1.067) data 0.875 (0.875) loss 0.4128 (0.4128) acc 87.5000 (87.5000) lr 1.7804e-03 eta 0:02:46
epoch [45/200] batch [1/1] time 0.969 (0.969) data 0.774 (0.774) loss 0.2323 (0.2323) acc 96.8750 (96.8750) lr 1.7705e-03 eta 0:02:30
epoch [46/200] batch [1/1] time 1.120 (1.120) data 0.915 (0.915) loss 0.4243 (0.4243) acc 87.5000 (87.5000) lr 1.7604e-03 eta 0:02:52
epoch [47/200] batch [1/1] time 1.002 (1.002) data 0.788 (0.788) loss 0.3699 (0.3699) acc 84.3750 (84.3750) lr 1.7501e-03 eta 0:02:33
epoch [48/200] batch [1/1] time 1.003 (1.003) data 0.800 (0.800) loss 0.3296 (0.3296) acc 87.5000 (87.5000) lr 1.7396e-03 eta 0:02:32
epoch [49/200] batch [1/1] time 1.021 (1.021) data 0.804 (0.804) loss 0.4966 (0.4966) acc 87.5000 (87.5000) lr 1.7290e-03 eta 0:02:34
epoch [50/200] batch [1/1] time 0.983 (0.983) data 0.782 (0.782) loss 0.2722 (0.2722) acc 93.7500 (93.7500) lr 1.7181e-03 eta 0:02:27
epoch [51/200] batch [1/1] time 0.989 (0.989) data 0.778 (0.778) loss 0.5674 (0.5674) acc 81.2500 (81.2500) lr 1.7071e-03 eta 0:02:27
epoch [52/200] batch [1/1] time 0.987 (0.987) data 0.770 (0.770) loss 0.3989 (0.3989) acc 87.5000 (87.5000) lr 1.6959e-03 eta 0:02:26
epoch [53/200] batch [1/1] time 1.087 (1.087) data 0.879 (0.879) loss 0.5308 (0.5308) acc 81.2500 (81.2500) lr 1.6845e-03 eta 0:02:39
epoch [54/200] batch [1/1] time 1.011 (1.011) data 0.821 (0.821) loss 0.4380 (0.4380) acc 90.6250 (90.6250) lr 1.6730e-03 eta 0:02:27
epoch [55/200] batch [1/1] time 1.069 (1.069) data 0.879 (0.879) loss 0.5312 (0.5312) acc 87.5000 (87.5000) lr 1.6613e-03 eta 0:02:34
epoch [56/200] batch [1/1] time 1.003 (1.003) data 0.801 (0.801) loss 0.4126 (0.4126) acc 87.5000 (87.5000) lr 1.6494e-03 eta 0:02:24
epoch [57/200] batch [1/1] time 0.991 (0.991) data 0.792 (0.792) loss 0.3645 (0.3645) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:02:21
epoch [58/200] batch [1/1] time 0.966 (0.966) data 0.772 (0.772) loss 0.4436 (0.4436) acc 87.5000 (87.5000) lr 1.6252e-03 eta 0:02:17
epoch [59/200] batch [1/1] time 0.997 (0.997) data 0.793 (0.793) loss 0.4312 (0.4312) acc 87.5000 (87.5000) lr 1.6129e-03 eta 0:02:20
epoch [60/200] batch [1/1] time 1.089 (1.089) data 0.880 (0.880) loss 0.4514 (0.4514) acc 87.5000 (87.5000) lr 1.6004e-03 eta 0:02:32
epoch [61/200] batch [1/1] time 1.064 (1.064) data 0.861 (0.861) loss 0.2759 (0.2759) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:02:27
epoch [62/200] batch [1/1] time 0.991 (0.991) data 0.791 (0.791) loss 0.2756 (0.2756) acc 90.6250 (90.6250) lr 1.5750e-03 eta 0:02:16
epoch [63/200] batch [1/1] time 1.003 (1.003) data 0.806 (0.806) loss 0.3396 (0.3396) acc 87.5000 (87.5000) lr 1.5621e-03 eta 0:02:17
epoch [64/200] batch [1/1] time 1.008 (1.008) data 0.793 (0.793) loss 0.3284 (0.3284) acc 90.6250 (90.6250) lr 1.5490e-03 eta 0:02:17
epoch [65/200] batch [1/1] time 0.973 (0.973) data 0.797 (0.797) loss 0.3491 (0.3491) acc 81.2500 (81.2500) lr 1.5358e-03 eta 0:02:11
epoch [66/200] batch [1/1] time 0.980 (0.980) data 0.802 (0.802) loss 0.3623 (0.3623) acc 87.5000 (87.5000) lr 1.5225e-03 eta 0:02:11
epoch [67/200] batch [1/1] time 1.011 (1.011) data 0.798 (0.798) loss 0.2903 (0.2903) acc 93.7500 (93.7500) lr 1.5090e-03 eta 0:02:14
epoch [68/200] batch [1/1] time 1.011 (1.011) data 0.803 (0.803) loss 0.4082 (0.4082) acc 84.3750 (84.3750) lr 1.4955e-03 eta 0:02:13
epoch [69/200] batch [1/1] time 0.970 (0.970) data 0.775 (0.775) loss 0.3101 (0.3101) acc 90.6250 (90.6250) lr 1.4818e-03 eta 0:02:07
epoch [70/200] batch [1/1] time 0.953 (0.953) data 0.763 (0.763) loss 0.3967 (0.3967) acc 90.6250 (90.6250) lr 1.4679e-03 eta 0:02:03
epoch [71/200] batch [1/1] time 0.948 (0.948) data 0.751 (0.751) loss 0.4158 (0.4158) acc 87.5000 (87.5000) lr 1.4540e-03 eta 0:02:02
epoch [72/200] batch [1/1] time 1.011 (1.011) data 0.808 (0.808) loss 0.4221 (0.4221) acc 84.3750 (84.3750) lr 1.4399e-03 eta 0:02:09
epoch [73/200] batch [1/1] time 1.016 (1.016) data 0.800 (0.800) loss 0.1475 (0.1475) acc 96.8750 (96.8750) lr 1.4258e-03 eta 0:02:08
epoch [74/200] batch [1/1] time 1.011 (1.011) data 0.796 (0.796) loss 0.5166 (0.5166) acc 84.3750 (84.3750) lr 1.4115e-03 eta 0:02:07
epoch [75/200] batch [1/1] time 1.194 (1.194) data 0.815 (0.815) loss 0.4868 (0.4868) acc 81.2500 (81.2500) lr 1.3971e-03 eta 0:02:29
epoch [76/200] batch [1/1] time 0.980 (0.980) data 0.783 (0.783) loss 0.2776 (0.2776) acc 90.6250 (90.6250) lr 1.3827e-03 eta 0:02:01
epoch [77/200] batch [1/1] time 1.045 (1.045) data 0.828 (0.828) loss 0.4050 (0.4050) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:02:08
epoch [78/200] batch [1/1] time 0.984 (0.984) data 0.791 (0.791) loss 0.3137 (0.3137) acc 90.6250 (90.6250) lr 1.3535e-03 eta 0:02:00
epoch [79/200] batch [1/1] time 0.993 (0.993) data 0.803 (0.803) loss 0.3694 (0.3694) acc 93.7500 (93.7500) lr 1.3387e-03 eta 0:02:00
epoch [80/200] batch [1/1] time 1.003 (1.003) data 0.805 (0.805) loss 0.4958 (0.4958) acc 84.3750 (84.3750) lr 1.3239e-03 eta 0:02:00
epoch [81/200] batch [1/1] time 1.006 (1.006) data 0.794 (0.794) loss 0.2700 (0.2700) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:01:59
epoch [82/200] batch [1/1] time 1.014 (1.014) data 0.801 (0.801) loss 0.3257 (0.3257) acc 87.5000 (87.5000) lr 1.2940e-03 eta 0:01:59
epoch [83/200] batch [1/1] time 1.015 (1.015) data 0.797 (0.797) loss 0.3389 (0.3389) acc 84.3750 (84.3750) lr 1.2790e-03 eta 0:01:58
epoch [84/200] batch [1/1] time 1.006 (1.006) data 0.808 (0.808) loss 0.3271 (0.3271) acc 90.6250 (90.6250) lr 1.2639e-03 eta 0:01:56
epoch [85/200] batch [1/1] time 1.003 (1.003) data 0.789 (0.789) loss 0.2627 (0.2627) acc 93.7500 (93.7500) lr 1.2487e-03 eta 0:01:55
epoch [86/200] batch [1/1] time 1.004 (1.004) data 0.794 (0.794) loss 0.3008 (0.3008) acc 84.3750 (84.3750) lr 1.2334e-03 eta 0:01:54
epoch [87/200] batch [1/1] time 1.008 (1.008) data 0.792 (0.792) loss 0.1589 (0.1589) acc 96.8750 (96.8750) lr 1.2181e-03 eta 0:01:53
epoch [88/200] batch [1/1] time 1.002 (1.002) data 0.787 (0.787) loss 0.2698 (0.2698) acc 87.5000 (87.5000) lr 1.2028e-03 eta 0:01:52
epoch [89/200] batch [1/1] time 1.011 (1.011) data 0.798 (0.798) loss 0.3135 (0.3135) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:01:52
epoch [90/200] batch [1/1] time 1.001 (1.001) data 0.804 (0.804) loss 0.1937 (0.1937) acc 96.8750 (96.8750) lr 1.1719e-03 eta 0:01:50
epoch [91/200] batch [1/1] time 1.020 (1.020) data 0.807 (0.807) loss 0.2783 (0.2783) acc 90.6250 (90.6250) lr 1.1564e-03 eta 0:01:51
epoch [92/200] batch [1/1] time 0.992 (0.992) data 0.788 (0.788) loss 0.3357 (0.3357) acc 87.5000 (87.5000) lr 1.1409e-03 eta 0:01:47
epoch [93/200] batch [1/1] time 1.003 (1.003) data 0.801 (0.801) loss 0.4258 (0.4258) acc 87.5000 (87.5000) lr 1.1253e-03 eta 0:01:47
epoch [94/200] batch [1/1] time 0.993 (0.993) data 0.789 (0.789) loss 0.2112 (0.2112) acc 93.7500 (93.7500) lr 1.1097e-03 eta 0:01:45
epoch [95/200] batch [1/1] time 1.144 (1.144) data 0.930 (0.930) loss 0.4490 (0.4490) acc 81.2500 (81.2500) lr 1.0941e-03 eta 0:02:00
epoch [96/200] batch [1/1] time 1.014 (1.014) data 0.801 (0.801) loss 0.3005 (0.3005) acc 93.7500 (93.7500) lr 1.0785e-03 eta 0:01:45
epoch [97/200] batch [1/1] time 0.979 (0.979) data 0.789 (0.789) loss 0.2612 (0.2612) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:01:40
epoch [98/200] batch [1/1] time 1.003 (1.003) data 0.792 (0.792) loss 0.3379 (0.3379) acc 87.5000 (87.5000) lr 1.0471e-03 eta 0:01:42
epoch [99/200] batch [1/1] time 1.075 (1.075) data 0.879 (0.879) loss 0.2194 (0.2194) acc 96.8750 (96.8750) lr 1.0314e-03 eta 0:01:48
epoch [100/200] batch [1/1] time 0.985 (0.985) data 0.794 (0.794) loss 0.3628 (0.3628) acc 87.5000 (87.5000) lr 1.0157e-03 eta 0:01:38
epoch [101/200] batch [1/1] time 0.964 (0.964) data 0.782 (0.782) loss 0.3611 (0.3611) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:01:35
epoch [102/200] batch [1/1] time 0.999 (0.999) data 0.787 (0.787) loss 0.2891 (0.2891) acc 87.5000 (87.5000) lr 9.8429e-04 eta 0:01:37
epoch [103/200] batch [1/1] time 0.962 (0.962) data 0.785 (0.785) loss 0.2029 (0.2029) acc 96.8750 (96.8750) lr 9.6859e-04 eta 0:01:33
epoch [104/200] batch [1/1] time 1.001 (1.001) data 0.790 (0.790) loss 0.1835 (0.1835) acc 93.7500 (93.7500) lr 9.5289e-04 eta 0:01:36
epoch [105/200] batch [1/1] time 1.007 (1.007) data 0.803 (0.803) loss 0.2534 (0.2534) acc 93.7500 (93.7500) lr 9.3721e-04 eta 0:01:35
epoch [106/200] batch [1/1] time 0.993 (0.993) data 0.789 (0.789) loss 0.2507 (0.2507) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:01:33
epoch [107/200] batch [1/1] time 1.018 (1.018) data 0.797 (0.797) loss 0.2756 (0.2756) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:01:34
epoch [108/200] batch [1/1] time 1.008 (1.008) data 0.798 (0.798) loss 0.4473 (0.4473) acc 90.6250 (90.6250) lr 8.9027e-04 eta 0:01:32
epoch [109/200] batch [1/1] time 0.980 (0.980) data 0.777 (0.777) loss 0.2418 (0.2418) acc 96.8750 (96.8750) lr 8.7467e-04 eta 0:01:29
epoch [110/200] batch [1/1] time 0.991 (0.991) data 0.802 (0.802) loss 0.2251 (0.2251) acc 96.8750 (96.8750) lr 8.5910e-04 eta 0:01:29
epoch [111/200] batch [1/1] time 1.003 (1.003) data 0.786 (0.786) loss 0.1670 (0.1670) acc 96.8750 (96.8750) lr 8.4357e-04 eta 0:01:29
epoch [112/200] batch [1/1] time 0.969 (0.969) data 0.783 (0.783) loss 0.1486 (0.1486) acc 100.0000 (100.0000) lr 8.2807e-04 eta 0:01:25
epoch [113/200] batch [1/1] time 0.994 (0.994) data 0.795 (0.795) loss 0.1793 (0.1793) acc 93.7500 (93.7500) lr 8.1262e-04 eta 0:01:26
epoch [114/200] batch [1/1] time 1.009 (1.009) data 0.796 (0.796) loss 0.2266 (0.2266) acc 96.8750 (96.8750) lr 7.9721e-04 eta 0:01:26
epoch [115/200] batch [1/1] time 1.024 (1.024) data 0.810 (0.810) loss 0.2314 (0.2314) acc 96.8750 (96.8750) lr 7.8186e-04 eta 0:01:27
epoch [116/200] batch [1/1] time 0.995 (0.995) data 0.792 (0.792) loss 0.1600 (0.1600) acc 96.8750 (96.8750) lr 7.6655e-04 eta 0:01:23
epoch [117/200] batch [1/1] time 1.031 (1.031) data 0.818 (0.818) loss 0.2407 (0.2407) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:01:25
epoch [118/200] batch [1/1] time 0.999 (0.999) data 0.782 (0.782) loss 0.2252 (0.2252) acc 93.7500 (93.7500) lr 7.3613e-04 eta 0:01:21
epoch [119/200] batch [1/1] time 1.056 (1.056) data 0.837 (0.837) loss 0.2629 (0.2629) acc 93.7500 (93.7500) lr 7.2101e-04 eta 0:01:25
epoch [120/200] batch [1/1] time 1.008 (1.008) data 0.797 (0.797) loss 0.2402 (0.2402) acc 90.6250 (90.6250) lr 7.0596e-04 eta 0:01:20
epoch [121/200] batch [1/1] time 1.004 (1.004) data 0.790 (0.790) loss 0.2151 (0.2151) acc 100.0000 (100.0000) lr 6.9098e-04 eta 0:01:19
epoch [122/200] batch [1/1] time 1.005 (1.005) data 0.790 (0.790) loss 0.2568 (0.2568) acc 90.6250 (90.6250) lr 6.7608e-04 eta 0:01:18
epoch [123/200] batch [1/1] time 0.995 (0.995) data 0.796 (0.796) loss 0.1445 (0.1445) acc 96.8750 (96.8750) lr 6.6126e-04 eta 0:01:16
epoch [124/200] batch [1/1] time 0.961 (0.961) data 0.788 (0.788) loss 0.1942 (0.1942) acc 93.7500 (93.7500) lr 6.4653e-04 eta 0:01:13
epoch [125/200] batch [1/1] time 0.981 (0.981) data 0.787 (0.787) loss 0.2157 (0.2157) acc 93.7500 (93.7500) lr 6.3188e-04 eta 0:01:13
epoch [126/200] batch [1/1] time 1.041 (1.041) data 0.829 (0.829) loss 0.1588 (0.1588) acc 96.8750 (96.8750) lr 6.1732e-04 eta 0:01:17
epoch [127/200] batch [1/1] time 1.120 (1.120) data 0.903 (0.903) loss 0.1482 (0.1482) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:01:21
epoch [128/200] batch [1/1] time 1.030 (1.030) data 0.817 (0.817) loss 0.1394 (0.1394) acc 96.8750 (96.8750) lr 5.8849e-04 eta 0:01:14
epoch [129/200] batch [1/1] time 1.003 (1.003) data 0.791 (0.791) loss 0.2034 (0.2034) acc 96.8750 (96.8750) lr 5.7422e-04 eta 0:01:11
epoch [130/200] batch [1/1] time 0.994 (0.994) data 0.795 (0.795) loss 0.2482 (0.2482) acc 90.6250 (90.6250) lr 5.6006e-04 eta 0:01:09
epoch [131/200] batch [1/1] time 1.006 (1.006) data 0.797 (0.797) loss 0.0839 (0.0839) acc 100.0000 (100.0000) lr 5.4601e-04 eta 0:01:09
epoch [132/200] batch [1/1] time 0.963 (0.963) data 0.788 (0.788) loss 0.2418 (0.2418) acc 93.7500 (93.7500) lr 5.3207e-04 eta 0:01:05
epoch [133/200] batch [1/1] time 1.000 (1.000) data 0.789 (0.789) loss 0.1749 (0.1749) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:01:07
epoch [134/200] batch [1/1] time 1.011 (1.011) data 0.796 (0.796) loss 0.2040 (0.2040) acc 96.8750 (96.8750) lr 5.0454e-04 eta 0:01:06
epoch [135/200] batch [1/1] time 1.109 (1.109) data 0.893 (0.893) loss 0.1494 (0.1494) acc 100.0000 (100.0000) lr 4.9096e-04 eta 0:01:12
epoch [136/200] batch [1/1] time 0.997 (0.997) data 0.786 (0.786) loss 0.2250 (0.2250) acc 93.7500 (93.7500) lr 4.7750e-04 eta 0:01:03
epoch [137/200] batch [1/1] time 0.984 (0.984) data 0.790 (0.790) loss 0.2551 (0.2551) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:01:01
epoch [138/200] batch [1/1] time 1.006 (1.006) data 0.794 (0.794) loss 0.2605 (0.2605) acc 93.7500 (93.7500) lr 4.5098e-04 eta 0:01:02
epoch [139/200] batch [1/1] time 1.036 (1.036) data 0.820 (0.820) loss 0.1693 (0.1693) acc 93.7500 (93.7500) lr 4.3792e-04 eta 0:01:03
epoch [140/200] batch [1/1] time 0.995 (0.995) data 0.795 (0.795) loss 0.1945 (0.1945) acc 93.7500 (93.7500) lr 4.2499e-04 eta 0:00:59
epoch [141/200] batch [1/1] time 1.008 (1.008) data 0.793 (0.793) loss 0.2142 (0.2142) acc 96.8750 (96.8750) lr 4.1221e-04 eta 0:00:59
epoch [142/200] batch [1/1] time 1.006 (1.006) data 0.788 (0.788) loss 0.2245 (0.2245) acc 90.6250 (90.6250) lr 3.9958e-04 eta 0:00:58
epoch [143/200] batch [1/1] time 1.015 (1.015) data 0.799 (0.799) loss 0.1963 (0.1963) acc 93.7500 (93.7500) lr 3.8709e-04 eta 0:00:57
epoch [144/200] batch [1/1] time 1.025 (1.025) data 0.805 (0.805) loss 0.2290 (0.2290) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:00:57
epoch [145/200] batch [1/1] time 1.009 (1.009) data 0.804 (0.804) loss 0.2104 (0.2104) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:00:55
epoch [146/200] batch [1/1] time 1.039 (1.039) data 0.826 (0.826) loss 0.1769 (0.1769) acc 96.8750 (96.8750) lr 3.5055e-04 eta 0:00:56
epoch [147/200] batch [1/1] time 1.018 (1.018) data 0.818 (0.818) loss 0.1320 (0.1320) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:00:53
epoch [148/200] batch [1/1] time 0.966 (0.966) data 0.788 (0.788) loss 0.1628 (0.1628) acc 100.0000 (100.0000) lr 3.2699e-04 eta 0:00:50
epoch [149/200] batch [1/1] time 0.986 (0.986) data 0.788 (0.788) loss 0.2078 (0.2078) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:00:50
epoch [150/200] batch [1/1] time 0.987 (0.987) data 0.805 (0.805) loss 0.4133 (0.4133) acc 87.5000 (87.5000) lr 3.0409e-04 eta 0:00:49
epoch [151/200] batch [1/1] time 1.010 (1.010) data 0.796 (0.796) loss 0.1892 (0.1892) acc 96.8750 (96.8750) lr 2.9289e-04 eta 0:00:49
epoch [152/200] batch [1/1] time 1.000 (1.000) data 0.791 (0.791) loss 0.1542 (0.1542) acc 96.8750 (96.8750) lr 2.8187e-04 eta 0:00:48
epoch [153/200] batch [1/1] time 0.989 (0.989) data 0.796 (0.796) loss 0.2428 (0.2428) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:00:46
epoch [154/200] batch [1/1] time 1.011 (1.011) data 0.793 (0.793) loss 0.2556 (0.2556) acc 93.7500 (93.7500) lr 2.6037e-04 eta 0:00:46
epoch [155/200] batch [1/1] time 0.998 (0.998) data 0.796 (0.796) loss 0.1908 (0.1908) acc 96.8750 (96.8750) lr 2.4989e-04 eta 0:00:44
epoch [156/200] batch [1/1] time 1.103 (1.103) data 0.911 (0.911) loss 0.2422 (0.2422) acc 93.7500 (93.7500) lr 2.3959e-04 eta 0:00:48
epoch [157/200] batch [1/1] time 1.054 (1.054) data 0.859 (0.859) loss 0.2644 (0.2644) acc 96.8750 (96.8750) lr 2.2949e-04 eta 0:00:45
epoch [158/200] batch [1/1] time 1.093 (1.093) data 0.890 (0.890) loss 0.1478 (0.1478) acc 100.0000 (100.0000) lr 2.1957e-04 eta 0:00:45
epoch [159/200] batch [1/1] time 1.124 (1.124) data 0.918 (0.918) loss 0.1266 (0.1266) acc 96.8750 (96.8750) lr 2.0984e-04 eta 0:00:46
epoch [160/200] batch [1/1] time 0.985 (0.985) data 0.791 (0.791) loss 0.1332 (0.1332) acc 96.8750 (96.8750) lr 2.0032e-04 eta 0:00:39
epoch [161/200] batch [1/1] time 1.021 (1.021) data 0.809 (0.809) loss 0.1294 (0.1294) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:00:39
epoch [162/200] batch [1/1] time 1.000 (1.000) data 0.810 (0.810) loss 0.2407 (0.2407) acc 90.6250 (90.6250) lr 1.8185e-04 eta 0:00:38
epoch [163/200] batch [1/1] time 1.016 (1.016) data 0.802 (0.802) loss 0.1743 (0.1743) acc 96.8750 (96.8750) lr 1.7292e-04 eta 0:00:37
epoch [164/200] batch [1/1] time 1.098 (1.098) data 0.880 (0.880) loss 0.1666 (0.1666) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:00:39
epoch [165/200] batch [1/1] time 1.020 (1.020) data 0.807 (0.807) loss 0.4041 (0.4041) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:00:35
epoch [166/200] batch [1/1] time 0.999 (0.999) data 0.807 (0.807) loss 0.3091 (0.3091) acc 90.6250 (90.6250) lr 1.4736e-04 eta 0:00:33
epoch [167/200] batch [1/1] time 0.991 (0.991) data 0.780 (0.780) loss 0.2529 (0.2529) acc 93.7500 (93.7500) lr 1.3926e-04 eta 0:00:32
epoch [168/200] batch [1/1] time 0.995 (0.995) data 0.790 (0.790) loss 0.2463 (0.2463) acc 87.5000 (87.5000) lr 1.3137e-04 eta 0:00:31
epoch [169/200] batch [1/1] time 1.108 (1.108) data 0.892 (0.892) loss 0.1898 (0.1898) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:00:34
epoch [170/200] batch [1/1] time 1.002 (1.002) data 0.794 (0.794) loss 0.2522 (0.2522) acc 96.8750 (96.8750) lr 1.1623e-04 eta 0:00:30
epoch [171/200] batch [1/1] time 1.008 (1.008) data 0.801 (0.801) loss 0.3503 (0.3503) acc 90.6250 (90.6250) lr 1.0899e-04 eta 0:00:29
epoch [172/200] batch [1/1] time 0.986 (0.986) data 0.794 (0.794) loss 0.1447 (0.1447) acc 100.0000 (100.0000) lr 1.0197e-04 eta 0:00:27
epoch [173/200] batch [1/1] time 1.016 (1.016) data 0.806 (0.806) loss 0.2368 (0.2368) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:00:27
epoch [174/200] batch [1/1] time 0.981 (0.981) data 0.787 (0.787) loss 0.2703 (0.2703) acc 93.7500 (93.7500) lr 8.8597e-05 eta 0:00:25
epoch [175/200] batch [1/1] time 1.016 (1.016) data 0.803 (0.803) loss 0.3887 (0.3887) acc 87.5000 (87.5000) lr 8.2245e-05 eta 0:00:25
epoch [176/200] batch [1/1] time 1.007 (1.007) data 0.794 (0.794) loss 0.2747 (0.2747) acc 90.6250 (90.6250) lr 7.6120e-05 eta 0:00:24
epoch [177/200] batch [1/1] time 1.021 (1.021) data 0.812 (0.812) loss 0.1849 (0.1849) acc 100.0000 (100.0000) lr 7.0224e-05 eta 0:00:23
epoch [178/200] batch [1/1] time 1.083 (1.083) data 0.871 (0.871) loss 0.1718 (0.1718) acc 96.8750 (96.8750) lr 6.4556e-05 eta 0:00:23
epoch [179/200] batch [1/1] time 0.947 (0.947) data 0.770 (0.770) loss 0.1597 (0.1597) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:00:19
epoch [180/200] batch [1/1] time 1.023 (1.023) data 0.825 (0.825) loss 0.1891 (0.1891) acc 93.7500 (93.7500) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [1/1] time 0.988 (0.988) data 0.795 (0.795) loss 0.1404 (0.1404) acc 100.0000 (100.0000) lr 4.8943e-05 eta 0:00:18
epoch [182/200] batch [1/1] time 1.021 (1.021) data 0.804 (0.804) loss 0.1951 (0.1951) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [1/1] time 1.009 (1.009) data 0.797 (0.797) loss 0.1825 (0.1825) acc 96.8750 (96.8750) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [1/1] time 1.089 (1.089) data 0.918 (0.918) loss 0.2435 (0.2435) acc 93.7500 (93.7500) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [1/1] time 0.976 (0.976) data 0.800 (0.800) loss 0.1489 (0.1489) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [1/1] time 1.030 (1.030) data 0.817 (0.817) loss 0.2007 (0.2007) acc 93.7500 (93.7500) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [1/1] time 1.006 (1.006) data 0.791 (0.791) loss 0.2411 (0.2411) acc 93.7500 (93.7500) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [1/1] time 0.987 (0.987) data 0.784 (0.784) loss 0.1531 (0.1531) acc 100.0000 (100.0000) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [1/1] time 0.981 (0.981) data 0.788 (0.788) loss 0.2478 (0.2478) acc 87.5000 (87.5000) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [1/1] time 1.009 (1.009) data 0.796 (0.796) loss 0.1766 (0.1766) acc 96.8750 (96.8750) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [1/1] time 0.994 (0.994) data 0.791 (0.791) loss 0.1327 (0.1327) acc 96.8750 (96.8750) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [1/1] time 1.078 (1.078) data 0.888 (0.888) loss 0.1628 (0.1628) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [1/1] time 0.976 (0.976) data 0.787 (0.787) loss 0.2325 (0.2325) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [1/1] time 1.024 (1.024) data 0.810 (0.810) loss 0.2391 (0.2391) acc 93.7500 (93.7500) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [1/1] time 0.944 (0.944) data 0.747 (0.747) loss 0.1630 (0.1630) acc 96.8750 (96.8750) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [1/1] time 0.958 (0.958) data 0.775 (0.775) loss 0.3271 (0.3271) acc 90.6250 (90.6250) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [1/1] time 0.959 (0.959) data 0.767 (0.767) loss 0.1874 (0.1874) acc 100.0000 (100.0000) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [1/1] time 0.978 (0.978) data 0.789 (0.789) loss 0.4138 (0.4138) acc 87.5000 (87.5000) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [1/1] time 1.023 (1.023) data 0.806 (0.806) loss 0.2922 (0.2922) acc 90.6250 (90.6250) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [1/1] time 1.007 (1.007) data 0.800 (0.800) loss 0.1731 (0.1731) acc 93.7500 (93.7500) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,100
* correct: 6,321
* accuracy: 78.0%
* error: 22.0%
* macro_f1: 77.5%
training time for 4-th round: 4689.95 seconds
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
| Calculating uncertainty of Unlabeled set
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
INITIALIZE the prompts weights
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
['X X X X X X X X X X X X X X X X Annual Crop Land.', 'X X X X X X X X X X X X X X X X Forest.', 'X X X X X X X X X X X X X X X X Herbaceous Vegetation Land.', 'X X X X X X X X X X X X X X X X Highway or Road.', 'X X X X X X X X X X X X X X X X Industrial Buildings.', 'X X X X X X X X X X X X X X X X Pasture Land.', 'X X X X X X X X X X X X X X X X Permanent Crop Land.', 'X X X X X X X X X X X X X X X X Residential Buildings.', 'X X X X X X X X X X X X X X X X River.', 'X X X X X X X X X X X X X X X X Sea or Lake.']
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CustomCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
Turning off gradients in both the image and the text encoder
Multiple GPUs detected (n_gpus=2), use all of them!
DataParallel(
  (module): CustomCLIP(
    (prompt_learner): PromptLearner()
    (image_encoder): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
epoch [1/200] batch [1/1] time 0.972 (0.972) data 0.780 (0.780) loss 3.3574 (3.3574) acc 12.5000 (12.5000) lr 2.0000e-03 eta 0:03:13
epoch [2/200] batch [1/1] time 0.991 (0.991) data 0.780 (0.780) loss 3.0859 (3.0859) acc 15.6250 (15.6250) lr 1.9999e-03 eta 0:03:16
epoch [3/200] batch [1/1] time 0.990 (0.990) data 0.785 (0.785) loss 2.1895 (2.1895) acc 37.5000 (37.5000) lr 1.9995e-03 eta 0:03:15
epoch [4/200] batch [1/1] time 1.089 (1.089) data 0.891 (0.891) loss 2.5254 (2.5254) acc 21.8750 (21.8750) lr 1.9989e-03 eta 0:03:33
epoch [5/200] batch [1/1] time 1.077 (1.077) data 0.882 (0.882) loss 2.3457 (2.3457) acc 37.5000 (37.5000) lr 1.9980e-03 eta 0:03:29
epoch [6/200] batch [1/1] time 1.077 (1.077) data 0.868 (0.868) loss 3.1953 (3.1953) acc 15.6250 (15.6250) lr 1.9969e-03 eta 0:03:29
epoch [7/200] batch [1/1] time 0.986 (0.986) data 0.778 (0.778) loss 2.0957 (2.0957) acc 31.2500 (31.2500) lr 1.9956e-03 eta 0:03:10
epoch [8/200] batch [1/1] time 1.060 (1.060) data 0.850 (0.850) loss 1.6221 (1.6221) acc 50.0000 (50.0000) lr 1.9940e-03 eta 0:03:23
epoch [9/200] batch [1/1] time 1.021 (1.021) data 0.816 (0.816) loss 1.4395 (1.4395) acc 37.5000 (37.5000) lr 1.9921e-03 eta 0:03:15
epoch [10/200] batch [1/1] time 1.002 (1.002) data 0.784 (0.784) loss 1.4707 (1.4707) acc 43.7500 (43.7500) lr 1.9900e-03 eta 0:03:10
epoch [11/200] batch [1/1] time 1.007 (1.007) data 0.792 (0.792) loss 1.7441 (1.7441) acc 43.7500 (43.7500) lr 1.9877e-03 eta 0:03:10
epoch [12/200] batch [1/1] time 1.035 (1.035) data 0.828 (0.828) loss 1.2588 (1.2588) acc 59.3750 (59.3750) lr 1.9851e-03 eta 0:03:14
epoch [13/200] batch [1/1] time 0.984 (0.984) data 0.784 (0.784) loss 1.2227 (1.2227) acc 59.3750 (59.3750) lr 1.9823e-03 eta 0:03:04
epoch [14/200] batch [1/1] time 0.967 (0.967) data 0.769 (0.769) loss 1.3789 (1.3789) acc 50.0000 (50.0000) lr 1.9792e-03 eta 0:02:59
epoch [15/200] batch [1/1] time 1.066 (1.066) data 0.870 (0.870) loss 1.0068 (1.0068) acc 65.6250 (65.6250) lr 1.9759e-03 eta 0:03:17
epoch [16/200] batch [1/1] time 0.979 (0.979) data 0.781 (0.781) loss 0.8364 (0.8364) acc 78.1250 (78.1250) lr 1.9724e-03 eta 0:03:00
epoch [17/200] batch [1/1] time 0.994 (0.994) data 0.792 (0.792) loss 0.8857 (0.8857) acc 65.6250 (65.6250) lr 1.9686e-03 eta 0:03:01
epoch [18/200] batch [1/1] time 0.981 (0.981) data 0.782 (0.782) loss 0.7480 (0.7480) acc 81.2500 (81.2500) lr 1.9646e-03 eta 0:02:58
epoch [19/200] batch [1/1] time 1.053 (1.053) data 0.864 (0.864) loss 0.9761 (0.9761) acc 68.7500 (68.7500) lr 1.9603e-03 eta 0:03:10
epoch [20/200] batch [1/1] time 1.109 (1.109) data 0.901 (0.901) loss 1.0127 (1.0127) acc 62.5000 (62.5000) lr 1.9558e-03 eta 0:03:19
epoch [21/200] batch [1/1] time 1.003 (1.003) data 0.792 (0.792) loss 1.0293 (1.0293) acc 65.6250 (65.6250) lr 1.9511e-03 eta 0:02:59
epoch [22/200] batch [1/1] time 0.963 (0.963) data 0.746 (0.746) loss 0.6523 (0.6523) acc 78.1250 (78.1250) lr 1.9461e-03 eta 0:02:51
epoch [23/200] batch [1/1] time 0.988 (0.988) data 0.771 (0.771) loss 1.0996 (1.0996) acc 65.6250 (65.6250) lr 1.9409e-03 eta 0:02:54
epoch [24/200] batch [1/1] time 1.048 (1.048) data 0.843 (0.843) loss 0.8267 (0.8267) acc 78.1250 (78.1250) lr 1.9354e-03 eta 0:03:04
epoch [25/200] batch [1/1] time 1.021 (1.021) data 0.808 (0.808) loss 0.6719 (0.6719) acc 84.3750 (84.3750) lr 1.9298e-03 eta 0:02:58
epoch [26/200] batch [1/1] time 1.096 (1.096) data 0.878 (0.878) loss 0.5039 (0.5039) acc 90.6250 (90.6250) lr 1.9239e-03 eta 0:03:10
epoch [27/200] batch [1/1] time 0.967 (0.967) data 0.758 (0.758) loss 0.6558 (0.6558) acc 84.3750 (84.3750) lr 1.9178e-03 eta 0:02:47
epoch [28/200] batch [1/1] time 1.061 (1.061) data 0.869 (0.869) loss 0.6514 (0.6514) acc 78.1250 (78.1250) lr 1.9114e-03 eta 0:03:02
epoch [29/200] batch [1/1] time 1.003 (1.003) data 0.786 (0.786) loss 0.7383 (0.7383) acc 84.3750 (84.3750) lr 1.9048e-03 eta 0:02:51
epoch [30/200] batch [1/1] time 1.082 (1.082) data 0.867 (0.867) loss 0.6079 (0.6079) acc 87.5000 (87.5000) lr 1.8980e-03 eta 0:03:03
epoch [31/200] batch [1/1] time 0.994 (0.994) data 0.779 (0.779) loss 0.6357 (0.6357) acc 81.2500 (81.2500) lr 1.8910e-03 eta 0:02:48
epoch [32/200] batch [1/1] time 0.987 (0.987) data 0.794 (0.794) loss 0.5845 (0.5845) acc 81.2500 (81.2500) lr 1.8838e-03 eta 0:02:45
epoch [33/200] batch [1/1] time 1.065 (1.065) data 0.876 (0.876) loss 0.7690 (0.7690) acc 78.1250 (78.1250) lr 1.8763e-03 eta 0:02:57
epoch [34/200] batch [1/1] time 1.006 (1.006) data 0.793 (0.793) loss 0.6812 (0.6812) acc 75.0000 (75.0000) lr 1.8686e-03 eta 0:02:46
epoch [35/200] batch [1/1] time 0.983 (0.983) data 0.779 (0.779) loss 0.7378 (0.7378) acc 75.0000 (75.0000) lr 1.8607e-03 eta 0:02:42
epoch [36/200] batch [1/1] time 1.009 (1.009) data 0.796 (0.796) loss 0.7695 (0.7695) acc 71.8750 (71.8750) lr 1.8526e-03 eta 0:02:45
epoch [37/200] batch [1/1] time 1.004 (1.004) data 0.799 (0.799) loss 0.6538 (0.6538) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:02:43
epoch [38/200] batch [1/1] time 0.990 (0.990) data 0.787 (0.787) loss 0.4282 (0.4282) acc 90.6250 (90.6250) lr 1.8358e-03 eta 0:02:40
epoch [39/200] batch [1/1] time 0.993 (0.993) data 0.777 (0.777) loss 0.3647 (0.3647) acc 90.6250 (90.6250) lr 1.8271e-03 eta 0:02:39
epoch [40/200] batch [1/1] time 0.991 (0.991) data 0.797 (0.797) loss 0.6030 (0.6030) acc 81.2500 (81.2500) lr 1.8181e-03 eta 0:02:38
epoch [41/200] batch [1/1] time 1.006 (1.006) data 0.791 (0.791) loss 0.3848 (0.3848) acc 90.6250 (90.6250) lr 1.8090e-03 eta 0:02:39
epoch [42/200] batch [1/1] time 1.130 (1.130) data 0.761 (0.761) loss 0.6089 (0.6089) acc 81.2500 (81.2500) lr 1.7997e-03 eta 0:02:58
epoch [43/200] batch [1/1] time 1.006 (1.006) data 0.816 (0.816) loss 0.5659 (0.5659) acc 90.6250 (90.6250) lr 1.7902e-03 eta 0:02:37
epoch [44/200] batch [1/1] time 1.078 (1.078) data 0.890 (0.890) loss 0.6743 (0.6743) acc 75.0000 (75.0000) lr 1.7804e-03 eta 0:02:48
epoch [45/200] batch [1/1] time 1.011 (1.011) data 0.802 (0.802) loss 0.5371 (0.5371) acc 81.2500 (81.2500) lr 1.7705e-03 eta 0:02:36
epoch [46/200] batch [1/1] time 1.015 (1.015) data 0.788 (0.788) loss 0.5396 (0.5396) acc 87.5000 (87.5000) lr 1.7604e-03 eta 0:02:36
epoch [47/200] batch [1/1] time 0.980 (0.980) data 0.782 (0.782) loss 0.5562 (0.5562) acc 84.3750 (84.3750) lr 1.7501e-03 eta 0:02:29
epoch [48/200] batch [1/1] time 1.000 (1.000) data 0.784 (0.784) loss 0.5146 (0.5146) acc 84.3750 (84.3750) lr 1.7396e-03 eta 0:02:32
epoch [49/200] batch [1/1] time 0.994 (0.994) data 0.785 (0.785) loss 0.5151 (0.5151) acc 78.1250 (78.1250) lr 1.7290e-03 eta 0:02:30
epoch [50/200] batch [1/1] time 0.999 (0.999) data 0.791 (0.791) loss 0.4609 (0.4609) acc 90.6250 (90.6250) lr 1.7181e-03 eta 0:02:29
epoch [51/200] batch [1/1] time 0.990 (0.990) data 0.781 (0.781) loss 0.5244 (0.5244) acc 84.3750 (84.3750) lr 1.7071e-03 eta 0:02:27
epoch [52/200] batch [1/1] time 0.999 (0.999) data 0.788 (0.788) loss 0.3972 (0.3972) acc 96.8750 (96.8750) lr 1.6959e-03 eta 0:02:27
epoch [53/200] batch [1/1] time 1.022 (1.022) data 0.813 (0.813) loss 0.4614 (0.4614) acc 87.5000 (87.5000) lr 1.6845e-03 eta 0:02:30
epoch [54/200] batch [1/1] time 1.012 (1.012) data 0.795 (0.795) loss 0.3647 (0.3647) acc 90.6250 (90.6250) lr 1.6730e-03 eta 0:02:27
epoch [55/200] batch [1/1] time 0.993 (0.993) data 0.806 (0.806) loss 0.3657 (0.3657) acc 90.6250 (90.6250) lr 1.6613e-03 eta 0:02:23
epoch [56/200] batch [1/1] time 1.002 (1.002) data 0.784 (0.784) loss 0.4443 (0.4443) acc 84.3750 (84.3750) lr 1.6494e-03 eta 0:02:24
epoch [57/200] batch [1/1] time 0.969 (0.969) data 0.786 (0.786) loss 0.4746 (0.4746) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:02:18
epoch [58/200] batch [1/1] time 0.982 (0.982) data 0.779 (0.779) loss 0.3489 (0.3489) acc 96.8750 (96.8750) lr 1.6252e-03 eta 0:02:19
epoch [59/200] batch [1/1] time 0.991 (0.991) data 0.781 (0.781) loss 0.2864 (0.2864) acc 90.6250 (90.6250) lr 1.6129e-03 eta 0:02:19
epoch [60/200] batch [1/1] time 1.069 (1.069) data 0.858 (0.858) loss 0.4358 (0.4358) acc 93.7500 (93.7500) lr 1.6004e-03 eta 0:02:29
epoch [61/200] batch [1/1] time 0.992 (0.992) data 0.785 (0.785) loss 0.2603 (0.2603) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:02:17
epoch [62/200] batch [1/1] time 0.989 (0.989) data 0.778 (0.778) loss 0.3611 (0.3611) acc 90.6250 (90.6250) lr 1.5750e-03 eta 0:02:16
epoch [63/200] batch [1/1] time 0.997 (0.997) data 0.788 (0.788) loss 0.4614 (0.4614) acc 90.6250 (90.6250) lr 1.5621e-03 eta 0:02:16
epoch [64/200] batch [1/1] time 1.020 (1.020) data 0.801 (0.801) loss 0.4465 (0.4465) acc 87.5000 (87.5000) lr 1.5490e-03 eta 0:02:18
epoch [65/200] batch [1/1] time 0.983 (0.983) data 0.785 (0.785) loss 0.5391 (0.5391) acc 81.2500 (81.2500) lr 1.5358e-03 eta 0:02:12
epoch [66/200] batch [1/1] time 0.958 (0.958) data 0.754 (0.754) loss 0.3088 (0.3088) acc 93.7500 (93.7500) lr 1.5225e-03 eta 0:02:08
epoch [67/200] batch [1/1] time 1.075 (1.075) data 0.872 (0.872) loss 0.3997 (0.3997) acc 90.6250 (90.6250) lr 1.5090e-03 eta 0:02:22
epoch [68/200] batch [1/1] time 0.990 (0.990) data 0.782 (0.782) loss 0.3716 (0.3716) acc 93.7500 (93.7500) lr 1.4955e-03 eta 0:02:10
epoch [69/200] batch [1/1] time 0.998 (0.998) data 0.787 (0.787) loss 0.3296 (0.3296) acc 93.7500 (93.7500) lr 1.4818e-03 eta 0:02:10
epoch [70/200] batch [1/1] time 1.016 (1.016) data 0.802 (0.802) loss 0.5811 (0.5811) acc 78.1250 (78.1250) lr 1.4679e-03 eta 0:02:12
epoch [71/200] batch [1/1] time 0.970 (0.970) data 0.759 (0.759) loss 0.3921 (0.3921) acc 90.6250 (90.6250) lr 1.4540e-03 eta 0:02:05
epoch [72/200] batch [1/1] time 0.992 (0.992) data 0.799 (0.799) loss 0.3567 (0.3567) acc 84.3750 (84.3750) lr 1.4399e-03 eta 0:02:07
epoch [73/200] batch [1/1] time 0.984 (0.984) data 0.785 (0.785) loss 0.3726 (0.3726) acc 87.5000 (87.5000) lr 1.4258e-03 eta 0:02:04
epoch [74/200] batch [1/1] time 1.044 (1.044) data 0.866 (0.866) loss 0.3877 (0.3877) acc 87.5000 (87.5000) lr 1.4115e-03 eta 0:02:11
epoch [75/200] batch [1/1] time 0.983 (0.983) data 0.774 (0.774) loss 0.2593 (0.2593) acc 96.8750 (96.8750) lr 1.3971e-03 eta 0:02:02
epoch [76/200] batch [1/1] time 1.021 (1.021) data 0.816 (0.816) loss 0.3621 (0.3621) acc 90.6250 (90.6250) lr 1.3827e-03 eta 0:02:06
epoch [77/200] batch [1/1] time 1.085 (1.085) data 0.872 (0.872) loss 0.4871 (0.4871) acc 90.6250 (90.6250) lr 1.3681e-03 eta 0:02:13
epoch [78/200] batch [1/1] time 0.978 (0.978) data 0.783 (0.783) loss 0.2788 (0.2788) acc 90.6250 (90.6250) lr 1.3535e-03 eta 0:01:59
epoch [79/200] batch [1/1] time 1.163 (1.163) data 0.954 (0.954) loss 0.3884 (0.3884) acc 87.5000 (87.5000) lr 1.3387e-03 eta 0:02:20
epoch [80/200] batch [1/1] time 1.146 (1.146) data 0.945 (0.945) loss 0.3242 (0.3242) acc 90.6250 (90.6250) lr 1.3239e-03 eta 0:02:17
epoch [81/200] batch [1/1] time 0.965 (0.965) data 0.774 (0.774) loss 0.3660 (0.3660) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:01:54
epoch [82/200] batch [1/1] time 1.002 (1.002) data 0.786 (0.786) loss 0.6104 (0.6104) acc 87.5000 (87.5000) lr 1.2940e-03 eta 0:01:58
epoch [83/200] batch [1/1] time 1.008 (1.008) data 0.797 (0.797) loss 0.2306 (0.2306) acc 96.8750 (96.8750) lr 1.2790e-03 eta 0:01:57
epoch [84/200] batch [1/1] time 0.995 (0.995) data 0.791 (0.791) loss 0.3293 (0.3293) acc 90.6250 (90.6250) lr 1.2639e-03 eta 0:01:55
epoch [85/200] batch [1/1] time 1.015 (1.015) data 0.803 (0.803) loss 0.4617 (0.4617) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:01:56
epoch [86/200] batch [1/1] time 0.982 (0.982) data 0.790 (0.790) loss 0.2213 (0.2213) acc 93.7500 (93.7500) lr 1.2334e-03 eta 0:01:51
epoch [87/200] batch [1/1] time 1.053 (1.053) data 0.860 (0.860) loss 0.4138 (0.4138) acc 87.5000 (87.5000) lr 1.2181e-03 eta 0:01:59
epoch [88/200] batch [1/1] time 1.156 (1.156) data 0.781 (0.781) loss 0.3430 (0.3430) acc 87.5000 (87.5000) lr 1.2028e-03 eta 0:02:09
epoch [89/200] batch [1/1] time 0.964 (0.964) data 0.785 (0.785) loss 0.2059 (0.2059) acc 96.8750 (96.8750) lr 1.1874e-03 eta 0:01:46
epoch [90/200] batch [1/1] time 1.006 (1.006) data 0.790 (0.790) loss 0.2119 (0.2119) acc 100.0000 (100.0000) lr 1.1719e-03 eta 0:01:50
epoch [91/200] batch [1/1] time 0.999 (0.999) data 0.786 (0.786) loss 0.1583 (0.1583) acc 96.8750 (96.8750) lr 1.1564e-03 eta 0:01:48
epoch [92/200] batch [1/1] time 0.987 (0.987) data 0.784 (0.784) loss 0.2629 (0.2629) acc 96.8750 (96.8750) lr 1.1409e-03 eta 0:01:46
epoch [93/200] batch [1/1] time 1.018 (1.018) data 0.784 (0.784) loss 0.3384 (0.3384) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:01:48
epoch [94/200] batch [1/1] time 1.119 (1.119) data 0.926 (0.926) loss 0.4001 (0.4001) acc 90.6250 (90.6250) lr 1.1097e-03 eta 0:01:58
epoch [95/200] batch [1/1] time 1.000 (1.000) data 0.791 (0.791) loss 0.2590 (0.2590) acc 96.8750 (96.8750) lr 1.0941e-03 eta 0:01:45
epoch [96/200] batch [1/1] time 1.077 (1.077) data 0.868 (0.868) loss 0.2722 (0.2722) acc 93.7500 (93.7500) lr 1.0785e-03 eta 0:01:52
epoch [97/200] batch [1/1] time 0.996 (0.996) data 0.784 (0.784) loss 0.1973 (0.1973) acc 96.8750 (96.8750) lr 1.0628e-03 eta 0:01:42
epoch [98/200] batch [1/1] time 0.968 (0.968) data 0.775 (0.775) loss 0.2268 (0.2268) acc 96.8750 (96.8750) lr 1.0471e-03 eta 0:01:38
epoch [99/200] batch [1/1] time 0.996 (0.996) data 0.785 (0.785) loss 0.1781 (0.1781) acc 96.8750 (96.8750) lr 1.0314e-03 eta 0:01:40
epoch [100/200] batch [1/1] time 1.012 (1.012) data 0.794 (0.794) loss 0.3020 (0.3020) acc 93.7500 (93.7500) lr 1.0157e-03 eta 0:01:41
epoch [101/200] batch [1/1] time 1.052 (1.052) data 0.870 (0.870) loss 0.2910 (0.2910) acc 93.7500 (93.7500) lr 1.0000e-03 eta 0:01:44
epoch [102/200] batch [1/1] time 1.081 (1.081) data 0.877 (0.877) loss 0.2637 (0.2637) acc 93.7500 (93.7500) lr 9.8429e-04 eta 0:01:45
epoch [103/200] batch [1/1] time 0.964 (0.964) data 0.787 (0.787) loss 0.2861 (0.2861) acc 90.6250 (90.6250) lr 9.6859e-04 eta 0:01:33
epoch [104/200] batch [1/1] time 0.998 (0.998) data 0.823 (0.823) loss 0.2070 (0.2070) acc 96.8750 (96.8750) lr 9.5289e-04 eta 0:01:35
epoch [105/200] batch [1/1] time 0.951 (0.951) data 0.783 (0.783) loss 0.4231 (0.4231) acc 84.3750 (84.3750) lr 9.3721e-04 eta 0:01:30
epoch [106/200] batch [1/1] time 1.002 (1.002) data 0.801 (0.801) loss 0.1567 (0.1567) acc 100.0000 (100.0000) lr 9.2154e-04 eta 0:01:34
epoch [107/200] batch [1/1] time 1.022 (1.022) data 0.805 (0.805) loss 0.3503 (0.3503) acc 90.6250 (90.6250) lr 9.0589e-04 eta 0:01:35
epoch [108/200] batch [1/1] time 1.005 (1.005) data 0.805 (0.805) loss 0.4446 (0.4446) acc 87.5000 (87.5000) lr 8.9027e-04 eta 0:01:32
epoch [109/200] batch [1/1] time 0.985 (0.985) data 0.787 (0.787) loss 0.4558 (0.4558) acc 90.6250 (90.6250) lr 8.7467e-04 eta 0:01:29
epoch [110/200] batch [1/1] time 0.991 (0.991) data 0.790 (0.790) loss 0.3870 (0.3870) acc 84.3750 (84.3750) lr 8.5910e-04 eta 0:01:29
epoch [111/200] batch [1/1] time 0.964 (0.964) data 0.792 (0.792) loss 0.1066 (0.1066) acc 100.0000 (100.0000) lr 8.4357e-04 eta 0:01:25
epoch [112/200] batch [1/1] time 0.970 (0.970) data 0.774 (0.774) loss 0.2134 (0.2134) acc 96.8750 (96.8750) lr 8.2807e-04 eta 0:01:25
epoch [113/200] batch [1/1] time 1.006 (1.006) data 0.795 (0.795) loss 0.2825 (0.2825) acc 90.6250 (90.6250) lr 8.1262e-04 eta 0:01:27
epoch [114/200] batch [1/1] time 1.123 (1.123) data 0.924 (0.924) loss 0.2537 (0.2537) acc 93.7500 (93.7500) lr 7.9721e-04 eta 0:01:36
epoch [115/200] batch [1/1] time 0.969 (0.969) data 0.771 (0.771) loss 0.3201 (0.3201) acc 90.6250 (90.6250) lr 7.8186e-04 eta 0:01:22
epoch [116/200] batch [1/1] time 1.000 (1.000) data 0.788 (0.788) loss 0.3105 (0.3105) acc 93.7500 (93.7500) lr 7.6655e-04 eta 0:01:23
epoch [117/200] batch [1/1] time 0.960 (0.960) data 0.787 (0.787) loss 0.3901 (0.3901) acc 90.6250 (90.6250) lr 7.5131e-04 eta 0:01:19
epoch [118/200] batch [1/1] time 0.988 (0.988) data 0.789 (0.789) loss 0.5542 (0.5542) acc 84.3750 (84.3750) lr 7.3613e-04 eta 0:01:21
epoch [119/200] batch [1/1] time 1.041 (1.041) data 0.857 (0.857) loss 0.2233 (0.2233) acc 96.8750 (96.8750) lr 7.2101e-04 eta 0:01:24
epoch [120/200] batch [1/1] time 1.133 (1.133) data 0.913 (0.913) loss 0.2876 (0.2876) acc 93.7500 (93.7500) lr 7.0596e-04 eta 0:01:30
epoch [121/200] batch [1/1] time 1.013 (1.013) data 0.803 (0.803) loss 0.2358 (0.2358) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:01:20
epoch [122/200] batch [1/1] time 1.024 (1.024) data 0.808 (0.808) loss 0.3259 (0.3259) acc 87.5000 (87.5000) lr 6.7608e-04 eta 0:01:19
epoch [123/200] batch [1/1] time 1.009 (1.009) data 0.790 (0.790) loss 0.1500 (0.1500) acc 100.0000 (100.0000) lr 6.6126e-04 eta 0:01:17
epoch [124/200] batch [1/1] time 1.011 (1.011) data 0.798 (0.798) loss 0.4641 (0.4641) acc 87.5000 (87.5000) lr 6.4653e-04 eta 0:01:16
epoch [125/200] batch [1/1] time 0.999 (0.999) data 0.798 (0.798) loss 0.3342 (0.3342) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:01:14
epoch [126/200] batch [1/1] time 0.958 (0.958) data 0.786 (0.786) loss 0.2866 (0.2866) acc 90.6250 (90.6250) lr 6.1732e-04 eta 0:01:10
epoch [127/200] batch [1/1] time 0.991 (0.991) data 0.784 (0.784) loss 0.3755 (0.3755) acc 90.6250 (90.6250) lr 6.0285e-04 eta 0:01:12
epoch [128/200] batch [1/1] time 1.050 (1.050) data 0.865 (0.865) loss 0.2698 (0.2698) acc 90.6250 (90.6250) lr 5.8849e-04 eta 0:01:15
epoch [129/200] batch [1/1] time 0.994 (0.994) data 0.785 (0.785) loss 0.3489 (0.3489) acc 90.6250 (90.6250) lr 5.7422e-04 eta 0:01:10
epoch [130/200] batch [1/1] time 1.053 (1.053) data 0.842 (0.842) loss 0.1392 (0.1392) acc 100.0000 (100.0000) lr 5.6006e-04 eta 0:01:13
epoch [131/200] batch [1/1] time 1.052 (1.052) data 0.866 (0.866) loss 0.2905 (0.2905) acc 93.7500 (93.7500) lr 5.4601e-04 eta 0:01:12
epoch [132/200] batch [1/1] time 1.005 (1.005) data 0.786 (0.786) loss 0.1641 (0.1641) acc 93.7500 (93.7500) lr 5.3207e-04 eta 0:01:08
epoch [133/200] batch [1/1] time 1.015 (1.015) data 0.798 (0.798) loss 0.2751 (0.2751) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:01:08
epoch [134/200] batch [1/1] time 1.066 (1.066) data 0.844 (0.844) loss 0.4216 (0.4216) acc 87.5000 (87.5000) lr 5.0454e-04 eta 0:01:10
epoch [135/200] batch [1/1] time 0.993 (0.993) data 0.791 (0.791) loss 0.1842 (0.1842) acc 96.8750 (96.8750) lr 4.9096e-04 eta 0:01:04
epoch [136/200] batch [1/1] time 0.988 (0.988) data 0.775 (0.775) loss 0.1385 (0.1385) acc 100.0000 (100.0000) lr 4.7750e-04 eta 0:01:03
epoch [137/200] batch [1/1] time 0.969 (0.969) data 0.773 (0.773) loss 0.3889 (0.3889) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:01:01
epoch [138/200] batch [1/1] time 1.075 (1.075) data 0.863 (0.863) loss 0.3477 (0.3477) acc 90.6250 (90.6250) lr 4.5098e-04 eta 0:01:06
epoch [139/200] batch [1/1] time 0.999 (0.999) data 0.805 (0.805) loss 0.1682 (0.1682) acc 96.8750 (96.8750) lr 4.3792e-04 eta 0:01:00
epoch [140/200] batch [1/1] time 1.022 (1.022) data 0.827 (0.827) loss 0.1975 (0.1975) acc 93.7500 (93.7500) lr 4.2499e-04 eta 0:01:01
epoch [141/200] batch [1/1] time 0.996 (0.996) data 0.787 (0.787) loss 0.3604 (0.3604) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:00:58
epoch [142/200] batch [1/1] time 0.980 (0.980) data 0.799 (0.799) loss 0.5161 (0.5161) acc 84.3750 (84.3750) lr 3.9958e-04 eta 0:00:56
epoch [143/200] batch [1/1] time 1.117 (1.117) data 0.924 (0.924) loss 0.2839 (0.2839) acc 93.7500 (93.7500) lr 3.8709e-04 eta 0:01:03
epoch [144/200] batch [1/1] time 1.014 (1.014) data 0.795 (0.795) loss 0.2651 (0.2651) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:00:56
epoch [145/200] batch [1/1] time 0.993 (0.993) data 0.782 (0.782) loss 0.3315 (0.3315) acc 90.6250 (90.6250) lr 3.6258e-04 eta 0:00:54
epoch [146/200] batch [1/1] time 1.001 (1.001) data 0.789 (0.789) loss 0.3411 (0.3411) acc 87.5000 (87.5000) lr 3.5055e-04 eta 0:00:54
epoch [147/200] batch [1/1] time 0.972 (0.972) data 0.763 (0.763) loss 0.2759 (0.2759) acc 93.7500 (93.7500) lr 3.3869e-04 eta 0:00:51
epoch [148/200] batch [1/1] time 0.984 (0.984) data 0.787 (0.787) loss 0.1503 (0.1503) acc 96.8750 (96.8750) lr 3.2699e-04 eta 0:00:51
epoch [149/200] batch [1/1] time 0.971 (0.971) data 0.778 (0.778) loss 0.3013 (0.3013) acc 90.6250 (90.6250) lr 3.1545e-04 eta 0:00:49
epoch [150/200] batch [1/1] time 1.011 (1.011) data 0.797 (0.797) loss 0.1510 (0.1510) acc 100.0000 (100.0000) lr 3.0409e-04 eta 0:00:50
epoch [151/200] batch [1/1] time 0.964 (0.964) data 0.787 (0.787) loss 0.1538 (0.1538) acc 100.0000 (100.0000) lr 2.9289e-04 eta 0:00:47
epoch [152/200] batch [1/1] time 0.999 (0.999) data 0.786 (0.786) loss 0.2111 (0.2111) acc 96.8750 (96.8750) lr 2.8187e-04 eta 0:00:47
epoch [153/200] batch [1/1] time 0.976 (0.976) data 0.785 (0.785) loss 0.2090 (0.2090) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:00:45
epoch [154/200] batch [1/1] time 1.105 (1.105) data 0.887 (0.887) loss 0.1708 (0.1708) acc 96.8750 (96.8750) lr 2.6037e-04 eta 0:00:50
epoch [155/200] batch [1/1] time 1.016 (1.016) data 0.797 (0.797) loss 0.2783 (0.2783) acc 93.7500 (93.7500) lr 2.4989e-04 eta 0:00:45
epoch [156/200] batch [1/1] time 1.004 (1.004) data 0.790 (0.790) loss 0.1245 (0.1245) acc 100.0000 (100.0000) lr 2.3959e-04 eta 0:00:44
epoch [157/200] batch [1/1] time 0.970 (0.970) data 0.793 (0.793) loss 0.3247 (0.3247) acc 87.5000 (87.5000) lr 2.2949e-04 eta 0:00:41
epoch [158/200] batch [1/1] time 1.007 (1.007) data 0.794 (0.794) loss 0.1986 (0.1986) acc 100.0000 (100.0000) lr 2.1957e-04 eta 0:00:42
epoch [159/200] batch [1/1] time 1.011 (1.011) data 0.797 (0.797) loss 0.1501 (0.1501) acc 100.0000 (100.0000) lr 2.0984e-04 eta 0:00:41
epoch [160/200] batch [1/1] time 1.042 (1.042) data 0.870 (0.870) loss 0.2964 (0.2964) acc 87.5000 (87.5000) lr 2.0032e-04 eta 0:00:41
epoch [161/200] batch [1/1] time 1.003 (1.003) data 0.793 (0.793) loss 0.1709 (0.1709) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:00:39
epoch [162/200] batch [1/1] time 1.006 (1.006) data 0.792 (0.792) loss 0.2942 (0.2942) acc 96.8750 (96.8750) lr 1.8185e-04 eta 0:00:38
epoch [163/200] batch [1/1] time 0.991 (0.991) data 0.791 (0.791) loss 0.2307 (0.2307) acc 96.8750 (96.8750) lr 1.7292e-04 eta 0:00:36
epoch [164/200] batch [1/1] time 0.964 (0.964) data 0.792 (0.792) loss 0.3364 (0.3364) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:00:34
epoch [165/200] batch [1/1] time 0.979 (0.979) data 0.788 (0.788) loss 0.2688 (0.2688) acc 100.0000 (100.0000) lr 1.5567e-04 eta 0:00:34
epoch [166/200] batch [1/1] time 0.995 (0.995) data 0.797 (0.797) loss 0.2900 (0.2900) acc 93.7500 (93.7500) lr 1.4736e-04 eta 0:00:33
epoch [167/200] batch [1/1] time 1.007 (1.007) data 0.805 (0.805) loss 0.2264 (0.2264) acc 96.8750 (96.8750) lr 1.3926e-04 eta 0:00:33
epoch [168/200] batch [1/1] time 1.024 (1.024) data 0.815 (0.815) loss 0.1860 (0.1860) acc 96.8750 (96.8750) lr 1.3137e-04 eta 0:00:32
epoch [169/200] batch [1/1] time 1.004 (1.004) data 0.791 (0.791) loss 0.2690 (0.2690) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:00:31
epoch [170/200] batch [1/1] time 0.966 (0.966) data 0.783 (0.783) loss 0.3005 (0.3005) acc 93.7500 (93.7500) lr 1.1623e-04 eta 0:00:28
epoch [171/200] batch [1/1] time 0.978 (0.978) data 0.790 (0.790) loss 0.1814 (0.1814) acc 100.0000 (100.0000) lr 1.0899e-04 eta 0:00:28
epoch [172/200] batch [1/1] time 0.986 (0.986) data 0.786 (0.786) loss 0.2888 (0.2888) acc 84.3750 (84.3750) lr 1.0197e-04 eta 0:00:27
epoch [173/200] batch [1/1] time 0.993 (0.993) data 0.787 (0.787) loss 0.2380 (0.2380) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:00:26
epoch [174/200] batch [1/1] time 1.007 (1.007) data 0.788 (0.788) loss 0.2444 (0.2444) acc 93.7500 (93.7500) lr 8.8597e-05 eta 0:00:26
epoch [175/200] batch [1/1] time 1.000 (1.000) data 0.809 (0.809) loss 0.2659 (0.2659) acc 93.7500 (93.7500) lr 8.2245e-05 eta 0:00:24
epoch [176/200] batch [1/1] time 0.980 (0.980) data 0.785 (0.785) loss 0.1854 (0.1854) acc 96.8750 (96.8750) lr 7.6120e-05 eta 0:00:23
epoch [177/200] batch [1/1] time 0.983 (0.983) data 0.780 (0.780) loss 0.2262 (0.2262) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:00:22
epoch [178/200] batch [1/1] time 1.001 (1.001) data 0.799 (0.799) loss 0.1768 (0.1768) acc 96.8750 (96.8750) lr 6.4556e-05 eta 0:00:22
epoch [179/200] batch [1/1] time 1.003 (1.003) data 0.803 (0.803) loss 0.2220 (0.2220) acc 90.6250 (90.6250) lr 5.9119e-05 eta 0:00:21
epoch [180/200] batch [1/1] time 1.000 (1.000) data 0.795 (0.795) loss 0.1350 (0.1350) acc 100.0000 (100.0000) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [1/1] time 1.008 (1.008) data 0.803 (0.803) loss 0.2046 (0.2046) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:19
epoch [182/200] batch [1/1] time 1.023 (1.023) data 0.804 (0.804) loss 0.1625 (0.1625) acc 96.8750 (96.8750) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [1/1] time 0.995 (0.995) data 0.782 (0.782) loss 0.2382 (0.2382) acc 93.7500 (93.7500) lr 3.9706e-05 eta 0:00:16
epoch [184/200] batch [1/1] time 0.999 (0.999) data 0.792 (0.792) loss 0.3091 (0.3091) acc 90.6250 (90.6250) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [1/1] time 1.002 (1.002) data 0.789 (0.789) loss 0.2573 (0.2573) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:15
epoch [186/200] batch [1/1] time 1.005 (1.005) data 0.792 (0.792) loss 0.2588 (0.2588) acc 93.7500 (93.7500) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [1/1] time 1.022 (1.022) data 0.808 (0.808) loss 0.2394 (0.2394) acc 96.8750 (96.8750) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [1/1] time 0.975 (0.975) data 0.783 (0.783) loss 0.2537 (0.2537) acc 93.7500 (93.7500) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [1/1] time 1.110 (1.110) data 0.901 (0.901) loss 0.3276 (0.3276) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [1/1] time 1.051 (1.051) data 0.834 (0.834) loss 0.1786 (0.1786) acc 96.8750 (96.8750) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [1/1] time 1.071 (1.071) data 0.855 (0.855) loss 0.2172 (0.2172) acc 93.7500 (93.7500) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [1/1] time 0.972 (0.972) data 0.788 (0.788) loss 0.2744 (0.2744) acc 93.7500 (93.7500) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [1/1] time 0.999 (0.999) data 0.809 (0.809) loss 0.1537 (0.1537) acc 96.8750 (96.8750) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [1/1] time 0.974 (0.974) data 0.782 (0.782) loss 0.2317 (0.2317) acc 93.7500 (93.7500) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [1/1] time 1.004 (1.004) data 0.788 (0.788) loss 0.3413 (0.3413) acc 87.5000 (87.5000) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [1/1] time 0.978 (0.978) data 0.784 (0.784) loss 0.1614 (0.1614) acc 96.8750 (96.8750) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [1/1] time 0.986 (0.986) data 0.792 (0.792) loss 0.1733 (0.1733) acc 100.0000 (100.0000) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [1/1] time 0.994 (0.994) data 0.793 (0.793) loss 0.2249 (0.2249) acc 96.8750 (96.8750) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [1/1] time 0.986 (0.986) data 0.777 (0.777) loss 0.1981 (0.1981) acc 96.8750 (96.8750) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [1/1] time 0.960 (0.960) data 0.783 (0.783) loss 0.1119 (0.1119) acc 100.0000 (100.0000) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,100
* correct: 6,681
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 82.1%
training time for 5-th round: 311.77 seconds
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
| Calculating uncertainty of Unlabeled set
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
INITIALIZE the prompts weights
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
['X X X X X X X X X X X X X X X X Annual Crop Land.', 'X X X X X X X X X X X X X X X X Forest.', 'X X X X X X X X X X X X X X X X Herbaceous Vegetation Land.', 'X X X X X X X X X X X X X X X X Highway or Road.', 'X X X X X X X X X X X X X X X X Industrial Buildings.', 'X X X X X X X X X X X X X X X X Pasture Land.', 'X X X X X X X X X X X X X X X X Permanent Crop Land.', 'X X X X X X X X X X X X X X X X Residential Buildings.', 'X X X X X X X X X X X X X X X X River.', 'X X X X X X X X X X X X X X X X Sea or Lake.']
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CustomCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
Turning off gradients in both the image and the text encoder
Multiple GPUs detected (n_gpus=2), use all of them!
DataParallel(
  (module): CustomCLIP(
    (prompt_learner): PromptLearner()
    (image_encoder): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
epoch [1/200] batch [1/2] time 0.965 (0.965) data 0.778 (0.778) loss 2.8125 (2.8125) acc 15.6250 (15.6250) lr 1.0000e-05 eta 0:06:25
epoch [1/200] batch [2/2] time 0.194 (0.579) data 0.000 (0.389) loss 2.1699 (2.4912) acc 28.1250 (21.8750) lr 2.0000e-03 eta 0:03:50
epoch [2/200] batch [1/2] time 1.128 (1.128) data 0.923 (0.923) loss 2.6113 (2.6113) acc 18.7500 (18.7500) lr 2.0000e-03 eta 0:07:27
epoch [2/200] batch [2/2] time 0.205 (0.667) data 0.000 (0.462) loss 2.2578 (2.4346) acc 25.0000 (21.8750) lr 1.9999e-03 eta 0:04:24
epoch [3/200] batch [1/2] time 0.992 (0.992) data 0.792 (0.792) loss 2.4473 (2.4473) acc 34.3750 (34.3750) lr 1.9999e-03 eta 0:06:31
epoch [3/200] batch [2/2] time 0.198 (0.595) data 0.000 (0.396) loss 2.1914 (2.3193) acc 25.0000 (29.6875) lr 1.9995e-03 eta 0:03:54
epoch [4/200] batch [1/2] time 0.999 (0.999) data 0.798 (0.798) loss 1.6309 (1.6309) acc 40.6250 (40.6250) lr 1.9995e-03 eta 0:06:32
epoch [4/200] batch [2/2] time 0.199 (0.599) data 0.000 (0.399) loss 1.9795 (1.8052) acc 37.5000 (39.0625) lr 1.9989e-03 eta 0:03:54
epoch [5/200] batch [1/2] time 1.012 (1.012) data 0.800 (0.800) loss 1.4668 (1.4668) acc 43.7500 (43.7500) lr 1.9989e-03 eta 0:06:35
epoch [5/200] batch [2/2] time 0.203 (0.607) data 0.000 (0.400) loss 1.9902 (1.7285) acc 50.0000 (46.8750) lr 1.9980e-03 eta 0:03:56
epoch [6/200] batch [1/2] time 1.019 (1.019) data 0.817 (0.817) loss 1.6045 (1.6045) acc 37.5000 (37.5000) lr 1.9980e-03 eta 0:06:36
epoch [6/200] batch [2/2] time 0.198 (0.609) data 0.000 (0.408) loss 1.8740 (1.7393) acc 34.3750 (35.9375) lr 1.9969e-03 eta 0:03:56
epoch [7/200] batch [1/2] time 1.022 (1.022) data 0.806 (0.806) loss 1.4219 (1.4219) acc 53.1250 (53.1250) lr 1.9969e-03 eta 0:06:35
epoch [7/200] batch [2/2] time 0.200 (0.611) data 0.000 (0.403) loss 1.5068 (1.4644) acc 65.6250 (59.3750) lr 1.9956e-03 eta 0:03:55
epoch [8/200] batch [1/2] time 0.961 (0.961) data 0.768 (0.768) loss 1.0918 (1.0918) acc 71.8750 (71.8750) lr 1.9956e-03 eta 0:06:09
epoch [8/200] batch [2/2] time 0.204 (0.583) data 0.000 (0.384) loss 1.0449 (1.0684) acc 65.6250 (68.7500) lr 1.9940e-03 eta 0:03:43
epoch [9/200] batch [1/2] time 1.070 (1.070) data 0.858 (0.858) loss 1.0342 (1.0342) acc 65.6250 (65.6250) lr 1.9940e-03 eta 0:06:49
epoch [9/200] batch [2/2] time 0.194 (0.632) data 0.000 (0.429) loss 1.0859 (1.0601) acc 68.7500 (67.1875) lr 1.9921e-03 eta 0:04:01
epoch [10/200] batch [1/2] time 1.056 (1.056) data 0.825 (0.825) loss 0.8257 (0.8257) acc 78.1250 (78.1250) lr 1.9921e-03 eta 0:06:42
epoch [10/200] batch [2/2] time 0.212 (0.634) data 0.000 (0.413) loss 1.0156 (0.9207) acc 59.3750 (68.7500) lr 1.9900e-03 eta 0:04:01
epoch [11/200] batch [1/2] time 1.025 (1.025) data 0.817 (0.817) loss 0.7969 (0.7969) acc 81.2500 (81.2500) lr 1.9900e-03 eta 0:06:28
epoch [11/200] batch [2/2] time 0.183 (0.604) data 0.000 (0.409) loss 0.9209 (0.8589) acc 62.5000 (71.8750) lr 1.9877e-03 eta 0:03:48
epoch [12/200] batch [1/2] time 0.978 (0.978) data 0.785 (0.785) loss 0.9678 (0.9678) acc 71.8750 (71.8750) lr 1.9877e-03 eta 0:06:08
epoch [12/200] batch [2/2] time 0.198 (0.588) data 0.000 (0.393) loss 1.1621 (1.0649) acc 56.2500 (64.0625) lr 1.9851e-03 eta 0:03:41
epoch [13/200] batch [1/2] time 1.057 (1.057) data 0.840 (0.840) loss 0.5396 (0.5396) acc 84.3750 (84.3750) lr 1.9851e-03 eta 0:06:36
epoch [13/200] batch [2/2] time 0.207 (0.632) data 0.000 (0.420) loss 0.8477 (0.6936) acc 78.1250 (81.2500) lr 1.9823e-03 eta 0:03:56
epoch [14/200] batch [1/2] time 1.027 (1.027) data 0.819 (0.819) loss 0.8174 (0.8174) acc 75.0000 (75.0000) lr 1.9823e-03 eta 0:06:23
epoch [14/200] batch [2/2] time 0.202 (0.615) data 0.000 (0.410) loss 0.6719 (0.7446) acc 75.0000 (75.0000) lr 1.9792e-03 eta 0:03:48
epoch [15/200] batch [1/2] time 1.019 (1.019) data 0.812 (0.812) loss 0.7588 (0.7588) acc 78.1250 (78.1250) lr 1.9792e-03 eta 0:06:18
epoch [15/200] batch [2/2] time 0.199 (0.609) data 0.000 (0.406) loss 0.7446 (0.7517) acc 75.0000 (76.5625) lr 1.9759e-03 eta 0:03:45
epoch [16/200] batch [1/2] time 0.969 (0.969) data 0.783 (0.783) loss 0.6289 (0.6289) acc 81.2500 (81.2500) lr 1.9759e-03 eta 0:05:57
epoch [16/200] batch [2/2] time 0.196 (0.582) data 0.000 (0.392) loss 0.8657 (0.7473) acc 75.0000 (78.1250) lr 1.9724e-03 eta 0:03:34
epoch [17/200] batch [1/2] time 1.026 (1.026) data 0.800 (0.800) loss 0.9009 (0.9009) acc 62.5000 (62.5000) lr 1.9724e-03 eta 0:06:16
epoch [17/200] batch [2/2] time 0.186 (0.606) data 0.000 (0.400) loss 0.9414 (0.9211) acc 75.0000 (68.7500) lr 1.9686e-03 eta 0:03:41
epoch [18/200] batch [1/2] time 1.148 (1.148) data 0.945 (0.945) loss 0.7202 (0.7202) acc 81.2500 (81.2500) lr 1.9686e-03 eta 0:06:59
epoch [18/200] batch [2/2] time 0.211 (0.680) data 0.000 (0.473) loss 0.7559 (0.7380) acc 75.0000 (78.1250) lr 1.9646e-03 eta 0:04:07
epoch [19/200] batch [1/2] time 1.132 (1.132) data 0.940 (0.940) loss 0.5518 (0.5518) acc 84.3750 (84.3750) lr 1.9646e-03 eta 0:06:50
epoch [19/200] batch [2/2] time 0.184 (0.658) data 0.000 (0.470) loss 0.6914 (0.6216) acc 78.1250 (81.2500) lr 1.9603e-03 eta 0:03:58
epoch [20/200] batch [1/2] time 0.968 (0.968) data 0.789 (0.789) loss 0.8984 (0.8984) acc 68.7500 (68.7500) lr 1.9603e-03 eta 0:05:49
epoch [20/200] batch [2/2] time 0.196 (0.582) data 0.000 (0.395) loss 0.5884 (0.7434) acc 75.0000 (71.8750) lr 1.9558e-03 eta 0:03:29
epoch [21/200] batch [1/2] time 1.007 (1.007) data 0.790 (0.790) loss 0.5068 (0.5068) acc 87.5000 (87.5000) lr 1.9558e-03 eta 0:06:01
epoch [21/200] batch [2/2] time 0.204 (0.605) data 0.000 (0.395) loss 0.6387 (0.5728) acc 87.5000 (87.5000) lr 1.9511e-03 eta 0:03:36
epoch [22/200] batch [1/2] time 1.083 (1.083) data 0.866 (0.866) loss 0.7466 (0.7466) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:06:26
epoch [22/200] batch [2/2] time 0.197 (0.640) data 0.000 (0.433) loss 0.4897 (0.6182) acc 90.6250 (85.9375) lr 1.9461e-03 eta 0:03:47
epoch [23/200] batch [1/2] time 1.110 (1.110) data 0.887 (0.887) loss 0.5005 (0.5005) acc 84.3750 (84.3750) lr 1.9461e-03 eta 0:06:34
epoch [23/200] batch [2/2] time 0.196 (0.653) data 0.000 (0.443) loss 0.4028 (0.4517) acc 90.6250 (87.5000) lr 1.9409e-03 eta 0:03:51
epoch [24/200] batch [1/2] time 0.991 (0.991) data 0.795 (0.795) loss 0.5044 (0.5044) acc 84.3750 (84.3750) lr 1.9409e-03 eta 0:05:49
epoch [24/200] batch [2/2] time 0.197 (0.594) data 0.000 (0.398) loss 0.6377 (0.5710) acc 84.3750 (84.3750) lr 1.9354e-03 eta 0:03:29
epoch [25/200] batch [1/2] time 1.017 (1.017) data 0.795 (0.795) loss 0.6177 (0.6177) acc 81.2500 (81.2500) lr 1.9354e-03 eta 0:05:57
epoch [25/200] batch [2/2] time 0.209 (0.613) data 0.000 (0.398) loss 0.4241 (0.5209) acc 90.6250 (85.9375) lr 1.9298e-03 eta 0:03:34
epoch [26/200] batch [1/2] time 1.151 (1.151) data 0.944 (0.944) loss 0.5337 (0.5337) acc 87.5000 (87.5000) lr 1.9298e-03 eta 0:06:41
epoch [26/200] batch [2/2] time 0.200 (0.676) data 0.000 (0.472) loss 0.7207 (0.6272) acc 81.2500 (84.3750) lr 1.9239e-03 eta 0:03:55
epoch [27/200] batch [1/2] time 1.019 (1.019) data 0.805 (0.805) loss 0.4497 (0.4497) acc 87.5000 (87.5000) lr 1.9239e-03 eta 0:05:53
epoch [27/200] batch [2/2] time 0.206 (0.612) data 0.000 (0.402) loss 0.4631 (0.4564) acc 78.1250 (82.8125) lr 1.9178e-03 eta 0:03:31
epoch [28/200] batch [1/2] time 0.993 (0.993) data 0.798 (0.798) loss 0.3708 (0.3708) acc 93.7500 (93.7500) lr 1.9178e-03 eta 0:05:42
epoch [28/200] batch [2/2] time 0.192 (0.593) data 0.000 (0.399) loss 0.6523 (0.5116) acc 78.1250 (85.9375) lr 1.9114e-03 eta 0:03:23
epoch [29/200] batch [1/2] time 1.012 (1.012) data 0.794 (0.794) loss 0.5474 (0.5474) acc 84.3750 (84.3750) lr 1.9114e-03 eta 0:05:47
epoch [29/200] batch [2/2] time 0.203 (0.608) data 0.000 (0.397) loss 0.6641 (0.6057) acc 81.2500 (82.8125) lr 1.9048e-03 eta 0:03:27
epoch [30/200] batch [1/2] time 1.102 (1.102) data 0.880 (0.880) loss 0.3579 (0.3579) acc 96.8750 (96.8750) lr 1.9048e-03 eta 0:06:15
epoch [30/200] batch [2/2] time 0.389 (0.746) data 0.000 (0.440) loss 0.5728 (0.4653) acc 81.2500 (89.0625) lr 1.8980e-03 eta 0:04:13
epoch [31/200] batch [1/2] time 0.974 (0.974) data 0.787 (0.787) loss 0.3022 (0.3022) acc 90.6250 (90.6250) lr 1.8980e-03 eta 0:05:30
epoch [31/200] batch [2/2] time 0.203 (0.588) data 0.000 (0.394) loss 0.6240 (0.4631) acc 81.2500 (85.9375) lr 1.8910e-03 eta 0:03:18
epoch [32/200] batch [1/2] time 1.011 (1.011) data 0.779 (0.779) loss 0.4243 (0.4243) acc 90.6250 (90.6250) lr 1.8910e-03 eta 0:05:40
epoch [32/200] batch [2/2] time 0.213 (0.612) data 0.000 (0.390) loss 0.4431 (0.4337) acc 84.3750 (87.5000) lr 1.8838e-03 eta 0:03:25
epoch [33/200] batch [1/2] time 1.007 (1.007) data 0.790 (0.790) loss 0.4263 (0.4263) acc 87.5000 (87.5000) lr 1.8838e-03 eta 0:05:37
epoch [33/200] batch [2/2] time 0.204 (0.606) data 0.000 (0.395) loss 0.4221 (0.4242) acc 90.6250 (89.0625) lr 1.8763e-03 eta 0:03:22
epoch [34/200] batch [1/2] time 1.008 (1.008) data 0.800 (0.800) loss 0.2064 (0.2064) acc 100.0000 (100.0000) lr 1.8763e-03 eta 0:05:35
epoch [34/200] batch [2/2] time 0.193 (0.600) data 0.000 (0.400) loss 0.6338 (0.4201) acc 75.0000 (87.5000) lr 1.8686e-03 eta 0:03:19
epoch [35/200] batch [1/2] time 1.022 (1.022) data 0.795 (0.795) loss 0.4939 (0.4939) acc 90.6250 (90.6250) lr 1.8686e-03 eta 0:05:38
epoch [35/200] batch [2/2] time 0.216 (0.619) data 0.000 (0.397) loss 0.3713 (0.4326) acc 90.6250 (90.6250) lr 1.8607e-03 eta 0:03:24
epoch [36/200] batch [1/2] time 1.065 (1.065) data 0.839 (0.839) loss 0.4124 (0.4124) acc 84.3750 (84.3750) lr 1.8607e-03 eta 0:05:50
epoch [36/200] batch [2/2] time 0.206 (0.635) data 0.000 (0.420) loss 0.4766 (0.4445) acc 87.5000 (85.9375) lr 1.8526e-03 eta 0:03:28
epoch [37/200] batch [1/2] time 1.028 (1.028) data 0.805 (0.805) loss 0.4246 (0.4246) acc 93.7500 (93.7500) lr 1.8526e-03 eta 0:05:36
epoch [37/200] batch [2/2] time 0.212 (0.620) data 0.000 (0.403) loss 0.2147 (0.3196) acc 96.8750 (95.3125) lr 1.8443e-03 eta 0:03:22
epoch [38/200] batch [1/2] time 1.023 (1.023) data 0.813 (0.813) loss 0.3477 (0.3477) acc 90.6250 (90.6250) lr 1.8443e-03 eta 0:05:32
epoch [38/200] batch [2/2] time 0.223 (0.623) data 0.000 (0.406) loss 0.4851 (0.4164) acc 84.3750 (87.5000) lr 1.8358e-03 eta 0:03:21
epoch [39/200] batch [1/2] time 1.017 (1.017) data 0.803 (0.803) loss 0.2189 (0.2189) acc 96.8750 (96.8750) lr 1.8358e-03 eta 0:05:28
epoch [39/200] batch [2/2] time 0.196 (0.607) data 0.000 (0.401) loss 0.4783 (0.3486) acc 81.2500 (89.0625) lr 1.8271e-03 eta 0:03:15
epoch [40/200] batch [1/2] time 0.995 (0.995) data 0.810 (0.810) loss 0.5781 (0.5781) acc 84.3750 (84.3750) lr 1.8271e-03 eta 0:05:19
epoch [40/200] batch [2/2] time 0.193 (0.594) data 0.000 (0.405) loss 0.3564 (0.4673) acc 93.7500 (89.0625) lr 1.8181e-03 eta 0:03:10
epoch [41/200] batch [1/2] time 0.998 (0.998) data 0.787 (0.787) loss 0.6108 (0.6108) acc 75.0000 (75.0000) lr 1.8181e-03 eta 0:05:18
epoch [41/200] batch [2/2] time 0.210 (0.604) data 0.000 (0.393) loss 0.3464 (0.4786) acc 90.6250 (82.8125) lr 1.8090e-03 eta 0:03:12
epoch [42/200] batch [1/2] time 1.027 (1.027) data 0.805 (0.805) loss 0.2966 (0.2966) acc 96.8750 (96.8750) lr 1.8090e-03 eta 0:05:25
epoch [42/200] batch [2/2] time 0.193 (0.610) data 0.000 (0.402) loss 0.4961 (0.3964) acc 84.3750 (90.6250) lr 1.7997e-03 eta 0:03:12
epoch [43/200] batch [1/2] time 0.988 (0.988) data 0.798 (0.798) loss 0.2944 (0.2944) acc 93.7500 (93.7500) lr 1.7997e-03 eta 0:05:11
epoch [43/200] batch [2/2] time 0.202 (0.595) data 0.000 (0.399) loss 0.4780 (0.3862) acc 90.6250 (92.1875) lr 1.7902e-03 eta 0:03:06
epoch [44/200] batch [1/2] time 1.096 (1.096) data 0.901 (0.901) loss 0.4602 (0.4602) acc 75.0000 (75.0000) lr 1.7902e-03 eta 0:05:42
epoch [44/200] batch [2/2] time 0.195 (0.645) data 0.000 (0.451) loss 0.2549 (0.3575) acc 96.8750 (85.9375) lr 1.7804e-03 eta 0:03:21
epoch [45/200] batch [1/2] time 1.126 (1.126) data 0.910 (0.910) loss 0.2649 (0.2649) acc 93.7500 (93.7500) lr 1.7804e-03 eta 0:05:50
epoch [45/200] batch [2/2] time 0.214 (0.670) data 0.000 (0.455) loss 0.4277 (0.3463) acc 87.5000 (90.6250) lr 1.7705e-03 eta 0:03:27
epoch [46/200] batch [1/2] time 1.119 (1.119) data 0.930 (0.930) loss 0.2856 (0.2856) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:05:45
epoch [46/200] batch [2/2] time 0.194 (0.656) data 0.000 (0.465) loss 0.4893 (0.3875) acc 87.5000 (87.5000) lr 1.7604e-03 eta 0:03:22
epoch [47/200] batch [1/2] time 1.039 (1.039) data 0.829 (0.829) loss 0.3235 (0.3235) acc 90.6250 (90.6250) lr 1.7604e-03 eta 0:05:19
epoch [47/200] batch [2/2] time 0.216 (0.627) data 0.000 (0.415) loss 0.3643 (0.3439) acc 93.7500 (92.1875) lr 1.7501e-03 eta 0:03:11
epoch [48/200] batch [1/2] time 1.034 (1.034) data 0.864 (0.864) loss 0.1901 (0.1901) acc 96.8750 (96.8750) lr 1.7501e-03 eta 0:05:15
epoch [48/200] batch [2/2] time 0.191 (0.613) data 0.000 (0.432) loss 0.3123 (0.2512) acc 96.8750 (96.8750) lr 1.7396e-03 eta 0:03:06
epoch [49/200] batch [1/2] time 1.005 (1.005) data 0.795 (0.795) loss 0.3738 (0.3738) acc 84.3750 (84.3750) lr 1.7396e-03 eta 0:05:04
epoch [49/200] batch [2/2] time 0.197 (0.601) data 0.000 (0.398) loss 0.3313 (0.3525) acc 90.6250 (87.5000) lr 1.7290e-03 eta 0:03:01
epoch [50/200] batch [1/2] time 1.095 (1.095) data 0.883 (0.883) loss 0.4839 (0.4839) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:05:29
epoch [50/200] batch [2/2] time 0.203 (0.649) data 0.000 (0.442) loss 0.4949 (0.4894) acc 81.2500 (82.8125) lr 1.7181e-03 eta 0:03:14
epoch [51/200] batch [1/2] time 1.010 (1.010) data 0.812 (0.812) loss 0.3193 (0.3193) acc 93.7500 (93.7500) lr 1.7181e-03 eta 0:05:01
epoch [51/200] batch [2/2] time 0.195 (0.603) data 0.000 (0.406) loss 0.3748 (0.3470) acc 87.5000 (90.6250) lr 1.7071e-03 eta 0:02:59
epoch [52/200] batch [1/2] time 1.028 (1.028) data 0.793 (0.793) loss 0.2349 (0.2349) acc 93.7500 (93.7500) lr 1.7071e-03 eta 0:05:05
epoch [52/200] batch [2/2] time 0.217 (0.622) data 0.000 (0.397) loss 0.2617 (0.2483) acc 96.8750 (95.3125) lr 1.6959e-03 eta 0:03:04
epoch [53/200] batch [1/2] time 1.007 (1.007) data 0.802 (0.802) loss 0.2864 (0.2864) acc 90.6250 (90.6250) lr 1.6959e-03 eta 0:04:57
epoch [53/200] batch [2/2] time 0.206 (0.607) data 0.000 (0.401) loss 0.3738 (0.3301) acc 90.6250 (90.6250) lr 1.6845e-03 eta 0:02:58
epoch [54/200] batch [1/2] time 1.004 (1.004) data 0.792 (0.792) loss 0.3401 (0.3401) acc 90.6250 (90.6250) lr 1.6845e-03 eta 0:04:54
epoch [54/200] batch [2/2] time 0.215 (0.610) data 0.000 (0.396) loss 0.4214 (0.3807) acc 84.3750 (87.5000) lr 1.6730e-03 eta 0:02:57
epoch [55/200] batch [1/2] time 1.022 (1.022) data 0.785 (0.785) loss 0.2201 (0.2201) acc 96.8750 (96.8750) lr 1.6730e-03 eta 0:04:57
epoch [55/200] batch [2/2] time 0.221 (0.621) data 0.000 (0.392) loss 0.3704 (0.2952) acc 87.5000 (92.1875) lr 1.6613e-03 eta 0:03:00
epoch [56/200] batch [1/2] time 1.019 (1.019) data 0.811 (0.811) loss 0.3132 (0.3132) acc 90.6250 (90.6250) lr 1.6613e-03 eta 0:04:54
epoch [56/200] batch [2/2] time 0.198 (0.608) data 0.000 (0.405) loss 0.1663 (0.2397) acc 96.8750 (93.7500) lr 1.6494e-03 eta 0:02:55
epoch [57/200] batch [1/2] time 0.993 (0.993) data 0.799 (0.799) loss 0.2820 (0.2820) acc 90.6250 (90.6250) lr 1.6494e-03 eta 0:04:44
epoch [57/200] batch [2/2] time 0.197 (0.595) data 0.000 (0.399) loss 0.3408 (0.3114) acc 90.6250 (90.6250) lr 1.6374e-03 eta 0:02:50
epoch [58/200] batch [1/2] time 1.006 (1.006) data 0.803 (0.803) loss 0.3870 (0.3870) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:04:46
epoch [58/200] batch [2/2] time 0.188 (0.597) data 0.000 (0.401) loss 0.3572 (0.3721) acc 90.6250 (89.0625) lr 1.6252e-03 eta 0:02:49
epoch [59/200] batch [1/2] time 1.007 (1.007) data 0.798 (0.798) loss 0.1863 (0.1863) acc 96.8750 (96.8750) lr 1.6252e-03 eta 0:04:44
epoch [59/200] batch [2/2] time 0.206 (0.606) data 0.000 (0.399) loss 0.3738 (0.2800) acc 90.6250 (93.7500) lr 1.6129e-03 eta 0:02:51
epoch [60/200] batch [1/2] time 1.008 (1.008) data 0.791 (0.791) loss 0.4673 (0.4673) acc 90.6250 (90.6250) lr 1.6129e-03 eta 0:04:43
epoch [60/200] batch [2/2] time 0.206 (0.607) data 0.000 (0.396) loss 0.2206 (0.3439) acc 100.0000 (95.3125) lr 1.6004e-03 eta 0:02:49
epoch [61/200] batch [1/2] time 1.026 (1.026) data 0.833 (0.833) loss 0.2467 (0.2467) acc 93.7500 (93.7500) lr 1.6004e-03 eta 0:04:46
epoch [61/200] batch [2/2] time 0.373 (0.700) data 0.000 (0.417) loss 0.5195 (0.3831) acc 75.0000 (84.3750) lr 1.5878e-03 eta 0:03:14
epoch [62/200] batch [1/2] time 1.039 (1.039) data 0.816 (0.816) loss 0.1729 (0.1729) acc 100.0000 (100.0000) lr 1.5878e-03 eta 0:04:47
epoch [62/200] batch [2/2] time 0.205 (0.622) data 0.000 (0.408) loss 0.1764 (0.1746) acc 100.0000 (100.0000) lr 1.5750e-03 eta 0:02:51
epoch [63/200] batch [1/2] time 0.994 (0.994) data 0.797 (0.797) loss 0.2205 (0.2205) acc 93.7500 (93.7500) lr 1.5750e-03 eta 0:04:33
epoch [63/200] batch [2/2] time 0.192 (0.593) data 0.000 (0.399) loss 0.3696 (0.2950) acc 96.8750 (95.3125) lr 1.5621e-03 eta 0:02:42
epoch [64/200] batch [1/2] time 1.016 (1.016) data 0.788 (0.788) loss 0.1996 (0.1996) acc 93.7500 (93.7500) lr 1.5621e-03 eta 0:04:37
epoch [64/200] batch [2/2] time 0.212 (0.614) data 0.000 (0.394) loss 0.2028 (0.2012) acc 96.8750 (95.3125) lr 1.5490e-03 eta 0:02:47
epoch [65/200] batch [1/2] time 0.994 (0.994) data 0.772 (0.772) loss 0.1009 (0.1009) acc 100.0000 (100.0000) lr 1.5490e-03 eta 0:04:29
epoch [65/200] batch [2/2] time 0.208 (0.601) data 0.000 (0.386) loss 0.3069 (0.2039) acc 93.7500 (96.8750) lr 1.5358e-03 eta 0:02:42
epoch [66/200] batch [1/2] time 1.118 (1.118) data 0.906 (0.906) loss 0.2236 (0.2236) acc 96.8750 (96.8750) lr 1.5358e-03 eta 0:05:00
epoch [66/200] batch [2/2] time 0.201 (0.660) data 0.000 (0.453) loss 0.2944 (0.2590) acc 93.7500 (95.3125) lr 1.5225e-03 eta 0:02:56
epoch [67/200] batch [1/2] time 1.001 (1.001) data 0.828 (0.828) loss 0.1235 (0.1235) acc 100.0000 (100.0000) lr 1.5225e-03 eta 0:04:27
epoch [67/200] batch [2/2] time 0.194 (0.598) data 0.000 (0.414) loss 0.2100 (0.1667) acc 93.7500 (96.8750) lr 1.5090e-03 eta 0:02:39
epoch [68/200] batch [1/2] time 1.062 (1.062) data 0.862 (0.862) loss 0.3330 (0.3330) acc 87.5000 (87.5000) lr 1.5090e-03 eta 0:04:41
epoch [68/200] batch [2/2] time 0.201 (0.632) data 0.000 (0.431) loss 0.2217 (0.2773) acc 96.8750 (92.1875) lr 1.4955e-03 eta 0:02:46
epoch [69/200] batch [1/2] time 1.007 (1.007) data 0.799 (0.799) loss 0.2007 (0.2007) acc 93.7500 (93.7500) lr 1.4955e-03 eta 0:04:24
epoch [69/200] batch [2/2] time 0.208 (0.608) data 0.000 (0.399) loss 0.2249 (0.2128) acc 96.8750 (95.3125) lr 1.4818e-03 eta 0:02:39
epoch [70/200] batch [1/2] time 1.023 (1.023) data 0.807 (0.807) loss 0.2332 (0.2332) acc 93.7500 (93.7500) lr 1.4818e-03 eta 0:04:27
epoch [70/200] batch [2/2] time 0.216 (0.620) data 0.000 (0.404) loss 0.2061 (0.2196) acc 96.8750 (95.3125) lr 1.4679e-03 eta 0:02:41
epoch [71/200] batch [1/2] time 0.997 (0.997) data 0.798 (0.798) loss 0.1923 (0.1923) acc 96.8750 (96.8750) lr 1.4679e-03 eta 0:04:18
epoch [71/200] batch [2/2] time 0.197 (0.597) data 0.000 (0.399) loss 0.1753 (0.1838) acc 100.0000 (98.4375) lr 1.4540e-03 eta 0:02:34
epoch [72/200] batch [1/2] time 1.007 (1.007) data 0.794 (0.794) loss 0.2133 (0.2133) acc 93.7500 (93.7500) lr 1.4540e-03 eta 0:04:18
epoch [72/200] batch [2/2] time 0.206 (0.607) data 0.000 (0.397) loss 0.3123 (0.2628) acc 90.6250 (92.1875) lr 1.4399e-03 eta 0:02:35
epoch [73/200] batch [1/2] time 0.998 (0.998) data 0.792 (0.792) loss 0.2732 (0.2732) acc 93.7500 (93.7500) lr 1.4399e-03 eta 0:04:14
epoch [73/200] batch [2/2] time 0.201 (0.599) data 0.000 (0.396) loss 0.2281 (0.2507) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:02:32
epoch [74/200] batch [1/2] time 0.975 (0.975) data 0.802 (0.802) loss 0.2644 (0.2644) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:04:06
epoch [74/200] batch [2/2] time 0.190 (0.583) data 0.000 (0.401) loss 0.2590 (0.2617) acc 96.8750 (93.7500) lr 1.4115e-03 eta 0:02:26
epoch [75/200] batch [1/2] time 0.989 (0.989) data 0.797 (0.797) loss 0.3237 (0.3237) acc 90.6250 (90.6250) lr 1.4115e-03 eta 0:04:08
epoch [75/200] batch [2/2] time 0.190 (0.589) data 0.000 (0.399) loss 0.2600 (0.2919) acc 93.7500 (92.1875) lr 1.3971e-03 eta 0:02:27
epoch [76/200] batch [1/2] time 1.008 (1.008) data 0.798 (0.798) loss 0.2690 (0.2690) acc 93.7500 (93.7500) lr 1.3971e-03 eta 0:04:11
epoch [76/200] batch [2/2] time 0.198 (0.603) data 0.000 (0.399) loss 0.3000 (0.2845) acc 93.7500 (93.7500) lr 1.3827e-03 eta 0:02:29
epoch [77/200] batch [1/2] time 0.996 (0.996) data 0.786 (0.786) loss 0.1628 (0.1628) acc 100.0000 (100.0000) lr 1.3827e-03 eta 0:04:06
epoch [77/200] batch [2/2] time 0.200 (0.598) data 0.000 (0.393) loss 0.1021 (0.1324) acc 100.0000 (100.0000) lr 1.3681e-03 eta 0:02:27
epoch [78/200] batch [1/2] time 0.960 (0.960) data 0.786 (0.786) loss 0.0994 (0.0994) acc 100.0000 (100.0000) lr 1.3681e-03 eta 0:03:55
epoch [78/200] batch [2/2] time 0.200 (0.580) data 0.000 (0.393) loss 0.3208 (0.2101) acc 87.5000 (93.7500) lr 1.3535e-03 eta 0:02:21
epoch [79/200] batch [1/2] time 0.979 (0.979) data 0.803 (0.803) loss 0.2427 (0.2427) acc 93.7500 (93.7500) lr 1.3535e-03 eta 0:03:57
epoch [79/200] batch [2/2] time 0.186 (0.582) data 0.000 (0.402) loss 0.3474 (0.2950) acc 87.5000 (90.6250) lr 1.3387e-03 eta 0:02:20
epoch [80/200] batch [1/2] time 0.955 (0.955) data 0.780 (0.780) loss 0.2771 (0.2771) acc 84.3750 (84.3750) lr 1.3387e-03 eta 0:03:50
epoch [80/200] batch [2/2] time 0.202 (0.578) data 0.000 (0.390) loss 0.1449 (0.2110) acc 100.0000 (92.1875) lr 1.3239e-03 eta 0:02:18
epoch [81/200] batch [1/2] time 1.118 (1.118) data 0.896 (0.896) loss 0.1989 (0.1989) acc 93.7500 (93.7500) lr 1.3239e-03 eta 0:04:27
epoch [81/200] batch [2/2] time 0.201 (0.660) data 0.000 (0.448) loss 0.4014 (0.3001) acc 84.3750 (89.0625) lr 1.3090e-03 eta 0:02:36
epoch [82/200] batch [1/2] time 1.019 (1.019) data 0.803 (0.803) loss 0.2096 (0.2096) acc 96.8750 (96.8750) lr 1.3090e-03 eta 0:04:01
epoch [82/200] batch [2/2] time 0.210 (0.614) data 0.000 (0.402) loss 0.3113 (0.2604) acc 93.7500 (95.3125) lr 1.2940e-03 eta 0:02:24
epoch [83/200] batch [1/2] time 1.025 (1.025) data 0.797 (0.797) loss 0.2712 (0.2712) acc 96.8750 (96.8750) lr 1.2940e-03 eta 0:04:00
epoch [83/200] batch [2/2] time 0.212 (0.618) data 0.000 (0.398) loss 0.1912 (0.2312) acc 96.8750 (96.8750) lr 1.2790e-03 eta 0:02:24
epoch [84/200] batch [1/2] time 1.027 (1.027) data 0.814 (0.814) loss 0.1895 (0.1895) acc 96.8750 (96.8750) lr 1.2790e-03 eta 0:03:59
epoch [84/200] batch [2/2] time 0.194 (0.610) data 0.000 (0.407) loss 0.2778 (0.2336) acc 90.6250 (93.7500) lr 1.2639e-03 eta 0:02:21
epoch [85/200] batch [1/2] time 1.010 (1.010) data 0.803 (0.803) loss 0.3115 (0.3115) acc 84.3750 (84.3750) lr 1.2639e-03 eta 0:03:53
epoch [85/200] batch [2/2] time 0.205 (0.608) data 0.000 (0.402) loss 0.3523 (0.3319) acc 93.7500 (89.0625) lr 1.2487e-03 eta 0:02:19
epoch [86/200] batch [1/2] time 1.164 (1.164) data 0.955 (0.955) loss 0.2544 (0.2544) acc 93.7500 (93.7500) lr 1.2487e-03 eta 0:04:26
epoch [86/200] batch [2/2] time 0.382 (0.773) data 0.000 (0.478) loss 0.1506 (0.2025) acc 96.8750 (95.3125) lr 1.2334e-03 eta 0:02:56
epoch [87/200] batch [1/2] time 1.005 (1.005) data 0.785 (0.785) loss 0.2155 (0.2155) acc 93.7500 (93.7500) lr 1.2334e-03 eta 0:03:48
epoch [87/200] batch [2/2] time 0.213 (0.609) data 0.000 (0.393) loss 0.2817 (0.2486) acc 96.8750 (95.3125) lr 1.2181e-03 eta 0:02:17
epoch [88/200] batch [1/2] time 1.207 (1.207) data 0.971 (0.971) loss 0.3120 (0.3120) acc 87.5000 (87.5000) lr 1.2181e-03 eta 0:04:31
epoch [88/200] batch [2/2] time 0.219 (0.713) data 0.000 (0.486) loss 0.2876 (0.2998) acc 93.7500 (90.6250) lr 1.2028e-03 eta 0:02:39
epoch [89/200] batch [1/2] time 1.024 (1.024) data 0.830 (0.830) loss 0.1814 (0.1814) acc 100.0000 (100.0000) lr 1.2028e-03 eta 0:03:48
epoch [89/200] batch [2/2] time 0.202 (0.613) data 0.000 (0.415) loss 0.2583 (0.2198) acc 90.6250 (95.3125) lr 1.1874e-03 eta 0:02:16
epoch [90/200] batch [1/2] time 0.955 (0.955) data 0.757 (0.757) loss 0.1234 (0.1234) acc 100.0000 (100.0000) lr 1.1874e-03 eta 0:03:30
epoch [90/200] batch [2/2] time 0.190 (0.572) data 0.000 (0.379) loss 0.1832 (0.1533) acc 96.8750 (98.4375) lr 1.1719e-03 eta 0:02:05
epoch [91/200] batch [1/2] time 1.055 (1.055) data 0.862 (0.862) loss 0.2277 (0.2277) acc 93.7500 (93.7500) lr 1.1719e-03 eta 0:03:51
epoch [91/200] batch [2/2] time 0.198 (0.627) data 0.000 (0.431) loss 0.2257 (0.2267) acc 93.7500 (93.7500) lr 1.1564e-03 eta 0:02:16
epoch [92/200] batch [1/2] time 1.000 (1.000) data 0.781 (0.781) loss 0.1992 (0.1992) acc 96.8750 (96.8750) lr 1.1564e-03 eta 0:03:37
epoch [92/200] batch [2/2] time 0.188 (0.594) data 0.000 (0.391) loss 0.2152 (0.2072) acc 93.7500 (95.3125) lr 1.1409e-03 eta 0:02:08
epoch [93/200] batch [1/2] time 0.979 (0.979) data 0.785 (0.785) loss 0.2124 (0.2124) acc 93.7500 (93.7500) lr 1.1409e-03 eta 0:03:30
epoch [93/200] batch [2/2] time 0.186 (0.582) data 0.000 (0.392) loss 0.0970 (0.1547) acc 100.0000 (96.8750) lr 1.1253e-03 eta 0:02:04
epoch [94/200] batch [1/2] time 0.956 (0.956) data 0.754 (0.754) loss 0.2664 (0.2664) acc 96.8750 (96.8750) lr 1.1253e-03 eta 0:03:23
epoch [94/200] batch [2/2] time 0.190 (0.573) data 0.000 (0.377) loss 0.2225 (0.2444) acc 93.7500 (95.3125) lr 1.1097e-03 eta 0:02:01
epoch [95/200] batch [1/2] time 0.956 (0.956) data 0.778 (0.778) loss 0.3862 (0.3862) acc 87.5000 (87.5000) lr 1.1097e-03 eta 0:03:21
epoch [95/200] batch [2/2] time 0.203 (0.579) data 0.000 (0.389) loss 0.3044 (0.3453) acc 93.7500 (90.6250) lr 1.0941e-03 eta 0:02:01
epoch [96/200] batch [1/2] time 1.006 (1.006) data 0.813 (0.813) loss 0.2404 (0.2404) acc 96.8750 (96.8750) lr 1.0941e-03 eta 0:03:30
epoch [96/200] batch [2/2] time 0.199 (0.603) data 0.000 (0.406) loss 0.2939 (0.2672) acc 93.7500 (95.3125) lr 1.0785e-03 eta 0:02:05
epoch [97/200] batch [1/2] time 1.030 (1.030) data 0.810 (0.810) loss 0.2422 (0.2422) acc 93.7500 (93.7500) lr 1.0785e-03 eta 0:03:33
epoch [97/200] batch [2/2] time 0.205 (0.618) data 0.000 (0.405) loss 0.2380 (0.2401) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:02:07
epoch [98/200] batch [1/2] time 1.013 (1.013) data 0.816 (0.816) loss 0.2325 (0.2325) acc 93.7500 (93.7500) lr 1.0628e-03 eta 0:03:27
epoch [98/200] batch [2/2] time 0.207 (0.610) data 0.000 (0.408) loss 0.2832 (0.2579) acc 87.5000 (90.6250) lr 1.0471e-03 eta 0:02:04
epoch [99/200] batch [1/2] time 1.015 (1.015) data 0.807 (0.807) loss 0.3506 (0.3506) acc 84.3750 (84.3750) lr 1.0471e-03 eta 0:03:25
epoch [99/200] batch [2/2] time 0.196 (0.605) data 0.000 (0.404) loss 0.2253 (0.2880) acc 93.7500 (89.0625) lr 1.0314e-03 eta 0:02:02
epoch [100/200] batch [1/2] time 1.005 (1.005) data 0.789 (0.789) loss 0.2930 (0.2930) acc 87.5000 (87.5000) lr 1.0314e-03 eta 0:03:21
epoch [100/200] batch [2/2] time 0.206 (0.605) data 0.000 (0.395) loss 0.1962 (0.2446) acc 93.7500 (90.6250) lr 1.0157e-03 eta 0:02:01
epoch [101/200] batch [1/2] time 1.022 (1.022) data 0.806 (0.806) loss 0.2571 (0.2571) acc 90.6250 (90.6250) lr 1.0157e-03 eta 0:03:23
epoch [101/200] batch [2/2] time 0.209 (0.616) data 0.000 (0.403) loss 0.1592 (0.2081) acc 96.8750 (93.7500) lr 1.0000e-03 eta 0:02:01
epoch [102/200] batch [1/2] time 1.057 (1.057) data 0.863 (0.863) loss 0.1376 (0.1376) acc 96.8750 (96.8750) lr 1.0000e-03 eta 0:03:28
epoch [102/200] batch [2/2] time 0.167 (0.612) data 0.000 (0.431) loss 0.3523 (0.2449) acc 93.7500 (95.3125) lr 9.8429e-04 eta 0:01:59
epoch [103/200] batch [1/2] time 1.029 (1.029) data 0.833 (0.833) loss 0.2747 (0.2747) acc 93.7500 (93.7500) lr 9.8429e-04 eta 0:03:20
epoch [103/200] batch [2/2] time 0.203 (0.616) data 0.000 (0.417) loss 0.1204 (0.1975) acc 96.8750 (95.3125) lr 9.6859e-04 eta 0:01:59
epoch [104/200] batch [1/2] time 1.068 (1.068) data 0.865 (0.865) loss 0.1683 (0.1683) acc 93.7500 (93.7500) lr 9.6859e-04 eta 0:03:26
epoch [104/200] batch [2/2] time 0.195 (0.632) data 0.000 (0.432) loss 0.1633 (0.1658) acc 96.8750 (95.3125) lr 9.5289e-04 eta 0:02:01
epoch [105/200] batch [1/2] time 1.019 (1.019) data 0.811 (0.811) loss 0.2006 (0.2006) acc 96.8750 (96.8750) lr 9.5289e-04 eta 0:03:14
epoch [105/200] batch [2/2] time 0.208 (0.614) data 0.000 (0.405) loss 0.1377 (0.1691) acc 100.0000 (98.4375) lr 9.3721e-04 eta 0:01:56
epoch [106/200] batch [1/2] time 1.005 (1.005) data 0.827 (0.827) loss 0.2788 (0.2788) acc 90.6250 (90.6250) lr 9.3721e-04 eta 0:03:09
epoch [106/200] batch [2/2] time 0.211 (0.608) data 0.000 (0.413) loss 0.3589 (0.3188) acc 93.7500 (92.1875) lr 9.2154e-04 eta 0:01:54
epoch [107/200] batch [1/2] time 1.098 (1.098) data 0.924 (0.924) loss 0.1628 (0.1628) acc 96.8750 (96.8750) lr 9.2154e-04 eta 0:03:25
epoch [107/200] batch [2/2] time 0.353 (0.725) data 0.000 (0.462) loss 0.1544 (0.1586) acc 96.8750 (96.8750) lr 9.0589e-04 eta 0:02:14
epoch [108/200] batch [1/2] time 1.020 (1.020) data 0.812 (0.812) loss 0.2151 (0.2151) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:03:08
epoch [108/200] batch [2/2] time 0.197 (0.608) data 0.000 (0.406) loss 0.1903 (0.2027) acc 93.7500 (93.7500) lr 8.9027e-04 eta 0:01:51
epoch [109/200] batch [1/2] time 0.980 (0.980) data 0.804 (0.804) loss 0.2603 (0.2603) acc 93.7500 (93.7500) lr 8.9027e-04 eta 0:02:59
epoch [109/200] batch [2/2] time 0.189 (0.584) data 0.000 (0.402) loss 0.2079 (0.2341) acc 93.7500 (93.7500) lr 8.7467e-04 eta 0:01:46
epoch [110/200] batch [1/2] time 0.996 (0.996) data 0.818 (0.818) loss 0.1808 (0.1808) acc 90.6250 (90.6250) lr 8.7467e-04 eta 0:03:00
epoch [110/200] batch [2/2] time 0.175 (0.586) data 0.000 (0.409) loss 0.3696 (0.2752) acc 93.7500 (92.1875) lr 8.5910e-04 eta 0:01:45
epoch [111/200] batch [1/2] time 0.998 (0.998) data 0.801 (0.801) loss 0.1783 (0.1783) acc 100.0000 (100.0000) lr 8.5910e-04 eta 0:02:58
epoch [111/200] batch [2/2] time 0.207 (0.602) data 0.000 (0.400) loss 0.3398 (0.2591) acc 87.5000 (93.7500) lr 8.4357e-04 eta 0:01:47
epoch [112/200] batch [1/2] time 1.031 (1.031) data 0.822 (0.822) loss 0.1542 (0.1542) acc 100.0000 (100.0000) lr 8.4357e-04 eta 0:03:02
epoch [112/200] batch [2/2] time 0.208 (0.620) data 0.000 (0.411) loss 0.0701 (0.1122) acc 100.0000 (100.0000) lr 8.2807e-04 eta 0:01:49
epoch [113/200] batch [1/2] time 1.023 (1.023) data 0.798 (0.798) loss 0.2158 (0.2158) acc 90.6250 (90.6250) lr 8.2807e-04 eta 0:02:58
epoch [113/200] batch [2/2] time 0.209 (0.616) data 0.000 (0.399) loss 0.2913 (0.2535) acc 90.6250 (90.6250) lr 8.1262e-04 eta 0:01:47
epoch [114/200] batch [1/2] time 1.006 (1.006) data 0.786 (0.786) loss 0.1942 (0.1942) acc 96.8750 (96.8750) lr 8.1262e-04 eta 0:02:54
epoch [114/200] batch [2/2] time 0.199 (0.602) data 0.000 (0.393) loss 0.2585 (0.2264) acc 96.8750 (96.8750) lr 7.9721e-04 eta 0:01:43
epoch [115/200] batch [1/2] time 1.013 (1.013) data 0.796 (0.796) loss 0.3035 (0.3035) acc 90.6250 (90.6250) lr 7.9721e-04 eta 0:02:53
epoch [115/200] batch [2/2] time 0.213 (0.613) data 0.000 (0.398) loss 0.3579 (0.3307) acc 87.5000 (89.0625) lr 7.8186e-04 eta 0:01:44
epoch [116/200] batch [1/2] time 0.999 (0.999) data 0.795 (0.795) loss 0.1432 (0.1432) acc 93.7500 (93.7500) lr 7.8186e-04 eta 0:02:48
epoch [116/200] batch [2/2] time 0.202 (0.601) data 0.000 (0.398) loss 0.2117 (0.1774) acc 90.6250 (92.1875) lr 7.6655e-04 eta 0:01:40
epoch [117/200] batch [1/2] time 1.009 (1.009) data 0.784 (0.784) loss 0.1885 (0.1885) acc 96.8750 (96.8750) lr 7.6655e-04 eta 0:02:48
epoch [117/200] batch [2/2] time 0.213 (0.611) data 0.000 (0.392) loss 0.1809 (0.1847) acc 93.7500 (95.3125) lr 7.5131e-04 eta 0:01:41
epoch [118/200] batch [1/2] time 1.100 (1.100) data 0.891 (0.891) loss 0.1913 (0.1913) acc 96.8750 (96.8750) lr 7.5131e-04 eta 0:03:01
epoch [118/200] batch [2/2] time 0.192 (0.646) data 0.000 (0.445) loss 0.1876 (0.1895) acc 96.8750 (96.8750) lr 7.3613e-04 eta 0:01:45
epoch [119/200] batch [1/2] time 0.980 (0.980) data 0.779 (0.779) loss 0.3567 (0.3567) acc 93.7500 (93.7500) lr 7.3613e-04 eta 0:02:39
epoch [119/200] batch [2/2] time 0.192 (0.586) data 0.000 (0.390) loss 0.1302 (0.2435) acc 96.8750 (95.3125) lr 7.2101e-04 eta 0:01:34
epoch [120/200] batch [1/2] time 0.990 (0.990) data 0.791 (0.791) loss 0.1207 (0.1207) acc 93.7500 (93.7500) lr 7.2101e-04 eta 0:02:39
epoch [120/200] batch [2/2] time 0.188 (0.589) data 0.000 (0.395) loss 0.2417 (0.1812) acc 87.5000 (90.6250) lr 7.0596e-04 eta 0:01:34
epoch [121/200] batch [1/2] time 0.977 (0.977) data 0.776 (0.776) loss 0.1249 (0.1249) acc 100.0000 (100.0000) lr 7.0596e-04 eta 0:02:35
epoch [121/200] batch [2/2] time 0.194 (0.586) data 0.000 (0.388) loss 0.2405 (0.1827) acc 96.8750 (98.4375) lr 6.9098e-04 eta 0:01:32
epoch [122/200] batch [1/2] time 1.014 (1.014) data 0.794 (0.794) loss 0.2153 (0.2153) acc 93.7500 (93.7500) lr 6.9098e-04 eta 0:02:39
epoch [122/200] batch [2/2] time 0.210 (0.612) data 0.000 (0.397) loss 0.0627 (0.1390) acc 100.0000 (96.8750) lr 6.7608e-04 eta 0:01:35
epoch [123/200] batch [1/2] time 1.057 (1.057) data 0.846 (0.846) loss 0.2218 (0.2218) acc 93.7500 (93.7500) lr 6.7608e-04 eta 0:02:43
epoch [123/200] batch [2/2] time 0.198 (0.627) data 0.000 (0.423) loss 0.2161 (0.2189) acc 96.8750 (95.3125) lr 6.6126e-04 eta 0:01:36
epoch [124/200] batch [1/2] time 1.094 (1.094) data 0.881 (0.881) loss 0.4277 (0.4277) acc 81.2500 (81.2500) lr 6.6126e-04 eta 0:02:47
epoch [124/200] batch [2/2] time 0.201 (0.648) data 0.000 (0.440) loss 0.2766 (0.3522) acc 90.6250 (85.9375) lr 6.4653e-04 eta 0:01:38
epoch [125/200] batch [1/2] time 1.008 (1.008) data 0.792 (0.792) loss 0.1610 (0.1610) acc 96.8750 (96.8750) lr 6.4653e-04 eta 0:02:32
epoch [125/200] batch [2/2] time 0.202 (0.605) data 0.000 (0.396) loss 0.3704 (0.2657) acc 87.5000 (92.1875) lr 6.3188e-04 eta 0:01:30
epoch [126/200] batch [1/2] time 1.120 (1.120) data 0.931 (0.931) loss 0.1877 (0.1877) acc 96.8750 (96.8750) lr 6.3188e-04 eta 0:02:46
epoch [126/200] batch [2/2] time 0.198 (0.659) data 0.000 (0.466) loss 0.1265 (0.1571) acc 100.0000 (98.4375) lr 6.1732e-04 eta 0:01:37
epoch [127/200] batch [1/2] time 1.005 (1.005) data 0.795 (0.795) loss 0.3298 (0.3298) acc 87.5000 (87.5000) lr 6.1732e-04 eta 0:02:27
epoch [127/200] batch [2/2] time 0.212 (0.609) data 0.000 (0.397) loss 0.3435 (0.3367) acc 84.3750 (85.9375) lr 6.0285e-04 eta 0:01:28
epoch [128/200] batch [1/2] time 0.993 (0.993) data 0.783 (0.783) loss 0.3640 (0.3640) acc 84.3750 (84.3750) lr 6.0285e-04 eta 0:02:23
epoch [128/200] batch [2/2] time 0.199 (0.596) data 0.000 (0.392) loss 0.2200 (0.2920) acc 96.8750 (90.6250) lr 5.8849e-04 eta 0:01:25
epoch [129/200] batch [1/2] time 1.028 (1.028) data 0.816 (0.816) loss 0.1185 (0.1185) acc 100.0000 (100.0000) lr 5.8849e-04 eta 0:02:27
epoch [129/200] batch [2/2] time 0.213 (0.621) data 0.000 (0.408) loss 0.1675 (0.1430) acc 93.7500 (96.8750) lr 5.7422e-04 eta 0:01:28
epoch [130/200] batch [1/2] time 1.011 (1.011) data 0.804 (0.804) loss 0.2233 (0.2233) acc 90.6250 (90.6250) lr 5.7422e-04 eta 0:02:22
epoch [130/200] batch [2/2] time 0.199 (0.605) data 0.000 (0.402) loss 0.2661 (0.2447) acc 90.6250 (90.6250) lr 5.6006e-04 eta 0:01:24
epoch [131/200] batch [1/2] time 1.001 (1.001) data 0.777 (0.777) loss 0.1420 (0.1420) acc 96.8750 (96.8750) lr 5.6006e-04 eta 0:02:19
epoch [131/200] batch [2/2] time 0.196 (0.599) data 0.000 (0.389) loss 0.1281 (0.1350) acc 100.0000 (98.4375) lr 5.4601e-04 eta 0:01:22
epoch [132/200] batch [1/2] time 1.040 (1.040) data 0.851 (0.851) loss 0.1608 (0.1608) acc 96.8750 (96.8750) lr 5.4601e-04 eta 0:02:22
epoch [132/200] batch [2/2] time 0.199 (0.620) data 0.000 (0.425) loss 0.2720 (0.2164) acc 90.6250 (93.7500) lr 5.3207e-04 eta 0:01:24
epoch [133/200] batch [1/2] time 0.978 (0.978) data 0.811 (0.811) loss 0.2261 (0.2261) acc 93.7500 (93.7500) lr 5.3207e-04 eta 0:02:12
epoch [133/200] batch [2/2] time 0.200 (0.589) data 0.000 (0.405) loss 0.2312 (0.2286) acc 93.7500 (93.7500) lr 5.1825e-04 eta 0:01:18
epoch [134/200] batch [1/2] time 1.020 (1.020) data 0.815 (0.815) loss 0.1447 (0.1447) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:02:15
epoch [134/200] batch [2/2] time 0.209 (0.615) data 0.000 (0.407) loss 0.1011 (0.1229) acc 100.0000 (98.4375) lr 5.0454e-04 eta 0:01:21
epoch [135/200] batch [1/2] time 1.031 (1.031) data 0.829 (0.829) loss 0.1516 (0.1516) acc 93.7500 (93.7500) lr 5.0454e-04 eta 0:02:15
epoch [135/200] batch [2/2] time 0.208 (0.620) data 0.000 (0.414) loss 0.2947 (0.2231) acc 90.6250 (92.1875) lr 4.9096e-04 eta 0:01:20
epoch [136/200] batch [1/2] time 0.998 (0.998) data 0.784 (0.784) loss 0.4048 (0.4048) acc 90.6250 (90.6250) lr 4.9096e-04 eta 0:02:08
epoch [136/200] batch [2/2] time 0.196 (0.597) data 0.000 (0.392) loss 0.1390 (0.2719) acc 96.8750 (93.7500) lr 4.7750e-04 eta 0:01:16
epoch [137/200] batch [1/2] time 0.985 (0.985) data 0.791 (0.791) loss 0.2585 (0.2585) acc 96.8750 (96.8750) lr 4.7750e-04 eta 0:02:05
epoch [137/200] batch [2/2] time 0.188 (0.586) data 0.000 (0.395) loss 0.3323 (0.2954) acc 90.6250 (93.7500) lr 4.6417e-04 eta 0:01:13
epoch [138/200] batch [1/2] time 0.997 (0.997) data 0.784 (0.784) loss 0.1797 (0.1797) acc 93.7500 (93.7500) lr 4.6417e-04 eta 0:02:04
epoch [138/200] batch [2/2] time 0.200 (0.598) data 0.000 (0.392) loss 0.1853 (0.1825) acc 96.8750 (95.3125) lr 4.5098e-04 eta 0:01:14
epoch [139/200] batch [1/2] time 1.029 (1.029) data 0.834 (0.834) loss 0.1753 (0.1753) acc 96.8750 (96.8750) lr 4.5098e-04 eta 0:02:06
epoch [139/200] batch [2/2] time 0.202 (0.615) data 0.000 (0.417) loss 0.3665 (0.2709) acc 87.5000 (92.1875) lr 4.3792e-04 eta 0:01:15
epoch [140/200] batch [1/2] time 1.019 (1.019) data 0.801 (0.801) loss 0.2013 (0.2013) acc 93.7500 (93.7500) lr 4.3792e-04 eta 0:02:03
epoch [140/200] batch [2/2] time 0.194 (0.606) data 0.000 (0.400) loss 0.2705 (0.2359) acc 90.6250 (92.1875) lr 4.2499e-04 eta 0:01:12
epoch [141/200] batch [1/2] time 0.990 (0.990) data 0.781 (0.781) loss 0.1615 (0.1615) acc 100.0000 (100.0000) lr 4.2499e-04 eta 0:01:57
epoch [141/200] batch [2/2] time 0.205 (0.597) data 0.000 (0.391) loss 0.4028 (0.2822) acc 90.6250 (95.3125) lr 4.1221e-04 eta 0:01:10
epoch [142/200] batch [1/2] time 0.993 (0.993) data 0.791 (0.791) loss 0.2020 (0.2020) acc 93.7500 (93.7500) lr 4.1221e-04 eta 0:01:56
epoch [142/200] batch [2/2] time 0.207 (0.600) data 0.000 (0.396) loss 0.1460 (0.1740) acc 96.8750 (95.3125) lr 3.9958e-04 eta 0:01:09
epoch [143/200] batch [1/2] time 1.008 (1.008) data 0.795 (0.795) loss 0.2094 (0.2094) acc 93.7500 (93.7500) lr 3.9958e-04 eta 0:01:55
epoch [143/200] batch [2/2] time 0.202 (0.605) data 0.000 (0.398) loss 0.1812 (0.1953) acc 96.8750 (95.3125) lr 3.8709e-04 eta 0:01:08
epoch [144/200] batch [1/2] time 0.977 (0.977) data 0.806 (0.806) loss 0.2175 (0.2175) acc 90.6250 (90.6250) lr 3.8709e-04 eta 0:01:50
epoch [144/200] batch [2/2] time 0.195 (0.586) data 0.000 (0.403) loss 0.2188 (0.2181) acc 93.7500 (92.1875) lr 3.7476e-04 eta 0:01:05
epoch [145/200] batch [1/2] time 1.028 (1.028) data 0.818 (0.818) loss 0.2329 (0.2329) acc 93.7500 (93.7500) lr 3.7476e-04 eta 0:01:54
epoch [145/200] batch [2/2] time 0.202 (0.615) data 0.000 (0.409) loss 0.2291 (0.2310) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:01:07
epoch [146/200] batch [1/2] time 1.010 (1.010) data 0.799 (0.799) loss 0.2649 (0.2649) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:01:50
epoch [146/200] batch [2/2] time 0.197 (0.604) data 0.000 (0.400) loss 0.1793 (0.2221) acc 96.8750 (95.3125) lr 3.5055e-04 eta 0:01:05
epoch [147/200] batch [1/2] time 0.978 (0.978) data 0.769 (0.769) loss 0.3452 (0.3452) acc 90.6250 (90.6250) lr 3.5055e-04 eta 0:01:44
epoch [147/200] batch [2/2] time 0.190 (0.584) data 0.000 (0.385) loss 0.1588 (0.2520) acc 96.8750 (93.7500) lr 3.3869e-04 eta 0:01:01
epoch [148/200] batch [1/2] time 1.025 (1.025) data 0.826 (0.826) loss 0.2458 (0.2458) acc 93.7500 (93.7500) lr 3.3869e-04 eta 0:01:47
epoch [148/200] batch [2/2] time 0.205 (0.615) data 0.000 (0.413) loss 0.3279 (0.2869) acc 93.7500 (93.7500) lr 3.2699e-04 eta 0:01:04
epoch [149/200] batch [1/2] time 1.004 (1.004) data 0.812 (0.812) loss 0.1788 (0.1788) acc 90.6250 (90.6250) lr 3.2699e-04 eta 0:01:43
epoch [149/200] batch [2/2] time 0.192 (0.598) data 0.000 (0.406) loss 0.2399 (0.2094) acc 90.6250 (90.6250) lr 3.1545e-04 eta 0:01:00
epoch [150/200] batch [1/2] time 1.019 (1.019) data 0.807 (0.807) loss 0.1216 (0.1216) acc 100.0000 (100.0000) lr 3.1545e-04 eta 0:01:42
epoch [150/200] batch [2/2] time 0.199 (0.609) data 0.000 (0.403) loss 0.1617 (0.1417) acc 96.8750 (98.4375) lr 3.0409e-04 eta 0:01:00
epoch [151/200] batch [1/2] time 1.010 (1.010) data 0.810 (0.810) loss 0.3628 (0.3628) acc 87.5000 (87.5000) lr 3.0409e-04 eta 0:01:40
epoch [151/200] batch [2/2] time 0.209 (0.610) data 0.000 (0.405) loss 0.2234 (0.2931) acc 93.7500 (90.6250) lr 2.9289e-04 eta 0:00:59
epoch [152/200] batch [1/2] time 1.098 (1.098) data 0.878 (0.878) loss 0.1405 (0.1405) acc 96.8750 (96.8750) lr 2.9289e-04 eta 0:01:46
epoch [152/200] batch [2/2] time 0.209 (0.654) data 0.000 (0.439) loss 0.2455 (0.1930) acc 93.7500 (95.3125) lr 2.8187e-04 eta 0:01:02
epoch [153/200] batch [1/2] time 0.990 (0.990) data 0.770 (0.770) loss 0.2217 (0.2217) acc 96.8750 (96.8750) lr 2.8187e-04 eta 0:01:34
epoch [153/200] batch [2/2] time 0.207 (0.598) data 0.000 (0.385) loss 0.2817 (0.2517) acc 93.7500 (95.3125) lr 2.7103e-04 eta 0:00:56
epoch [154/200] batch [1/2] time 1.013 (1.013) data 0.814 (0.814) loss 0.1305 (0.1305) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:01:34
epoch [154/200] batch [2/2] time 0.205 (0.609) data 0.000 (0.407) loss 0.0904 (0.1104) acc 96.8750 (96.8750) lr 2.6037e-04 eta 0:00:56
epoch [155/200] batch [1/2] time 1.011 (1.011) data 0.797 (0.797) loss 0.1361 (0.1361) acc 100.0000 (100.0000) lr 2.6037e-04 eta 0:01:31
epoch [155/200] batch [2/2] time 0.197 (0.604) data 0.000 (0.399) loss 0.2705 (0.2033) acc 96.8750 (98.4375) lr 2.4989e-04 eta 0:00:54
epoch [156/200] batch [1/2] time 1.028 (1.028) data 0.823 (0.823) loss 0.1559 (0.1559) acc 96.8750 (96.8750) lr 2.4989e-04 eta 0:01:31
epoch [156/200] batch [2/2] time 0.185 (0.606) data 0.000 (0.412) loss 0.2646 (0.2103) acc 93.7500 (95.3125) lr 2.3959e-04 eta 0:00:53
epoch [157/200] batch [1/2] time 1.003 (1.003) data 0.791 (0.791) loss 0.2316 (0.2316) acc 93.7500 (93.7500) lr 2.3959e-04 eta 0:01:27
epoch [157/200] batch [2/2] time 0.212 (0.608) data 0.000 (0.396) loss 0.2629 (0.2473) acc 93.7500 (93.7500) lr 2.2949e-04 eta 0:00:52
epoch [158/200] batch [1/2] time 1.016 (1.016) data 0.801 (0.801) loss 0.2676 (0.2676) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:01:26
epoch [158/200] batch [2/2] time 0.201 (0.608) data 0.000 (0.400) loss 0.1698 (0.2187) acc 96.8750 (93.7500) lr 2.1957e-04 eta 0:00:51
epoch [159/200] batch [1/2] time 1.040 (1.040) data 0.826 (0.826) loss 0.1248 (0.1248) acc 96.8750 (96.8750) lr 2.1957e-04 eta 0:01:26
epoch [159/200] batch [2/2] time 0.210 (0.625) data 0.000 (0.413) loss 0.2776 (0.2012) acc 90.6250 (93.7500) lr 2.0984e-04 eta 0:00:51
epoch [160/200] batch [1/2] time 0.963 (0.963) data 0.791 (0.791) loss 0.1724 (0.1724) acc 96.8750 (96.8750) lr 2.0984e-04 eta 0:01:18
epoch [160/200] batch [2/2] time 0.207 (0.585) data 0.000 (0.396) loss 0.1403 (0.1563) acc 100.0000 (98.4375) lr 2.0032e-04 eta 0:00:46
epoch [161/200] batch [1/2] time 1.003 (1.003) data 0.782 (0.782) loss 0.1656 (0.1656) acc 93.7500 (93.7500) lr 2.0032e-04 eta 0:01:19
epoch [161/200] batch [2/2] time 0.221 (0.612) data 0.000 (0.391) loss 0.3813 (0.2735) acc 87.5000 (90.6250) lr 1.9098e-04 eta 0:00:47
epoch [162/200] batch [1/2] time 1.049 (1.049) data 0.827 (0.827) loss 0.1538 (0.1538) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:01:20
epoch [162/200] batch [2/2] time 0.209 (0.629) data 0.000 (0.414) loss 0.2109 (0.1824) acc 93.7500 (95.3125) lr 1.8185e-04 eta 0:00:47
epoch [163/200] batch [1/2] time 1.094 (1.094) data 0.904 (0.904) loss 0.1259 (0.1259) acc 96.8750 (96.8750) lr 1.8185e-04 eta 0:01:22
epoch [163/200] batch [2/2] time 0.202 (0.648) data 0.000 (0.452) loss 0.1558 (0.1408) acc 93.7500 (95.3125) lr 1.7292e-04 eta 0:00:47
epoch [164/200] batch [1/2] time 1.001 (1.001) data 0.788 (0.788) loss 0.2869 (0.2869) acc 93.7500 (93.7500) lr 1.7292e-04 eta 0:01:13
epoch [164/200] batch [2/2] time 0.212 (0.606) data 0.000 (0.394) loss 0.2295 (0.2582) acc 96.8750 (95.3125) lr 1.6419e-04 eta 0:00:43
epoch [165/200] batch [1/2] time 1.002 (1.002) data 0.793 (0.793) loss 0.1556 (0.1556) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:01:11
epoch [165/200] batch [2/2] time 0.207 (0.605) data 0.000 (0.397) loss 0.1917 (0.1736) acc 96.8750 (95.3125) lr 1.5567e-04 eta 0:00:42
epoch [166/200] batch [1/2] time 1.076 (1.076) data 0.869 (0.869) loss 0.1860 (0.1860) acc 96.8750 (96.8750) lr 1.5567e-04 eta 0:01:14
epoch [166/200] batch [2/2] time 0.197 (0.636) data 0.000 (0.435) loss 0.1683 (0.1772) acc 100.0000 (98.4375) lr 1.4736e-04 eta 0:00:43
epoch [167/200] batch [1/2] time 0.977 (0.977) data 0.784 (0.784) loss 0.3928 (0.3928) acc 87.5000 (87.5000) lr 1.4736e-04 eta 0:01:05
epoch [167/200] batch [2/2] time 0.206 (0.592) data 0.000 (0.392) loss 0.1917 (0.2922) acc 96.8750 (92.1875) lr 1.3926e-04 eta 0:00:39
epoch [168/200] batch [1/2] time 1.002 (1.002) data 0.789 (0.789) loss 0.0756 (0.0756) acc 100.0000 (100.0000) lr 1.3926e-04 eta 0:01:05
epoch [168/200] batch [2/2] time 0.198 (0.600) data 0.000 (0.395) loss 0.4075 (0.2415) acc 81.2500 (90.6250) lr 1.3137e-04 eta 0:00:38
epoch [169/200] batch [1/2] time 1.084 (1.084) data 0.883 (0.883) loss 0.1947 (0.1947) acc 93.7500 (93.7500) lr 1.3137e-04 eta 0:01:08
epoch [169/200] batch [2/2] time 0.194 (0.639) data 0.000 (0.442) loss 0.1097 (0.1522) acc 100.0000 (96.8750) lr 1.2369e-04 eta 0:00:39
epoch [170/200] batch [1/2] time 1.046 (1.046) data 0.837 (0.837) loss 0.1415 (0.1415) acc 96.8750 (96.8750) lr 1.2369e-04 eta 0:01:03
epoch [170/200] batch [2/2] time 0.209 (0.628) data 0.000 (0.419) loss 0.1060 (0.1237) acc 100.0000 (98.4375) lr 1.1623e-04 eta 0:00:37
epoch [171/200] batch [1/2] time 1.338 (1.338) data 0.969 (0.969) loss 0.0875 (0.0875) acc 100.0000 (100.0000) lr 1.1623e-04 eta 0:01:18
epoch [171/200] batch [2/2] time 0.185 (0.762) data 0.000 (0.485) loss 0.3533 (0.2204) acc 90.6250 (95.3125) lr 1.0899e-04 eta 0:00:44
epoch [172/200] batch [1/2] time 1.028 (1.028) data 0.806 (0.806) loss 0.1990 (0.1990) acc 90.6250 (90.6250) lr 1.0899e-04 eta 0:00:58
epoch [172/200] batch [2/2] time 0.203 (0.616) data 0.000 (0.403) loss 0.1188 (0.1589) acc 96.8750 (93.7500) lr 1.0197e-04 eta 0:00:34
epoch [173/200] batch [1/2] time 1.093 (1.093) data 0.883 (0.883) loss 0.1812 (0.1812) acc 96.8750 (96.8750) lr 1.0197e-04 eta 0:01:00
epoch [173/200] batch [2/2] time 0.190 (0.642) data 0.000 (0.442) loss 0.1711 (0.1761) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:00:34
epoch [174/200] batch [1/2] time 1.009 (1.009) data 0.788 (0.788) loss 0.2664 (0.2664) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:00:53
epoch [174/200] batch [2/2] time 0.214 (0.612) data 0.000 (0.394) loss 0.1069 (0.1866) acc 100.0000 (95.3125) lr 8.8597e-05 eta 0:00:31
epoch [175/200] batch [1/2] time 1.028 (1.028) data 0.806 (0.806) loss 0.1237 (0.1237) acc 100.0000 (100.0000) lr 8.8597e-05 eta 0:00:52
epoch [175/200] batch [2/2] time 0.206 (0.617) data 0.000 (0.403) loss 0.0900 (0.1068) acc 100.0000 (100.0000) lr 8.2245e-05 eta 0:00:30
epoch [176/200] batch [1/2] time 0.989 (0.989) data 0.772 (0.772) loss 0.2517 (0.2517) acc 90.6250 (90.6250) lr 8.2245e-05 eta 0:00:48
epoch [176/200] batch [2/2] time 0.207 (0.598) data 0.000 (0.386) loss 0.1265 (0.1891) acc 100.0000 (95.3125) lr 7.6120e-05 eta 0:00:28
epoch [177/200] batch [1/2] time 1.036 (1.036) data 0.829 (0.829) loss 0.1576 (0.1576) acc 96.8750 (96.8750) lr 7.6120e-05 eta 0:00:48
epoch [177/200] batch [2/2] time 0.189 (0.612) data 0.000 (0.414) loss 0.1112 (0.1344) acc 100.0000 (98.4375) lr 7.0224e-05 eta 0:00:28
epoch [178/200] batch [1/2] time 0.980 (0.980) data 0.799 (0.799) loss 0.2827 (0.2827) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:00:44
epoch [178/200] batch [2/2] time 0.199 (0.589) data 0.000 (0.400) loss 0.2012 (0.2419) acc 93.7500 (92.1875) lr 6.4556e-05 eta 0:00:25
epoch [179/200] batch [1/2] time 1.027 (1.027) data 0.833 (0.833) loss 0.2499 (0.2499) acc 93.7500 (93.7500) lr 6.4556e-05 eta 0:00:44
epoch [179/200] batch [2/2] time 0.196 (0.612) data 0.000 (0.417) loss 0.1975 (0.2237) acc 96.8750 (95.3125) lr 5.9119e-05 eta 0:00:25
epoch [180/200] batch [1/2] time 1.080 (1.080) data 0.863 (0.863) loss 0.0761 (0.0761) acc 100.0000 (100.0000) lr 5.9119e-05 eta 0:00:44
epoch [180/200] batch [2/2] time 0.207 (0.643) data 0.000 (0.432) loss 0.2952 (0.1856) acc 87.5000 (93.7500) lr 5.3915e-05 eta 0:00:25
epoch [181/200] batch [1/2] time 0.997 (0.997) data 0.777 (0.777) loss 0.2417 (0.2417) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:00:38
epoch [181/200] batch [2/2] time 0.208 (0.603) data 0.000 (0.388) loss 0.1052 (0.1734) acc 100.0000 (98.4375) lr 4.8943e-05 eta 0:00:22
epoch [182/200] batch [1/2] time 1.135 (1.135) data 0.935 (0.935) loss 0.2734 (0.2734) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:42
epoch [182/200] batch [2/2] time 0.203 (0.669) data 0.000 (0.467) loss 0.2876 (0.2805) acc 90.6250 (92.1875) lr 4.4207e-05 eta 0:00:24
epoch [183/200] batch [1/2] time 1.004 (1.004) data 0.799 (0.799) loss 0.3237 (0.3237) acc 90.6250 (90.6250) lr 4.4207e-05 eta 0:00:35
epoch [183/200] batch [2/2] time 0.206 (0.605) data 0.000 (0.400) loss 0.1023 (0.2130) acc 100.0000 (95.3125) lr 3.9706e-05 eta 0:00:20
epoch [184/200] batch [1/2] time 1.037 (1.037) data 0.807 (0.807) loss 0.1779 (0.1779) acc 100.0000 (100.0000) lr 3.9706e-05 eta 0:00:34
epoch [184/200] batch [2/2] time 0.205 (0.621) data 0.000 (0.404) loss 0.1827 (0.1803) acc 90.6250 (95.3125) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [1/2] time 0.955 (0.955) data 0.783 (0.783) loss 0.1635 (0.1635) acc 96.8750 (96.8750) lr 3.5443e-05 eta 0:00:29
epoch [185/200] batch [2/2] time 0.184 (0.570) data 0.000 (0.391) loss 0.1437 (0.1536) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [1/2] time 1.016 (1.016) data 0.812 (0.812) loss 0.3940 (0.3940) acc 90.6250 (90.6250) lr 3.1417e-05 eta 0:00:29
epoch [186/200] batch [2/2] time 0.211 (0.613) data 0.000 (0.406) loss 0.2830 (0.3385) acc 87.5000 (89.0625) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [1/2] time 1.008 (1.008) data 0.787 (0.787) loss 0.1143 (0.1143) acc 100.0000 (100.0000) lr 2.7630e-05 eta 0:00:27
epoch [187/200] batch [2/2] time 0.223 (0.616) data 0.000 (0.393) loss 0.1976 (0.1559) acc 93.7500 (96.8750) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [1/2] time 1.045 (1.045) data 0.863 (0.863) loss 0.1210 (0.1210) acc 100.0000 (100.0000) lr 2.4083e-05 eta 0:00:26
epoch [188/200] batch [2/2] time 0.187 (0.616) data 0.000 (0.432) loss 0.2473 (0.1842) acc 93.7500 (96.8750) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [1/2] time 1.095 (1.095) data 0.902 (0.902) loss 0.1903 (0.1903) acc 93.7500 (93.7500) lr 2.0777e-05 eta 0:00:25
epoch [189/200] batch [2/2] time 0.200 (0.648) data 0.000 (0.451) loss 0.2769 (0.2336) acc 90.6250 (92.1875) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [1/2] time 1.003 (1.003) data 0.802 (0.802) loss 0.2803 (0.2803) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:21
epoch [190/200] batch [2/2] time 0.197 (0.600) data 0.000 (0.401) loss 0.2327 (0.2565) acc 93.7500 (93.7500) lr 1.4891e-05 eta 0:00:12
epoch [191/200] batch [1/2] time 0.960 (0.960) data 0.787 (0.787) loss 0.2279 (0.2279) acc 93.7500 (93.7500) lr 1.4891e-05 eta 0:00:18
epoch [191/200] batch [2/2] time 0.201 (0.581) data 0.000 (0.394) loss 0.1285 (0.1782) acc 96.8750 (95.3125) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [1/2] time 1.092 (1.092) data 0.880 (0.880) loss 0.2942 (0.2942) acc 93.7500 (93.7500) lr 1.2312e-05 eta 0:00:18
epoch [192/200] batch [2/2] time 0.211 (0.651) data 0.000 (0.440) loss 0.1829 (0.2385) acc 93.7500 (93.7500) lr 9.9763e-06 eta 0:00:10
epoch [193/200] batch [1/2] time 1.006 (1.006) data 0.796 (0.796) loss 0.1835 (0.1835) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:15
epoch [193/200] batch [2/2] time 0.209 (0.607) data 0.000 (0.398) loss 0.2208 (0.2021) acc 93.7500 (95.3125) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [1/2] time 1.015 (1.015) data 0.813 (0.813) loss 0.1367 (0.1367) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:13
epoch [194/200] batch [2/2] time 0.206 (0.611) data 0.000 (0.407) loss 0.2957 (0.2162) acc 90.6250 (92.1875) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [1/2] time 1.015 (1.015) data 0.784 (0.784) loss 0.1213 (0.1213) acc 96.8750 (96.8750) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [2/2] time 0.212 (0.613) data 0.000 (0.392) loss 0.2162 (0.1687) acc 90.6250 (93.7500) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [1/2] time 1.006 (1.006) data 0.788 (0.788) loss 0.2705 (0.2705) acc 96.8750 (96.8750) lr 4.4380e-06 eta 0:00:09
epoch [196/200] batch [2/2] time 0.203 (0.604) data 0.000 (0.394) loss 0.0726 (0.1715) acc 100.0000 (98.4375) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [1/2] time 0.995 (0.995) data 0.782 (0.782) loss 0.2056 (0.2056) acc 93.7500 (93.7500) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [2/2] time 0.206 (0.600) data 0.000 (0.391) loss 0.1733 (0.1895) acc 93.7500 (93.7500) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [1/2] time 0.992 (0.992) data 0.772 (0.772) loss 0.1985 (0.1985) acc 90.6250 (90.6250) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [2/2] time 0.186 (0.589) data 0.000 (0.386) loss 0.3738 (0.2861) acc 90.6250 (90.6250) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [1/2] time 0.998 (0.998) data 0.801 (0.801) loss 0.1073 (0.1073) acc 96.8750 (96.8750) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [2/2] time 0.192 (0.595) data 0.000 (0.401) loss 0.4395 (0.2734) acc 87.5000 (92.1875) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [1/2] time 1.063 (1.063) data 0.870 (0.870) loss 0.0922 (0.0922) acc 100.0000 (100.0000) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [2/2] time 0.193 (0.628) data 0.000 (0.435) loss 0.2025 (0.1473) acc 93.7500 (96.8750) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,100
* correct: 6,974
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.8%
training time for 6-th round: 352.21 seconds
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
| Calculating uncertainty of Unlabeled set
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
INITIALIZE the prompts weights
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
['X X X X X X X X X X X X X X X X Annual Crop Land.', 'X X X X X X X X X X X X X X X X Forest.', 'X X X X X X X X X X X X X X X X Herbaceous Vegetation Land.', 'X X X X X X X X X X X X X X X X Highway or Road.', 'X X X X X X X X X X X X X X X X Industrial Buildings.', 'X X X X X X X X X X X X X X X X Pasture Land.', 'X X X X X X X X X X X X X X X X Permanent Crop Land.', 'X X X X X X X X X X X X X X X X Residential Buildings.', 'X X X X X X X X X X X X X X X X River.', 'X X X X X X X X X X X X X X X X Sea or Lake.']
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
CustomCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
Turning off gradients in both the image and the text encoder
Multiple GPUs detected (n_gpus=2), use all of them!
DataParallel(
  (module): CustomCLIP(
    (prompt_learner): PromptLearner()
    (image_encoder): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
epoch [1/200] batch [1/2] time 0.995 (0.995) data 0.806 (0.806) loss 2.8965 (2.8965) acc 12.5000 (12.5000) lr 1.0000e-05 eta 0:06:37
epoch [1/200] batch [2/2] time 0.197 (0.596) data 0.000 (0.403) loss 2.4414 (2.6689) acc 6.2500 (9.3750) lr 2.0000e-03 eta 0:03:57
epoch [2/200] batch [1/2] time 1.071 (1.071) data 0.843 (0.843) loss 2.6484 (2.6484) acc 18.7500 (18.7500) lr 2.0000e-03 eta 0:07:05
epoch [2/200] batch [2/2] time 0.207 (0.639) data 0.000 (0.421) loss 2.7773 (2.7129) acc 18.7500 (18.7500) lr 1.9999e-03 eta 0:04:12
epoch [3/200] batch [1/2] time 1.114 (1.114) data 0.910 (0.910) loss 1.7676 (1.7676) acc 40.6250 (40.6250) lr 1.9999e-03 eta 0:07:20
epoch [3/200] batch [2/2] time 0.187 (0.651) data 0.000 (0.455) loss 2.1289 (1.9482) acc 28.1250 (34.3750) lr 1.9995e-03 eta 0:04:16
epoch [4/200] batch [1/2] time 1.081 (1.081) data 0.873 (0.873) loss 1.5137 (1.5137) acc 53.1250 (53.1250) lr 1.9995e-03 eta 0:07:05
epoch [4/200] batch [2/2] time 0.198 (0.640) data 0.000 (0.437) loss 2.1699 (1.8418) acc 25.0000 (39.0625) lr 1.9989e-03 eta 0:04:10
epoch [5/200] batch [1/2] time 1.002 (1.002) data 0.773 (0.773) loss 1.3555 (1.3555) acc 62.5000 (62.5000) lr 1.9989e-03 eta 0:06:31
epoch [5/200] batch [2/2] time 0.207 (0.605) data 0.000 (0.387) loss 1.5771 (1.4663) acc 50.0000 (56.2500) lr 1.9980e-03 eta 0:03:55
epoch [6/200] batch [1/2] time 0.982 (0.982) data 0.783 (0.783) loss 1.7598 (1.7598) acc 40.6250 (40.6250) lr 1.9980e-03 eta 0:06:21
epoch [6/200] batch [2/2] time 0.199 (0.591) data 0.000 (0.392) loss 1.5586 (1.6592) acc 40.6250 (40.6250) lr 1.9969e-03 eta 0:03:49
epoch [7/200] batch [1/2] time 1.016 (1.016) data 0.801 (0.801) loss 1.4619 (1.4619) acc 53.1250 (53.1250) lr 1.9969e-03 eta 0:06:33
epoch [7/200] batch [2/2] time 0.201 (0.608) data 0.000 (0.401) loss 1.2705 (1.3662) acc 56.2500 (54.6875) lr 1.9956e-03 eta 0:03:54
epoch [8/200] batch [1/2] time 1.212 (1.212) data 0.997 (0.997) loss 1.3477 (1.3477) acc 56.2500 (56.2500) lr 1.9956e-03 eta 0:07:46
epoch [8/200] batch [2/2] time 0.214 (0.713) data 0.000 (0.498) loss 1.1943 (1.2710) acc 56.2500 (56.2500) lr 1.9940e-03 eta 0:04:33
epoch [9/200] batch [1/2] time 1.091 (1.091) data 0.873 (0.873) loss 1.0977 (1.0977) acc 65.6250 (65.6250) lr 1.9940e-03 eta 0:06:58
epoch [9/200] batch [2/2] time 0.196 (0.644) data 0.000 (0.437) loss 1.3896 (1.2437) acc 59.3750 (62.5000) lr 1.9921e-03 eta 0:04:05
epoch [10/200] batch [1/2] time 1.012 (1.012) data 0.789 (0.789) loss 0.9355 (0.9355) acc 71.8750 (71.8750) lr 1.9921e-03 eta 0:06:25
epoch [10/200] batch [2/2] time 0.214 (0.613) data 0.000 (0.395) loss 1.0703 (1.0029) acc 62.5000 (67.1875) lr 1.9900e-03 eta 0:03:52
epoch [11/200] batch [1/2] time 0.987 (0.987) data 0.778 (0.778) loss 1.0488 (1.0488) acc 68.7500 (68.7500) lr 1.9900e-03 eta 0:06:14
epoch [11/200] batch [2/2] time 0.203 (0.595) data 0.000 (0.389) loss 0.9966 (1.0227) acc 65.6250 (67.1875) lr 1.9877e-03 eta 0:03:44
epoch [12/200] batch [1/2] time 1.031 (1.031) data 0.816 (0.816) loss 0.6938 (0.6938) acc 87.5000 (87.5000) lr 1.9877e-03 eta 0:06:28
epoch [12/200] batch [2/2] time 0.192 (0.611) data 0.000 (0.408) loss 0.8076 (0.7507) acc 75.0000 (81.2500) lr 1.9851e-03 eta 0:03:49
epoch [13/200] batch [1/2] time 1.004 (1.004) data 0.774 (0.774) loss 0.8013 (0.8013) acc 75.0000 (75.0000) lr 1.9851e-03 eta 0:06:16
epoch [13/200] batch [2/2] time 0.217 (0.610) data 0.000 (0.387) loss 0.7417 (0.7715) acc 75.0000 (75.0000) lr 1.9823e-03 eta 0:03:48
epoch [14/200] batch [1/2] time 1.034 (1.034) data 0.819 (0.819) loss 0.5488 (0.5488) acc 81.2500 (81.2500) lr 1.9823e-03 eta 0:06:25
epoch [14/200] batch [2/2] time 0.206 (0.620) data 0.000 (0.409) loss 0.8667 (0.7078) acc 68.7500 (75.0000) lr 1.9792e-03 eta 0:03:50
epoch [15/200] batch [1/2] time 0.968 (0.968) data 0.775 (0.775) loss 0.9644 (0.9644) acc 65.6250 (65.6250) lr 1.9792e-03 eta 0:05:59
epoch [15/200] batch [2/2] time 0.211 (0.590) data 0.000 (0.387) loss 0.8076 (0.8860) acc 71.8750 (68.7500) lr 1.9759e-03 eta 0:03:38
epoch [16/200] batch [1/2] time 1.035 (1.035) data 0.814 (0.814) loss 0.8979 (0.8979) acc 65.6250 (65.6250) lr 1.9759e-03 eta 0:06:22
epoch [16/200] batch [2/2] time 0.204 (0.620) data 0.000 (0.407) loss 0.6411 (0.7695) acc 78.1250 (71.8750) lr 1.9724e-03 eta 0:03:48
epoch [17/200] batch [1/2] time 1.014 (1.014) data 0.788 (0.788) loss 0.6807 (0.6807) acc 81.2500 (81.2500) lr 1.9724e-03 eta 0:06:12
epoch [17/200] batch [2/2] time 0.204 (0.609) data 0.000 (0.394) loss 0.7026 (0.6917) acc 78.1250 (79.6875) lr 1.9686e-03 eta 0:03:42
epoch [18/200] batch [1/2] time 1.027 (1.027) data 0.817 (0.817) loss 0.4668 (0.4668) acc 84.3750 (84.3750) lr 1.9686e-03 eta 0:06:14
epoch [18/200] batch [2/2] time 0.212 (0.620) data 0.000 (0.408) loss 0.8364 (0.6516) acc 71.8750 (78.1250) lr 1.9646e-03 eta 0:03:45
epoch [19/200] batch [1/2] time 1.031 (1.031) data 0.821 (0.821) loss 0.4661 (0.4661) acc 87.5000 (87.5000) lr 1.9646e-03 eta 0:06:14
epoch [19/200] batch [2/2] time 0.207 (0.619) data 0.000 (0.410) loss 0.8120 (0.6390) acc 75.0000 (81.2500) lr 1.9603e-03 eta 0:03:44
epoch [20/200] batch [1/2] time 0.995 (0.995) data 0.798 (0.798) loss 0.5571 (0.5571) acc 84.3750 (84.3750) lr 1.9603e-03 eta 0:05:59
epoch [20/200] batch [2/2] time 0.208 (0.602) data 0.000 (0.399) loss 0.6953 (0.6262) acc 75.0000 (79.6875) lr 1.9558e-03 eta 0:03:36
epoch [21/200] batch [1/2] time 1.044 (1.044) data 0.832 (0.832) loss 0.6646 (0.6646) acc 78.1250 (78.1250) lr 1.9558e-03 eta 0:06:14
epoch [21/200] batch [2/2] time 0.214 (0.629) data 0.000 (0.416) loss 0.7251 (0.6948) acc 71.8750 (75.0000) lr 1.9511e-03 eta 0:03:45
epoch [22/200] batch [1/2] time 0.992 (0.992) data 0.766 (0.766) loss 0.8149 (0.8149) acc 71.8750 (71.8750) lr 1.9511e-03 eta 0:05:54
epoch [22/200] batch [2/2] time 0.205 (0.599) data 0.000 (0.383) loss 0.4368 (0.6259) acc 84.3750 (78.1250) lr 1.9461e-03 eta 0:03:33
epoch [23/200] batch [1/2] time 1.034 (1.034) data 0.823 (0.823) loss 0.5059 (0.5059) acc 87.5000 (87.5000) lr 1.9461e-03 eta 0:06:07
epoch [23/200] batch [2/2] time 0.207 (0.620) data 0.000 (0.411) loss 0.4321 (0.4690) acc 87.5000 (87.5000) lr 1.9409e-03 eta 0:03:39
epoch [24/200] batch [1/2] time 1.092 (1.092) data 0.887 (0.887) loss 0.5586 (0.5586) acc 84.3750 (84.3750) lr 1.9409e-03 eta 0:06:25
epoch [24/200] batch [2/2] time 0.203 (0.647) data 0.000 (0.443) loss 0.9414 (0.7500) acc 68.7500 (76.5625) lr 1.9354e-03 eta 0:03:47
epoch [25/200] batch [1/2] time 1.103 (1.103) data 0.913 (0.913) loss 0.5947 (0.5947) acc 87.5000 (87.5000) lr 1.9354e-03 eta 0:06:27
epoch [25/200] batch [2/2] time 0.197 (0.650) data 0.000 (0.457) loss 0.5151 (0.5549) acc 84.3750 (85.9375) lr 1.9298e-03 eta 0:03:47
epoch [26/200] batch [1/2] time 1.008 (1.008) data 0.786 (0.786) loss 0.6279 (0.6279) acc 84.3750 (84.3750) lr 1.9298e-03 eta 0:05:51
epoch [26/200] batch [2/2] time 0.195 (0.602) data 0.000 (0.393) loss 0.3855 (0.5067) acc 90.6250 (87.5000) lr 1.9239e-03 eta 0:03:29
epoch [27/200] batch [1/2] time 0.993 (0.993) data 0.777 (0.777) loss 0.5366 (0.5366) acc 78.1250 (78.1250) lr 1.9239e-03 eta 0:05:44
epoch [27/200] batch [2/2] time 0.213 (0.603) data 0.000 (0.389) loss 0.4849 (0.5107) acc 84.3750 (81.2500) lr 1.9178e-03 eta 0:03:28
epoch [28/200] batch [1/2] time 1.020 (1.020) data 0.817 (0.817) loss 0.3542 (0.3542) acc 93.7500 (93.7500) lr 1.9178e-03 eta 0:05:52
epoch [28/200] batch [2/2] time 0.209 (0.615) data 0.000 (0.409) loss 0.7510 (0.5526) acc 71.8750 (82.8125) lr 1.9114e-03 eta 0:03:31
epoch [29/200] batch [1/2] time 1.026 (1.026) data 0.810 (0.810) loss 0.4727 (0.4727) acc 84.3750 (84.3750) lr 1.9114e-03 eta 0:05:51
epoch [29/200] batch [2/2] time 0.205 (0.615) data 0.000 (0.405) loss 0.5171 (0.4949) acc 84.3750 (84.3750) lr 1.9048e-03 eta 0:03:30
epoch [30/200] batch [1/2] time 1.003 (1.003) data 0.796 (0.796) loss 0.6636 (0.6636) acc 78.1250 (78.1250) lr 1.9048e-03 eta 0:05:41
epoch [30/200] batch [2/2] time 0.210 (0.606) data 0.000 (0.398) loss 0.4644 (0.5640) acc 81.2500 (79.6875) lr 1.8980e-03 eta 0:03:26
epoch [31/200] batch [1/2] time 1.113 (1.113) data 0.912 (0.912) loss 0.5015 (0.5015) acc 87.5000 (87.5000) lr 1.8980e-03 eta 0:06:17
epoch [31/200] batch [2/2] time 0.203 (0.658) data 0.000 (0.456) loss 0.4353 (0.4684) acc 93.7500 (90.6250) lr 1.8910e-03 eta 0:03:42
epoch [32/200] batch [1/2] time 1.012 (1.012) data 0.801 (0.801) loss 0.3926 (0.3926) acc 90.6250 (90.6250) lr 1.8910e-03 eta 0:05:41
epoch [32/200] batch [2/2] time 0.196 (0.604) data 0.000 (0.401) loss 0.4634 (0.4280) acc 84.3750 (87.5000) lr 1.8838e-03 eta 0:03:22
epoch [33/200] batch [1/2] time 1.035 (1.035) data 0.819 (0.819) loss 0.4636 (0.4636) acc 84.3750 (84.3750) lr 1.8838e-03 eta 0:05:46
epoch [33/200] batch [2/2] time 0.366 (0.701) data 0.000 (0.410) loss 0.5127 (0.4882) acc 87.5000 (85.9375) lr 1.8763e-03 eta 0:03:54
epoch [34/200] batch [1/2] time 1.019 (1.019) data 0.827 (0.827) loss 0.4644 (0.4644) acc 90.6250 (90.6250) lr 1.8763e-03 eta 0:05:39
epoch [34/200] batch [2/2] time 0.200 (0.610) data 0.000 (0.413) loss 0.5781 (0.5212) acc 78.1250 (84.3750) lr 1.8686e-03 eta 0:03:22
epoch [35/200] batch [1/2] time 0.988 (0.988) data 0.785 (0.785) loss 0.3252 (0.3252) acc 93.7500 (93.7500) lr 1.8686e-03 eta 0:05:26
epoch [35/200] batch [2/2] time 0.208 (0.598) data 0.000 (0.393) loss 0.3140 (0.3196) acc 93.7500 (93.7500) lr 1.8607e-03 eta 0:03:17
epoch [36/200] batch [1/2] time 1.083 (1.083) data 0.876 (0.876) loss 0.3740 (0.3740) acc 87.5000 (87.5000) lr 1.8607e-03 eta 0:05:56
epoch [36/200] batch [2/2] time 0.205 (0.644) data 0.000 (0.438) loss 0.6221 (0.4980) acc 81.2500 (84.3750) lr 1.8526e-03 eta 0:03:31
epoch [37/200] batch [1/2] time 1.019 (1.019) data 0.809 (0.809) loss 0.3501 (0.3501) acc 93.7500 (93.7500) lr 1.8526e-03 eta 0:05:33
epoch [37/200] batch [2/2] time 0.207 (0.613) data 0.000 (0.405) loss 0.3672 (0.3586) acc 93.7500 (93.7500) lr 1.8443e-03 eta 0:03:19
epoch [38/200] batch [1/2] time 1.113 (1.113) data 0.894 (0.894) loss 0.4580 (0.4580) acc 87.5000 (87.5000) lr 1.8443e-03 eta 0:06:01
epoch [38/200] batch [2/2] time 0.219 (0.666) data 0.000 (0.447) loss 0.3406 (0.3993) acc 90.6250 (89.0625) lr 1.8358e-03 eta 0:03:35
epoch [39/200] batch [1/2] time 0.975 (0.975) data 0.760 (0.760) loss 0.6328 (0.6328) acc 78.1250 (78.1250) lr 1.8358e-03 eta 0:05:15
epoch [39/200] batch [2/2] time 0.209 (0.592) data 0.000 (0.380) loss 0.5195 (0.5762) acc 78.1250 (78.1250) lr 1.8271e-03 eta 0:03:10
epoch [40/200] batch [1/2] time 1.027 (1.027) data 0.809 (0.809) loss 0.5166 (0.5166) acc 81.2500 (81.2500) lr 1.8271e-03 eta 0:05:29
epoch [40/200] batch [2/2] time 0.194 (0.611) data 0.000 (0.405) loss 0.4561 (0.4863) acc 84.3750 (82.8125) lr 1.8181e-03 eta 0:03:15
epoch [41/200] batch [1/2] time 0.996 (0.996) data 0.793 (0.793) loss 0.5483 (0.5483) acc 78.1250 (78.1250) lr 1.8181e-03 eta 0:05:17
epoch [41/200] batch [2/2] time 0.198 (0.597) data 0.000 (0.396) loss 0.2915 (0.4199) acc 93.7500 (85.9375) lr 1.8090e-03 eta 0:03:09
epoch [42/200] batch [1/2] time 1.003 (1.003) data 0.787 (0.787) loss 0.5078 (0.5078) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:05:17
epoch [42/200] batch [2/2] time 0.206 (0.605) data 0.000 (0.394) loss 0.2288 (0.3683) acc 96.8750 (92.1875) lr 1.7997e-03 eta 0:03:11
epoch [43/200] batch [1/2] time 1.066 (1.066) data 0.864 (0.864) loss 0.3064 (0.3064) acc 93.7500 (93.7500) lr 1.7997e-03 eta 0:05:35
epoch [43/200] batch [2/2] time 0.210 (0.638) data 0.000 (0.432) loss 0.4260 (0.3662) acc 90.6250 (92.1875) lr 1.7902e-03 eta 0:03:20
epoch [44/200] batch [1/2] time 1.014 (1.014) data 0.808 (0.808) loss 0.3494 (0.3494) acc 96.8750 (96.8750) lr 1.7902e-03 eta 0:05:17
epoch [44/200] batch [2/2] time 0.201 (0.608) data 0.000 (0.404) loss 0.5112 (0.4303) acc 81.2500 (89.0625) lr 1.7804e-03 eta 0:03:09
epoch [45/200] batch [1/2] time 1.033 (1.033) data 0.834 (0.834) loss 0.4888 (0.4888) acc 84.3750 (84.3750) lr 1.7804e-03 eta 0:05:21
epoch [45/200] batch [2/2] time 0.194 (0.614) data 0.000 (0.417) loss 0.3867 (0.4377) acc 87.5000 (85.9375) lr 1.7705e-03 eta 0:03:10
epoch [46/200] batch [1/2] time 0.968 (0.968) data 0.798 (0.798) loss 0.6021 (0.6021) acc 84.3750 (84.3750) lr 1.7705e-03 eta 0:04:59
epoch [46/200] batch [2/2] time 0.199 (0.584) data 0.000 (0.399) loss 0.4895 (0.5458) acc 81.2500 (82.8125) lr 1.7604e-03 eta 0:02:59
epoch [47/200] batch [1/2] time 1.002 (1.002) data 0.780 (0.780) loss 0.4370 (0.4370) acc 90.6250 (90.6250) lr 1.7604e-03 eta 0:05:07
epoch [47/200] batch [2/2] time 0.204 (0.603) data 0.000 (0.390) loss 0.4382 (0.4376) acc 84.3750 (87.5000) lr 1.7501e-03 eta 0:03:04
epoch [48/200] batch [1/2] time 0.987 (0.987) data 0.774 (0.774) loss 0.4307 (0.4307) acc 84.3750 (84.3750) lr 1.7501e-03 eta 0:05:01
epoch [48/200] batch [2/2] time 0.194 (0.591) data 0.000 (0.387) loss 0.2820 (0.3563) acc 90.6250 (87.5000) lr 1.7396e-03 eta 0:02:59
epoch [49/200] batch [1/2] time 0.996 (0.996) data 0.792 (0.792) loss 0.4097 (0.4097) acc 87.5000 (87.5000) lr 1.7396e-03 eta 0:05:01
epoch [49/200] batch [2/2] time 0.203 (0.599) data 0.000 (0.396) loss 0.3704 (0.3900) acc 84.3750 (85.9375) lr 1.7290e-03 eta 0:03:00
epoch [50/200] batch [1/2] time 0.974 (0.974) data 0.780 (0.780) loss 0.5581 (0.5581) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:04:53
epoch [50/200] batch [2/2] time 0.215 (0.594) data 0.000 (0.390) loss 0.5366 (0.5474) acc 81.2500 (82.8125) lr 1.7181e-03 eta 0:02:58
epoch [51/200] batch [1/2] time 1.021 (1.021) data 0.812 (0.812) loss 0.4714 (0.4714) acc 87.5000 (87.5000) lr 1.7181e-03 eta 0:05:05
epoch [51/200] batch [2/2] time 0.210 (0.615) data 0.000 (0.406) loss 0.3782 (0.4248) acc 87.5000 (87.5000) lr 1.7071e-03 eta 0:03:03
epoch [52/200] batch [1/2] time 1.029 (1.029) data 0.795 (0.795) loss 0.3030 (0.3030) acc 93.7500 (93.7500) lr 1.7071e-03 eta 0:05:05
epoch [52/200] batch [2/2] time 0.208 (0.618) data 0.000 (0.397) loss 0.2252 (0.2641) acc 96.8750 (95.3125) lr 1.6959e-03 eta 0:03:03
epoch [53/200] batch [1/2] time 1.021 (1.021) data 0.796 (0.796) loss 0.3350 (0.3350) acc 90.6250 (90.6250) lr 1.6959e-03 eta 0:05:01
epoch [53/200] batch [2/2] time 0.216 (0.619) data 0.000 (0.398) loss 0.5161 (0.4255) acc 84.3750 (87.5000) lr 1.6845e-03 eta 0:03:01
epoch [54/200] batch [1/2] time 1.049 (1.049) data 0.875 (0.875) loss 0.3003 (0.3003) acc 93.7500 (93.7500) lr 1.6845e-03 eta 0:05:07
epoch [54/200] batch [2/2] time 0.195 (0.622) data 0.000 (0.437) loss 0.3184 (0.3093) acc 87.5000 (90.6250) lr 1.6730e-03 eta 0:03:01
epoch [55/200] batch [1/2] time 1.044 (1.044) data 0.818 (0.818) loss 0.2788 (0.2788) acc 93.7500 (93.7500) lr 1.6730e-03 eta 0:05:03
epoch [55/200] batch [2/2] time 0.210 (0.627) data 0.000 (0.409) loss 0.3350 (0.3069) acc 93.7500 (93.7500) lr 1.6613e-03 eta 0:03:01
epoch [56/200] batch [1/2] time 1.062 (1.062) data 0.860 (0.860) loss 0.2629 (0.2629) acc 90.6250 (90.6250) lr 1.6613e-03 eta 0:05:06
epoch [56/200] batch [2/2] time 0.204 (0.633) data 0.000 (0.430) loss 0.2666 (0.2648) acc 96.8750 (93.7500) lr 1.6494e-03 eta 0:03:02
epoch [57/200] batch [1/2] time 0.973 (0.973) data 0.779 (0.779) loss 0.1931 (0.1931) acc 93.7500 (93.7500) lr 1.6494e-03 eta 0:04:39
epoch [57/200] batch [2/2] time 0.199 (0.586) data 0.000 (0.390) loss 0.4763 (0.3347) acc 90.6250 (92.1875) lr 1.6374e-03 eta 0:02:47
epoch [58/200] batch [1/2] time 1.008 (1.008) data 0.804 (0.804) loss 0.3582 (0.3582) acc 90.6250 (90.6250) lr 1.6374e-03 eta 0:04:47
epoch [58/200] batch [2/2] time 0.201 (0.605) data 0.000 (0.402) loss 0.4180 (0.3881) acc 90.6250 (90.6250) lr 1.6252e-03 eta 0:02:51
epoch [59/200] batch [1/2] time 1.034 (1.034) data 0.823 (0.823) loss 0.3816 (0.3816) acc 84.3750 (84.3750) lr 1.6252e-03 eta 0:04:52
epoch [59/200] batch [2/2] time 0.208 (0.621) data 0.000 (0.412) loss 0.2469 (0.3143) acc 96.8750 (90.6250) lr 1.6129e-03 eta 0:02:55
epoch [60/200] batch [1/2] time 1.002 (1.002) data 0.787 (0.787) loss 0.2477 (0.2477) acc 93.7500 (93.7500) lr 1.6129e-03 eta 0:04:41
epoch [60/200] batch [2/2] time 0.213 (0.607) data 0.000 (0.394) loss 0.2400 (0.2438) acc 90.6250 (92.1875) lr 1.6004e-03 eta 0:02:49
epoch [61/200] batch [1/2] time 1.009 (1.009) data 0.797 (0.797) loss 0.3074 (0.3074) acc 93.7500 (93.7500) lr 1.6004e-03 eta 0:04:41
epoch [61/200] batch [2/2] time 0.204 (0.606) data 0.000 (0.399) loss 0.1849 (0.2462) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:02:48
epoch [62/200] batch [1/2] time 1.059 (1.059) data 0.867 (0.867) loss 0.2576 (0.2576) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:04:53
epoch [62/200] batch [2/2] time 0.186 (0.622) data 0.000 (0.434) loss 0.3506 (0.3041) acc 93.7500 (92.1875) lr 1.5750e-03 eta 0:02:51
epoch [63/200] batch [1/2] time 1.157 (1.157) data 0.940 (0.940) loss 0.6021 (0.6021) acc 81.2500 (81.2500) lr 1.5750e-03 eta 0:05:18
epoch [63/200] batch [2/2] time 0.196 (0.676) data 0.000 (0.470) loss 0.3113 (0.4567) acc 93.7500 (87.5000) lr 1.5621e-03 eta 0:03:05
epoch [64/200] batch [1/2] time 1.035 (1.035) data 0.815 (0.815) loss 0.2183 (0.2183) acc 100.0000 (100.0000) lr 1.5621e-03 eta 0:04:42
epoch [64/200] batch [2/2] time 0.415 (0.725) data 0.000 (0.408) loss 0.3862 (0.3022) acc 90.6250 (95.3125) lr 1.5490e-03 eta 0:03:17
epoch [65/200] batch [1/2] time 1.014 (1.014) data 0.786 (0.786) loss 0.1477 (0.1477) acc 96.8750 (96.8750) lr 1.5490e-03 eta 0:04:34
epoch [65/200] batch [2/2] time 0.202 (0.608) data 0.000 (0.393) loss 0.2805 (0.2141) acc 93.7500 (95.3125) lr 1.5358e-03 eta 0:02:44
epoch [66/200] batch [1/2] time 1.025 (1.025) data 0.810 (0.810) loss 0.1960 (0.1960) acc 96.8750 (96.8750) lr 1.5358e-03 eta 0:04:35
epoch [66/200] batch [2/2] time 0.207 (0.616) data 0.000 (0.405) loss 0.4143 (0.3052) acc 90.6250 (93.7500) lr 1.5225e-03 eta 0:02:45
epoch [67/200] batch [1/2] time 1.029 (1.029) data 0.792 (0.792) loss 0.3560 (0.3560) acc 87.5000 (87.5000) lr 1.5225e-03 eta 0:04:34
epoch [67/200] batch [2/2] time 0.220 (0.624) data 0.000 (0.396) loss 0.2903 (0.3231) acc 90.6250 (89.0625) lr 1.5090e-03 eta 0:02:46
epoch [68/200] batch [1/2] time 1.209 (1.209) data 1.015 (1.015) loss 0.2954 (0.2954) acc 90.6250 (90.6250) lr 1.5090e-03 eta 0:05:20
epoch [68/200] batch [2/2] time 0.196 (0.703) data 0.000 (0.507) loss 0.3259 (0.3107) acc 87.5000 (89.0625) lr 1.4955e-03 eta 0:03:05
epoch [69/200] batch [1/2] time 1.012 (1.012) data 0.793 (0.793) loss 0.2881 (0.2881) acc 93.7500 (93.7500) lr 1.4955e-03 eta 0:04:26
epoch [69/200] batch [2/2] time 0.212 (0.612) data 0.000 (0.397) loss 0.3430 (0.3156) acc 90.6250 (92.1875) lr 1.4818e-03 eta 0:02:40
epoch [70/200] batch [1/2] time 0.999 (0.999) data 0.776 (0.776) loss 0.1940 (0.1940) acc 93.7500 (93.7500) lr 1.4818e-03 eta 0:04:20
epoch [70/200] batch [2/2] time 0.203 (0.601) data 0.000 (0.388) loss 0.1632 (0.1786) acc 100.0000 (96.8750) lr 1.4679e-03 eta 0:02:36
epoch [71/200] batch [1/2] time 1.001 (1.001) data 0.800 (0.800) loss 0.2532 (0.2532) acc 93.7500 (93.7500) lr 1.4679e-03 eta 0:04:19
epoch [71/200] batch [2/2] time 0.202 (0.601) data 0.000 (0.400) loss 0.2979 (0.2755) acc 90.6250 (92.1875) lr 1.4540e-03 eta 0:02:35
epoch [72/200] batch [1/2] time 1.151 (1.151) data 0.937 (0.937) loss 0.3784 (0.3784) acc 84.3750 (84.3750) lr 1.4540e-03 eta 0:04:55
epoch [72/200] batch [2/2] time 0.201 (0.676) data 0.000 (0.468) loss 0.5215 (0.4500) acc 87.5000 (85.9375) lr 1.4399e-03 eta 0:02:53
epoch [73/200] batch [1/2] time 1.020 (1.020) data 0.800 (0.800) loss 0.2491 (0.2491) acc 93.7500 (93.7500) lr 1.4399e-03 eta 0:04:20
epoch [73/200] batch [2/2] time 0.209 (0.615) data 0.000 (0.400) loss 0.2603 (0.2547) acc 90.6250 (92.1875) lr 1.4258e-03 eta 0:02:36
epoch [74/200] batch [1/2] time 1.001 (1.001) data 0.792 (0.792) loss 0.2666 (0.2666) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:04:13
epoch [74/200] batch [2/2] time 0.209 (0.605) data 0.000 (0.396) loss 0.2788 (0.2727) acc 90.6250 (92.1875) lr 1.4115e-03 eta 0:02:32
epoch [75/200] batch [1/2] time 0.984 (0.984) data 0.774 (0.774) loss 0.3916 (0.3916) acc 81.2500 (81.2500) lr 1.4115e-03 eta 0:04:06
epoch [75/200] batch [2/2] time 0.197 (0.590) data 0.000 (0.387) loss 0.2788 (0.3352) acc 90.6250 (85.9375) lr 1.3971e-03 eta 0:02:27
epoch [76/200] batch [1/2] time 1.010 (1.010) data 0.792 (0.792) loss 0.2561 (0.2561) acc 96.8750 (96.8750) lr 1.3971e-03 eta 0:04:11
epoch [76/200] batch [2/2] time 0.210 (0.610) data 0.000 (0.396) loss 0.2617 (0.2589) acc 90.6250 (93.7500) lr 1.3827e-03 eta 0:02:31
epoch [77/200] batch [1/2] time 1.017 (1.017) data 0.800 (0.800) loss 0.3955 (0.3955) acc 87.5000 (87.5000) lr 1.3827e-03 eta 0:04:11
epoch [77/200] batch [2/2] time 0.202 (0.609) data 0.000 (0.400) loss 0.2389 (0.3172) acc 93.7500 (90.6250) lr 1.3681e-03 eta 0:02:29
epoch [78/200] batch [1/2] time 1.031 (1.031) data 0.812 (0.812) loss 0.4441 (0.4441) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:04:12
epoch [78/200] batch [2/2] time 0.220 (0.625) data 0.000 (0.406) loss 0.3789 (0.4115) acc 90.6250 (89.0625) lr 1.3535e-03 eta 0:02:32
epoch [79/200] batch [1/2] time 0.998 (0.998) data 0.823 (0.823) loss 0.2324 (0.2324) acc 96.8750 (96.8750) lr 1.3535e-03 eta 0:04:02
epoch [79/200] batch [2/2] time 0.199 (0.598) data 0.000 (0.412) loss 0.3738 (0.3031) acc 90.6250 (93.7500) lr 1.3387e-03 eta 0:02:24
epoch [80/200] batch [1/2] time 1.005 (1.005) data 0.785 (0.785) loss 0.1946 (0.1946) acc 96.8750 (96.8750) lr 1.3387e-03 eta 0:04:02
epoch [80/200] batch [2/2] time 0.210 (0.607) data 0.000 (0.393) loss 0.3154 (0.2550) acc 96.8750 (96.8750) lr 1.3239e-03 eta 0:02:25
epoch [81/200] batch [1/2] time 1.012 (1.012) data 0.787 (0.787) loss 0.3013 (0.3013) acc 96.8750 (96.8750) lr 1.3239e-03 eta 0:04:01
epoch [81/200] batch [2/2] time 0.216 (0.614) data 0.000 (0.394) loss 0.2069 (0.2541) acc 90.6250 (93.7500) lr 1.3090e-03 eta 0:02:26
epoch [82/200] batch [1/2] time 0.995 (0.995) data 0.790 (0.790) loss 0.2866 (0.2866) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:03:55
epoch [82/200] batch [2/2] time 0.197 (0.596) data 0.000 (0.395) loss 0.1998 (0.2432) acc 93.7500 (92.1875) lr 1.2940e-03 eta 0:02:20
epoch [83/200] batch [1/2] time 1.040 (1.040) data 0.825 (0.825) loss 0.3425 (0.3425) acc 96.8750 (96.8750) lr 1.2940e-03 eta 0:04:04
epoch [83/200] batch [2/2] time 0.218 (0.629) data 0.000 (0.413) loss 0.3950 (0.3688) acc 96.8750 (96.8750) lr 1.2790e-03 eta 0:02:27
epoch [84/200] batch [1/2] time 1.008 (1.008) data 0.792 (0.792) loss 0.2722 (0.2722) acc 93.7500 (93.7500) lr 1.2790e-03 eta 0:03:54
epoch [84/200] batch [2/2] time 0.203 (0.606) data 0.000 (0.396) loss 0.2498 (0.2610) acc 93.7500 (93.7500) lr 1.2639e-03 eta 0:02:20
epoch [85/200] batch [1/2] time 1.014 (1.014) data 0.803 (0.803) loss 0.3608 (0.3608) acc 84.3750 (84.3750) lr 1.2639e-03 eta 0:03:54
epoch [85/200] batch [2/2] time 0.195 (0.605) data 0.000 (0.401) loss 0.2935 (0.3271) acc 90.6250 (87.5000) lr 1.2487e-03 eta 0:02:19
epoch [86/200] batch [1/2] time 1.163 (1.163) data 0.950 (0.950) loss 0.1909 (0.1909) acc 96.8750 (96.8750) lr 1.2487e-03 eta 0:04:26
epoch [86/200] batch [2/2] time 0.198 (0.681) data 0.000 (0.475) loss 0.2102 (0.2006) acc 96.8750 (96.8750) lr 1.2334e-03 eta 0:02:35
epoch [87/200] batch [1/2] time 0.976 (0.976) data 0.789 (0.789) loss 0.2054 (0.2054) acc 93.7500 (93.7500) lr 1.2334e-03 eta 0:03:41
epoch [87/200] batch [2/2] time 0.193 (0.584) data 0.000 (0.395) loss 0.1426 (0.1740) acc 100.0000 (96.8750) lr 1.2181e-03 eta 0:02:12
epoch [88/200] batch [1/2] time 1.113 (1.113) data 0.889 (0.889) loss 0.2537 (0.2537) acc 93.7500 (93.7500) lr 1.2181e-03 eta 0:04:10
epoch [88/200] batch [2/2] time 0.216 (0.665) data 0.000 (0.445) loss 0.2354 (0.2445) acc 96.8750 (95.3125) lr 1.2028e-03 eta 0:02:28
epoch [89/200] batch [1/2] time 1.020 (1.020) data 0.808 (0.808) loss 0.3054 (0.3054) acc 93.7500 (93.7500) lr 1.2028e-03 eta 0:03:47
epoch [89/200] batch [2/2] time 0.385 (0.702) data 0.000 (0.404) loss 0.2318 (0.2686) acc 96.8750 (95.3125) lr 1.1874e-03 eta 0:02:35
epoch [90/200] batch [1/2] time 0.993 (0.993) data 0.788 (0.788) loss 0.3318 (0.3318) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:03:39
epoch [90/200] batch [2/2] time 0.208 (0.601) data 0.000 (0.394) loss 0.3123 (0.3220) acc 93.7500 (92.1875) lr 1.1719e-03 eta 0:02:12
epoch [91/200] batch [1/2] time 1.005 (1.005) data 0.791 (0.791) loss 0.2239 (0.2239) acc 100.0000 (100.0000) lr 1.1719e-03 eta 0:03:39
epoch [91/200] batch [2/2] time 0.204 (0.604) data 0.000 (0.396) loss 0.4475 (0.3357) acc 84.3750 (92.1875) lr 1.1564e-03 eta 0:02:11
epoch [92/200] batch [1/2] time 1.100 (1.100) data 0.863 (0.863) loss 0.2283 (0.2283) acc 93.7500 (93.7500) lr 1.1564e-03 eta 0:03:58
epoch [92/200] batch [2/2] time 0.202 (0.651) data 0.000 (0.431) loss 0.2000 (0.2141) acc 96.8750 (95.3125) lr 1.1409e-03 eta 0:02:20
epoch [93/200] batch [1/2] time 1.031 (1.031) data 0.798 (0.798) loss 0.2771 (0.2771) acc 93.7500 (93.7500) lr 1.1409e-03 eta 0:03:41
epoch [93/200] batch [2/2] time 0.217 (0.624) data 0.000 (0.399) loss 0.3057 (0.2914) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:02:13
epoch [94/200] batch [1/2] time 1.000 (1.000) data 0.799 (0.799) loss 0.3726 (0.3726) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:03:32
epoch [94/200] batch [2/2] time 0.207 (0.604) data 0.000 (0.400) loss 0.3010 (0.3368) acc 93.7500 (93.7500) lr 1.1097e-03 eta 0:02:07
epoch [95/200] batch [1/2] time 1.031 (1.031) data 0.820 (0.820) loss 0.3540 (0.3540) acc 96.8750 (96.8750) lr 1.1097e-03 eta 0:03:37
epoch [95/200] batch [2/2] time 0.203 (0.617) data 0.000 (0.410) loss 0.3296 (0.3418) acc 93.7500 (95.3125) lr 1.0941e-03 eta 0:02:09
epoch [96/200] batch [1/2] time 1.086 (1.086) data 0.868 (0.868) loss 0.3386 (0.3386) acc 90.6250 (90.6250) lr 1.0941e-03 eta 0:03:47
epoch [96/200] batch [2/2] time 0.185 (0.635) data 0.000 (0.434) loss 0.1725 (0.2556) acc 96.8750 (93.7500) lr 1.0785e-03 eta 0:02:12
epoch [97/200] batch [1/2] time 1.003 (1.003) data 0.821 (0.821) loss 0.4915 (0.4915) acc 84.3750 (84.3750) lr 1.0785e-03 eta 0:03:27
epoch [97/200] batch [2/2] time 0.202 (0.602) data 0.000 (0.411) loss 0.3210 (0.4062) acc 87.5000 (85.9375) lr 1.0628e-03 eta 0:02:04
epoch [98/200] batch [1/2] time 1.006 (1.006) data 0.794 (0.794) loss 0.2410 (0.2410) acc 90.6250 (90.6250) lr 1.0628e-03 eta 0:03:26
epoch [98/200] batch [2/2] time 0.208 (0.607) data 0.000 (0.397) loss 0.3040 (0.2725) acc 87.5000 (89.0625) lr 1.0471e-03 eta 0:02:03
epoch [99/200] batch [1/2] time 0.995 (0.995) data 0.788 (0.788) loss 0.3499 (0.3499) acc 84.3750 (84.3750) lr 1.0471e-03 eta 0:03:21
epoch [99/200] batch [2/2] time 0.198 (0.596) data 0.000 (0.394) loss 0.1760 (0.2629) acc 96.8750 (90.6250) lr 1.0314e-03 eta 0:02:00
epoch [100/200] batch [1/2] time 1.024 (1.024) data 0.835 (0.835) loss 0.2996 (0.2996) acc 90.6250 (90.6250) lr 1.0314e-03 eta 0:03:25
epoch [100/200] batch [2/2] time 0.212 (0.618) data 0.000 (0.418) loss 0.3423 (0.3209) acc 90.6250 (90.6250) lr 1.0157e-03 eta 0:02:03
epoch [101/200] batch [1/2] time 1.014 (1.014) data 0.788 (0.788) loss 0.2158 (0.2158) acc 93.7500 (93.7500) lr 1.0157e-03 eta 0:03:21
epoch [101/200] batch [2/2] time 0.210 (0.612) data 0.000 (0.394) loss 0.2113 (0.2136) acc 96.8750 (95.3125) lr 1.0000e-03 eta 0:02:01
epoch [102/200] batch [1/2] time 1.042 (1.042) data 0.823 (0.823) loss 0.2791 (0.2791) acc 93.7500 (93.7500) lr 1.0000e-03 eta 0:03:25
epoch [102/200] batch [2/2] time 0.205 (0.624) data 0.000 (0.412) loss 0.1228 (0.2009) acc 100.0000 (96.8750) lr 9.8429e-04 eta 0:02:02
epoch [103/200] batch [1/2] time 1.022 (1.022) data 0.814 (0.814) loss 0.1849 (0.1849) acc 100.0000 (100.0000) lr 9.8429e-04 eta 0:03:19
epoch [103/200] batch [2/2] time 0.207 (0.614) data 0.000 (0.407) loss 0.5493 (0.3671) acc 84.3750 (92.1875) lr 9.6859e-04 eta 0:01:59
epoch [104/200] batch [1/2] time 1.006 (1.006) data 0.788 (0.788) loss 0.2007 (0.2007) acc 96.8750 (96.8750) lr 9.6859e-04 eta 0:03:14
epoch [104/200] batch [2/2] time 0.203 (0.604) data 0.000 (0.394) loss 0.2510 (0.2258) acc 100.0000 (98.4375) lr 9.5289e-04 eta 0:01:56
epoch [105/200] batch [1/2] time 0.997 (0.997) data 0.791 (0.791) loss 0.2002 (0.2002) acc 93.7500 (93.7500) lr 9.5289e-04 eta 0:03:10
epoch [105/200] batch [2/2] time 0.197 (0.597) data 0.000 (0.395) loss 0.3154 (0.2578) acc 90.6250 (92.1875) lr 9.3721e-04 eta 0:01:53
epoch [106/200] batch [1/2] time 1.005 (1.005) data 0.800 (0.800) loss 0.3008 (0.3008) acc 90.6250 (90.6250) lr 9.3721e-04 eta 0:03:10
epoch [106/200] batch [2/2] time 0.199 (0.602) data 0.000 (0.400) loss 0.1809 (0.2408) acc 100.0000 (95.3125) lr 9.2154e-04 eta 0:01:53
epoch [107/200] batch [1/2] time 1.003 (1.003) data 0.788 (0.788) loss 0.2778 (0.2778) acc 93.7500 (93.7500) lr 9.2154e-04 eta 0:03:07
epoch [107/200] batch [2/2] time 0.202 (0.603) data 0.000 (0.394) loss 0.2678 (0.2728) acc 93.7500 (93.7500) lr 9.0589e-04 eta 0:01:52
epoch [108/200] batch [1/2] time 0.979 (0.979) data 0.806 (0.806) loss 0.2092 (0.2092) acc 96.8750 (96.8750) lr 9.0589e-04 eta 0:03:01
epoch [108/200] batch [2/2] time 0.194 (0.587) data 0.000 (0.403) loss 0.1666 (0.1879) acc 93.7500 (95.3125) lr 8.9027e-04 eta 0:01:47
epoch [109/200] batch [1/2] time 0.996 (0.996) data 0.795 (0.795) loss 0.4949 (0.4949) acc 81.2500 (81.2500) lr 8.9027e-04 eta 0:03:02
epoch [109/200] batch [2/2] time 0.217 (0.607) data 0.000 (0.398) loss 0.1700 (0.3325) acc 100.0000 (90.6250) lr 8.7467e-04 eta 0:01:50
epoch [110/200] batch [1/2] time 1.037 (1.037) data 0.824 (0.824) loss 0.1755 (0.1755) acc 93.7500 (93.7500) lr 8.7467e-04 eta 0:03:07
epoch [110/200] batch [2/2] time 0.424 (0.731) data 0.000 (0.412) loss 0.2578 (0.2167) acc 90.6250 (92.1875) lr 8.5910e-04 eta 0:02:11
epoch [111/200] batch [1/2] time 0.971 (0.971) data 0.771 (0.771) loss 0.2964 (0.2964) acc 87.5000 (87.5000) lr 8.5910e-04 eta 0:02:53
epoch [111/200] batch [2/2] time 0.182 (0.577) data 0.000 (0.385) loss 0.2856 (0.2910) acc 90.6250 (89.0625) lr 8.4357e-04 eta 0:01:42
epoch [112/200] batch [1/2] time 1.016 (1.016) data 0.803 (0.803) loss 0.1984 (0.1984) acc 93.7500 (93.7500) lr 8.4357e-04 eta 0:02:59
epoch [112/200] batch [2/2] time 0.201 (0.609) data 0.000 (0.402) loss 0.2546 (0.2265) acc 93.7500 (93.7500) lr 8.2807e-04 eta 0:01:47
epoch [113/200] batch [1/2] time 1.078 (1.078) data 0.866 (0.866) loss 0.2479 (0.2479) acc 96.8750 (96.8750) lr 8.2807e-04 eta 0:03:08
epoch [113/200] batch [2/2] time 0.210 (0.644) data 0.000 (0.433) loss 0.1665 (0.2072) acc 93.7500 (95.3125) lr 8.1262e-04 eta 0:01:52
epoch [114/200] batch [1/2] time 1.061 (1.061) data 0.852 (0.852) loss 0.1423 (0.1423) acc 100.0000 (100.0000) lr 8.1262e-04 eta 0:03:03
epoch [114/200] batch [2/2] time 0.203 (0.632) data 0.000 (0.426) loss 0.2118 (0.1771) acc 90.6250 (95.3125) lr 7.9721e-04 eta 0:01:48
epoch [115/200] batch [1/2] time 1.017 (1.017) data 0.823 (0.823) loss 0.3577 (0.3577) acc 87.5000 (87.5000) lr 7.9721e-04 eta 0:02:53
epoch [115/200] batch [2/2] time 0.213 (0.615) data 0.000 (0.412) loss 0.2971 (0.3274) acc 90.6250 (89.0625) lr 7.8186e-04 eta 0:01:44
epoch [116/200] batch [1/2] time 1.015 (1.015) data 0.800 (0.800) loss 0.1190 (0.1190) acc 96.8750 (96.8750) lr 7.8186e-04 eta 0:02:51
epoch [116/200] batch [2/2] time 0.208 (0.611) data 0.000 (0.400) loss 0.2091 (0.1640) acc 90.6250 (93.7500) lr 7.6655e-04 eta 0:01:42
epoch [117/200] batch [1/2] time 0.985 (0.985) data 0.775 (0.775) loss 0.1775 (0.1775) acc 96.8750 (96.8750) lr 7.6655e-04 eta 0:02:44
epoch [117/200] batch [2/2] time 0.201 (0.593) data 0.000 (0.388) loss 0.1181 (0.1478) acc 96.8750 (96.8750) lr 7.5131e-04 eta 0:01:38
epoch [118/200] batch [1/2] time 0.992 (0.992) data 0.788 (0.788) loss 0.2347 (0.2347) acc 96.8750 (96.8750) lr 7.5131e-04 eta 0:02:43
epoch [118/200] batch [2/2] time 0.201 (0.597) data 0.000 (0.394) loss 0.1624 (0.1985) acc 100.0000 (98.4375) lr 7.3613e-04 eta 0:01:37
epoch [119/200] batch [1/2] time 1.093 (1.093) data 0.909 (0.909) loss 0.2352 (0.2352) acc 96.8750 (96.8750) lr 7.3613e-04 eta 0:02:58
epoch [119/200] batch [2/2] time 0.211 (0.652) data 0.000 (0.455) loss 0.2961 (0.2657) acc 93.7500 (95.3125) lr 7.2101e-04 eta 0:01:45
epoch [120/200] batch [1/2] time 1.000 (1.000) data 0.792 (0.792) loss 0.0819 (0.0819) acc 100.0000 (100.0000) lr 7.2101e-04 eta 0:02:41
epoch [120/200] batch [2/2] time 0.200 (0.600) data 0.000 (0.396) loss 0.3027 (0.1923) acc 90.6250 (95.3125) lr 7.0596e-04 eta 0:01:36
epoch [121/200] batch [1/2] time 1.028 (1.028) data 0.827 (0.827) loss 0.3086 (0.3086) acc 93.7500 (93.7500) lr 7.0596e-04 eta 0:02:43
epoch [121/200] batch [2/2] time 0.199 (0.613) data 0.000 (0.413) loss 0.1709 (0.2397) acc 93.7500 (93.7500) lr 6.9098e-04 eta 0:01:36
epoch [122/200] batch [1/2] time 0.999 (0.999) data 0.809 (0.809) loss 0.4211 (0.4211) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:02:36
epoch [122/200] batch [2/2] time 0.200 (0.599) data 0.000 (0.405) loss 0.3218 (0.3715) acc 90.6250 (90.6250) lr 6.7608e-04 eta 0:01:33
epoch [123/200] batch [1/2] time 0.985 (0.985) data 0.781 (0.781) loss 0.2019 (0.2019) acc 100.0000 (100.0000) lr 6.7608e-04 eta 0:02:32
epoch [123/200] batch [2/2] time 0.208 (0.596) data 0.000 (0.391) loss 0.3301 (0.2660) acc 90.6250 (95.3125) lr 6.6126e-04 eta 0:01:31
epoch [124/200] batch [1/2] time 1.018 (1.018) data 0.804 (0.804) loss 0.2260 (0.2260) acc 93.7500 (93.7500) lr 6.6126e-04 eta 0:02:35
epoch [124/200] batch [2/2] time 0.194 (0.606) data 0.000 (0.402) loss 0.1462 (0.1861) acc 96.8750 (95.3125) lr 6.4653e-04 eta 0:01:32
epoch [125/200] batch [1/2] time 1.100 (1.100) data 0.877 (0.877) loss 0.2483 (0.2483) acc 93.7500 (93.7500) lr 6.4653e-04 eta 0:02:46
epoch [125/200] batch [2/2] time 0.211 (0.656) data 0.000 (0.439) loss 0.0966 (0.1724) acc 100.0000 (96.8750) lr 6.3188e-04 eta 0:01:38
epoch [126/200] batch [1/2] time 1.022 (1.022) data 0.808 (0.808) loss 0.1835 (0.1835) acc 100.0000 (100.0000) lr 6.3188e-04 eta 0:02:32
epoch [126/200] batch [2/2] time 0.197 (0.609) data 0.000 (0.404) loss 0.1912 (0.1873) acc 93.7500 (96.8750) lr 6.1732e-04 eta 0:01:30
epoch [127/200] batch [1/2] time 1.021 (1.021) data 0.812 (0.812) loss 0.1797 (0.1797) acc 96.8750 (96.8750) lr 6.1732e-04 eta 0:02:30
epoch [127/200] batch [2/2] time 0.210 (0.615) data 0.000 (0.406) loss 0.1699 (0.1748) acc 96.8750 (96.8750) lr 6.0285e-04 eta 0:01:29
epoch [128/200] batch [1/2] time 1.004 (1.004) data 0.787 (0.787) loss 0.3220 (0.3220) acc 87.5000 (87.5000) lr 6.0285e-04 eta 0:02:25
epoch [128/200] batch [2/2] time 0.198 (0.601) data 0.000 (0.393) loss 0.1047 (0.2133) acc 100.0000 (93.7500) lr 5.8849e-04 eta 0:01:26
epoch [129/200] batch [1/2] time 1.013 (1.013) data 0.790 (0.790) loss 0.2876 (0.2876) acc 90.6250 (90.6250) lr 5.8849e-04 eta 0:02:24
epoch [129/200] batch [2/2] time 0.202 (0.608) data 0.000 (0.395) loss 0.1434 (0.2155) acc 96.8750 (93.7500) lr 5.7422e-04 eta 0:01:26
epoch [130/200] batch [1/2] time 1.033 (1.033) data 0.832 (0.832) loss 0.1052 (0.1052) acc 100.0000 (100.0000) lr 5.7422e-04 eta 0:02:25
epoch [130/200] batch [2/2] time 0.198 (0.615) data 0.000 (0.416) loss 0.1123 (0.1087) acc 96.8750 (98.4375) lr 5.6006e-04 eta 0:01:26
epoch [131/200] batch [1/2] time 1.048 (1.048) data 0.834 (0.834) loss 0.1576 (0.1576) acc 96.8750 (96.8750) lr 5.6006e-04 eta 0:02:25
epoch [131/200] batch [2/2] time 0.214 (0.631) data 0.000 (0.417) loss 0.1158 (0.1367) acc 96.8750 (96.8750) lr 5.4601e-04 eta 0:01:27
epoch [132/200] batch [1/2] time 1.021 (1.021) data 0.798 (0.798) loss 0.2971 (0.2971) acc 90.6250 (90.6250) lr 5.4601e-04 eta 0:02:19
epoch [132/200] batch [2/2] time 0.211 (0.616) data 0.000 (0.399) loss 0.1475 (0.2223) acc 96.8750 (93.7500) lr 5.3207e-04 eta 0:01:23
epoch [133/200] batch [1/2] time 1.095 (1.095) data 0.892 (0.892) loss 0.1873 (0.1873) acc 100.0000 (100.0000) lr 5.3207e-04 eta 0:02:27
epoch [133/200] batch [2/2] time 0.208 (0.651) data 0.000 (0.446) loss 0.4399 (0.3136) acc 90.6250 (95.3125) lr 5.1825e-04 eta 0:01:27
epoch [134/200] batch [1/2] time 1.033 (1.033) data 0.823 (0.823) loss 0.3787 (0.3787) acc 84.3750 (84.3750) lr 5.1825e-04 eta 0:02:17
epoch [134/200] batch [2/2] time 0.209 (0.621) data 0.000 (0.411) loss 0.2346 (0.3066) acc 93.7500 (89.0625) lr 5.0454e-04 eta 0:01:21
epoch [135/200] batch [1/2] time 0.999 (0.999) data 0.792 (0.792) loss 0.1595 (0.1595) acc 100.0000 (100.0000) lr 5.0454e-04 eta 0:02:10
epoch [135/200] batch [2/2] time 0.211 (0.605) data 0.000 (0.396) loss 0.2209 (0.1902) acc 93.7500 (96.8750) lr 4.9096e-04 eta 0:01:18
epoch [136/200] batch [1/2] time 0.993 (0.993) data 0.807 (0.807) loss 0.1285 (0.1285) acc 96.8750 (96.8750) lr 4.9096e-04 eta 0:02:08
epoch [136/200] batch [2/2] time 0.197 (0.595) data 0.000 (0.404) loss 0.4094 (0.2690) acc 87.5000 (92.1875) lr 4.7750e-04 eta 0:01:16
epoch [137/200] batch [1/2] time 1.009 (1.009) data 0.793 (0.793) loss 0.1698 (0.1698) acc 93.7500 (93.7500) lr 4.7750e-04 eta 0:02:08
epoch [137/200] batch [2/2] time 0.203 (0.606) data 0.000 (0.397) loss 0.2289 (0.1993) acc 100.0000 (96.8750) lr 4.6417e-04 eta 0:01:16
epoch [138/200] batch [1/2] time 1.109 (1.109) data 0.896 (0.896) loss 0.3057 (0.3057) acc 93.7500 (93.7500) lr 4.6417e-04 eta 0:02:18
epoch [138/200] batch [2/2] time 0.207 (0.658) data 0.000 (0.448) loss 0.1323 (0.2190) acc 100.0000 (96.8750) lr 4.5098e-04 eta 0:01:21
epoch [139/200] batch [1/2] time 1.003 (1.003) data 0.787 (0.787) loss 0.5078 (0.5078) acc 81.2500 (81.2500) lr 4.5098e-04 eta 0:02:03
epoch [139/200] batch [2/2] time 0.202 (0.603) data 0.000 (0.394) loss 0.2798 (0.3938) acc 96.8750 (89.0625) lr 4.3792e-04 eta 0:01:13
epoch [140/200] batch [1/2] time 1.014 (1.014) data 0.788 (0.788) loss 0.3020 (0.3020) acc 87.5000 (87.5000) lr 4.3792e-04 eta 0:02:02
epoch [140/200] batch [2/2] time 0.212 (0.613) data 0.000 (0.394) loss 0.1720 (0.2370) acc 100.0000 (93.7500) lr 4.2499e-04 eta 0:01:13
epoch [141/200] batch [1/2] time 1.038 (1.038) data 0.834 (0.834) loss 0.1708 (0.1708) acc 96.8750 (96.8750) lr 4.2499e-04 eta 0:02:03
epoch [141/200] batch [2/2] time 0.201 (0.619) data 0.000 (0.417) loss 0.2474 (0.2091) acc 93.7500 (95.3125) lr 4.1221e-04 eta 0:01:13
epoch [142/200] batch [1/2] time 1.101 (1.101) data 0.876 (0.876) loss 0.3223 (0.3223) acc 90.6250 (90.6250) lr 4.1221e-04 eta 0:02:08
epoch [142/200] batch [2/2] time 0.205 (0.653) data 0.000 (0.438) loss 0.2209 (0.2716) acc 96.8750 (93.7500) lr 3.9958e-04 eta 0:01:15
epoch [143/200] batch [1/2] time 1.003 (1.003) data 0.790 (0.790) loss 0.2812 (0.2812) acc 96.8750 (96.8750) lr 3.9958e-04 eta 0:01:55
epoch [143/200] batch [2/2] time 0.208 (0.605) data 0.000 (0.395) loss 0.1414 (0.2113) acc 96.8750 (96.8750) lr 3.8709e-04 eta 0:01:09
epoch [144/200] batch [1/2] time 1.003 (1.003) data 0.796 (0.796) loss 0.3496 (0.3496) acc 90.6250 (90.6250) lr 3.8709e-04 eta 0:01:53
epoch [144/200] batch [2/2] time 0.215 (0.609) data 0.000 (0.398) loss 0.4050 (0.3773) acc 87.5000 (89.0625) lr 3.7476e-04 eta 0:01:08
epoch [145/200] batch [1/2] time 1.012 (1.012) data 0.804 (0.804) loss 0.1830 (0.1830) acc 96.8750 (96.8750) lr 3.7476e-04 eta 0:01:52
epoch [145/200] batch [2/2] time 0.205 (0.608) data 0.000 (0.402) loss 0.2345 (0.2087) acc 93.7500 (95.3125) lr 3.6258e-04 eta 0:01:06
epoch [146/200] batch [1/2] time 1.034 (1.034) data 0.807 (0.807) loss 0.2651 (0.2651) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:01:52
epoch [146/200] batch [2/2] time 0.200 (0.617) data 0.000 (0.404) loss 0.2299 (0.2475) acc 93.7500 (95.3125) lr 3.5055e-04 eta 0:01:06
epoch [147/200] batch [1/2] time 1.006 (1.006) data 0.797 (0.797) loss 0.2043 (0.2043) acc 96.8750 (96.8750) lr 3.5055e-04 eta 0:01:47
epoch [147/200] batch [2/2] time 0.209 (0.607) data 0.000 (0.399) loss 0.2036 (0.2040) acc 96.8750 (96.8750) lr 3.3869e-04 eta 0:01:04
epoch [148/200] batch [1/2] time 0.989 (0.989) data 0.774 (0.774) loss 0.4277 (0.4277) acc 87.5000 (87.5000) lr 3.3869e-04 eta 0:01:43
epoch [148/200] batch [2/2] time 0.197 (0.593) data 0.000 (0.387) loss 0.2520 (0.3398) acc 93.7500 (90.6250) lr 3.2699e-04 eta 0:01:01
epoch [149/200] batch [1/2] time 1.016 (1.016) data 0.788 (0.788) loss 0.2085 (0.2085) acc 96.8750 (96.8750) lr 3.2699e-04 eta 0:01:44
epoch [149/200] batch [2/2] time 0.192 (0.604) data 0.000 (0.394) loss 0.3003 (0.2544) acc 90.6250 (93.7500) lr 3.1545e-04 eta 0:01:01
epoch [150/200] batch [1/2] time 1.005 (1.005) data 0.797 (0.797) loss 0.3567 (0.3567) acc 87.5000 (87.5000) lr 3.1545e-04 eta 0:01:41
epoch [150/200] batch [2/2] time 0.201 (0.603) data 0.000 (0.399) loss 0.1793 (0.2680) acc 96.8750 (92.1875) lr 3.0409e-04 eta 0:01:00
epoch [151/200] batch [1/2] time 1.024 (1.024) data 0.797 (0.797) loss 0.1393 (0.1393) acc 100.0000 (100.0000) lr 3.0409e-04 eta 0:01:41
epoch [151/200] batch [2/2] time 0.205 (0.615) data 0.000 (0.398) loss 0.2954 (0.2173) acc 90.6250 (95.3125) lr 2.9289e-04 eta 0:01:00
epoch [152/200] batch [1/2] time 0.940 (0.940) data 0.744 (0.744) loss 0.1310 (0.1310) acc 100.0000 (100.0000) lr 2.9289e-04 eta 0:01:31
epoch [152/200] batch [2/2] time 0.200 (0.570) data 0.000 (0.372) loss 0.1273 (0.1292) acc 100.0000 (100.0000) lr 2.8187e-04 eta 0:00:54
epoch [153/200] batch [1/2] time 0.999 (0.999) data 0.787 (0.787) loss 0.1581 (0.1581) acc 96.8750 (96.8750) lr 2.8187e-04 eta 0:01:34
epoch [153/200] batch [2/2] time 0.200 (0.600) data 0.000 (0.393) loss 0.2263 (0.1922) acc 93.7500 (95.3125) lr 2.7103e-04 eta 0:00:56
epoch [154/200] batch [1/2] time 1.038 (1.038) data 0.828 (0.828) loss 0.1774 (0.1774) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:01:36
epoch [154/200] batch [2/2] time 0.204 (0.621) data 0.000 (0.414) loss 0.1520 (0.1647) acc 100.0000 (98.4375) lr 2.6037e-04 eta 0:00:57
epoch [155/200] batch [1/2] time 1.101 (1.101) data 0.890 (0.890) loss 0.1801 (0.1801) acc 93.7500 (93.7500) lr 2.6037e-04 eta 0:01:40
epoch [155/200] batch [2/2] time 0.176 (0.639) data 0.000 (0.445) loss 0.2681 (0.2241) acc 93.7500 (93.7500) lr 2.4989e-04 eta 0:00:57
epoch [156/200] batch [1/2] time 1.014 (1.014) data 0.795 (0.795) loss 0.1827 (0.1827) acc 96.8750 (96.8750) lr 2.4989e-04 eta 0:01:30
epoch [156/200] batch [2/2] time 0.213 (0.614) data 0.000 (0.397) loss 0.1012 (0.1420) acc 100.0000 (98.4375) lr 2.3959e-04 eta 0:00:54
epoch [157/200] batch [1/2] time 1.023 (1.023) data 0.811 (0.811) loss 0.1183 (0.1183) acc 100.0000 (100.0000) lr 2.3959e-04 eta 0:01:29
epoch [157/200] batch [2/2] time 0.204 (0.614) data 0.000 (0.405) loss 0.2771 (0.1977) acc 93.7500 (96.8750) lr 2.2949e-04 eta 0:00:52
epoch [158/200] batch [1/2] time 1.004 (1.004) data 0.796 (0.796) loss 0.2439 (0.2439) acc 96.8750 (96.8750) lr 2.2949e-04 eta 0:01:25
epoch [158/200] batch [2/2] time 0.203 (0.604) data 0.000 (0.398) loss 0.2463 (0.2451) acc 90.6250 (93.7500) lr 2.1957e-04 eta 0:00:50
epoch [159/200] batch [1/2] time 0.986 (0.986) data 0.779 (0.779) loss 0.1631 (0.1631) acc 96.8750 (96.8750) lr 2.1957e-04 eta 0:01:21
epoch [159/200] batch [2/2] time 0.202 (0.594) data 0.000 (0.390) loss 0.2118 (0.1874) acc 90.6250 (93.7500) lr 2.0984e-04 eta 0:00:48
epoch [160/200] batch [1/2] time 1.022 (1.022) data 0.797 (0.797) loss 0.4666 (0.4666) acc 90.6250 (90.6250) lr 2.0984e-04 eta 0:01:22
epoch [160/200] batch [2/2] time 0.207 (0.615) data 0.000 (0.399) loss 0.1887 (0.3276) acc 93.7500 (92.1875) lr 2.0032e-04 eta 0:00:49
epoch [161/200] batch [1/2] time 1.069 (1.069) data 0.868 (0.868) loss 0.1954 (0.1954) acc 93.7500 (93.7500) lr 2.0032e-04 eta 0:01:24
epoch [161/200] batch [2/2] time 0.215 (0.642) data 0.000 (0.434) loss 0.1956 (0.1955) acc 96.8750 (95.3125) lr 1.9098e-04 eta 0:00:50
epoch [162/200] batch [1/2] time 1.066 (1.066) data 0.852 (0.852) loss 0.4399 (0.4399) acc 87.5000 (87.5000) lr 1.9098e-04 eta 0:01:22
epoch [162/200] batch [2/2] time 0.215 (0.641) data 0.000 (0.426) loss 0.3982 (0.4191) acc 87.5000 (87.5000) lr 1.8185e-04 eta 0:00:48
epoch [163/200] batch [1/2] time 1.073 (1.073) data 0.836 (0.836) loss 0.2356 (0.2356) acc 93.7500 (93.7500) lr 1.8185e-04 eta 0:01:20
epoch [163/200] batch [2/2] time 0.215 (0.644) data 0.000 (0.418) loss 0.1241 (0.1798) acc 100.0000 (96.8750) lr 1.7292e-04 eta 0:00:47
epoch [164/200] batch [1/2] time 1.061 (1.061) data 0.874 (0.874) loss 0.0945 (0.0945) acc 96.8750 (96.8750) lr 1.7292e-04 eta 0:01:17
epoch [164/200] batch [2/2] time 0.199 (0.630) data 0.000 (0.437) loss 0.1888 (0.1417) acc 93.7500 (95.3125) lr 1.6419e-04 eta 0:00:45
epoch [165/200] batch [1/2] time 1.145 (1.145) data 0.913 (0.913) loss 0.3025 (0.3025) acc 93.7500 (93.7500) lr 1.6419e-04 eta 0:01:21
epoch [165/200] batch [2/2] time 0.211 (0.678) data 0.000 (0.457) loss 0.2288 (0.2656) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:00:47
epoch [166/200] batch [1/2] time 1.139 (1.139) data 0.899 (0.899) loss 0.1421 (0.1421) acc 100.0000 (100.0000) lr 1.5567e-04 eta 0:01:18
epoch [166/200] batch [2/2] time 0.197 (0.668) data 0.000 (0.449) loss 0.3306 (0.2363) acc 93.7500 (96.8750) lr 1.4736e-04 eta 0:00:45
epoch [167/200] batch [1/2] time 1.055 (1.055) data 0.824 (0.824) loss 0.1274 (0.1274) acc 96.8750 (96.8750) lr 1.4736e-04 eta 0:01:10
epoch [167/200] batch [2/2] time 0.219 (0.637) data 0.000 (0.412) loss 0.2300 (0.1787) acc 90.6250 (93.7500) lr 1.3926e-04 eta 0:00:42
epoch [168/200] batch [1/2] time 1.049 (1.049) data 0.845 (0.845) loss 0.1277 (0.1277) acc 96.8750 (96.8750) lr 1.3926e-04 eta 0:01:08
epoch [168/200] batch [2/2] time 0.181 (0.615) data 0.000 (0.423) loss 0.1589 (0.1433) acc 100.0000 (98.4375) lr 1.3137e-04 eta 0:00:39
epoch [169/200] batch [1/2] time 1.085 (1.085) data 0.857 (0.857) loss 0.1565 (0.1565) acc 96.8750 (96.8750) lr 1.3137e-04 eta 0:01:08
epoch [169/200] batch [2/2] time 0.212 (0.649) data 0.000 (0.429) loss 0.2515 (0.2040) acc 96.8750 (96.8750) lr 1.2369e-04 eta 0:00:40
epoch [170/200] batch [1/2] time 1.009 (1.009) data 0.791 (0.791) loss 0.3567 (0.3567) acc 87.5000 (87.5000) lr 1.2369e-04 eta 0:01:01
epoch [170/200] batch [2/2] time 0.209 (0.609) data 0.000 (0.396) loss 0.2842 (0.3204) acc 90.6250 (89.0625) lr 1.1623e-04 eta 0:00:36
epoch [171/200] batch [1/2] time 1.001 (1.001) data 0.788 (0.788) loss 0.3423 (0.3423) acc 87.5000 (87.5000) lr 1.1623e-04 eta 0:00:59
epoch [171/200] batch [2/2] time 0.203 (0.602) data 0.000 (0.394) loss 0.1572 (0.2498) acc 100.0000 (93.7500) lr 1.0899e-04 eta 0:00:34
epoch [172/200] batch [1/2] time 0.963 (0.963) data 0.758 (0.758) loss 0.1597 (0.1597) acc 96.8750 (96.8750) lr 1.0899e-04 eta 0:00:54
epoch [172/200] batch [2/2] time 0.203 (0.583) data 0.000 (0.379) loss 0.0800 (0.1198) acc 100.0000 (98.4375) lr 1.0197e-04 eta 0:00:32
epoch [173/200] batch [1/2] time 1.183 (1.183) data 0.951 (0.951) loss 0.1924 (0.1924) acc 96.8750 (96.8750) lr 1.0197e-04 eta 0:01:05
epoch [173/200] batch [2/2] time 0.218 (0.700) data 0.000 (0.476) loss 0.1749 (0.1837) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:00:37
epoch [174/200] batch [1/2] time 1.519 (1.519) data 1.153 (1.153) loss 0.1844 (0.1844) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:01:20
epoch [174/200] batch [2/2] time 0.196 (0.857) data 0.000 (0.576) loss 0.2018 (0.1931) acc 93.7500 (93.7500) lr 8.8597e-05 eta 0:00:44
epoch [175/200] batch [1/2] time 1.010 (1.010) data 0.829 (0.829) loss 0.2399 (0.2399) acc 96.8750 (96.8750) lr 8.8597e-05 eta 0:00:51
epoch [175/200] batch [2/2] time 0.206 (0.608) data 0.000 (0.415) loss 0.1986 (0.2192) acc 96.8750 (96.8750) lr 8.2245e-05 eta 0:00:30
epoch [176/200] batch [1/2] time 1.043 (1.043) data 0.809 (0.809) loss 0.2512 (0.2512) acc 90.6250 (90.6250) lr 8.2245e-05 eta 0:00:51
epoch [176/200] batch [2/2] time 0.209 (0.626) data 0.000 (0.404) loss 0.2734 (0.2623) acc 90.6250 (90.6250) lr 7.6120e-05 eta 0:00:30
epoch [177/200] batch [1/2] time 1.001 (1.001) data 0.788 (0.788) loss 0.1785 (0.1785) acc 93.7500 (93.7500) lr 7.6120e-05 eta 0:00:47
epoch [177/200] batch [2/2] time 0.208 (0.605) data 0.000 (0.394) loss 0.2866 (0.2325) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:00:27
epoch [178/200] batch [1/2] time 1.017 (1.017) data 0.811 (0.811) loss 0.0899 (0.0899) acc 100.0000 (100.0000) lr 7.0224e-05 eta 0:00:45
epoch [178/200] batch [2/2] time 0.205 (0.611) data 0.000 (0.405) loss 0.2158 (0.1529) acc 96.8750 (98.4375) lr 6.4556e-05 eta 0:00:26
epoch [179/200] batch [1/2] time 0.986 (0.986) data 0.813 (0.813) loss 0.1688 (0.1688) acc 93.7500 (93.7500) lr 6.4556e-05 eta 0:00:42
epoch [179/200] batch [2/2] time 0.203 (0.595) data 0.000 (0.407) loss 0.2255 (0.1971) acc 93.7500 (93.7500) lr 5.9119e-05 eta 0:00:24
epoch [180/200] batch [1/2] time 1.014 (1.014) data 0.800 (0.800) loss 0.1923 (0.1923) acc 96.8750 (96.8750) lr 5.9119e-05 eta 0:00:41
epoch [180/200] batch [2/2] time 0.198 (0.606) data 0.000 (0.400) loss 0.1075 (0.1499) acc 100.0000 (98.4375) lr 5.3915e-05 eta 0:00:24
epoch [181/200] batch [1/2] time 1.008 (1.008) data 0.794 (0.794) loss 0.2201 (0.2201) acc 96.8750 (96.8750) lr 5.3915e-05 eta 0:00:39
epoch [181/200] batch [2/2] time 0.213 (0.610) data 0.000 (0.397) loss 0.0885 (0.1543) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:23
epoch [182/200] batch [1/2] time 1.022 (1.022) data 0.807 (0.807) loss 0.1683 (0.1683) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:37
epoch [182/200] batch [2/2] time 0.198 (0.610) data 0.000 (0.404) loss 0.2076 (0.1880) acc 93.7500 (93.7500) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [1/2] time 1.015 (1.015) data 0.787 (0.787) loss 0.1929 (0.1929) acc 96.8750 (96.8750) lr 4.4207e-05 eta 0:00:35
epoch [183/200] batch [2/2] time 0.220 (0.617) data 0.000 (0.394) loss 0.4131 (0.3030) acc 93.7500 (95.3125) lr 3.9706e-05 eta 0:00:20
epoch [184/200] batch [1/2] time 1.038 (1.038) data 0.821 (0.821) loss 0.1686 (0.1686) acc 96.8750 (96.8750) lr 3.9706e-05 eta 0:00:34
epoch [184/200] batch [2/2] time 0.200 (0.619) data 0.000 (0.411) loss 0.1870 (0.1778) acc 96.8750 (96.8750) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [1/2] time 1.001 (1.001) data 0.796 (0.796) loss 0.2598 (0.2598) acc 96.8750 (96.8750) lr 3.5443e-05 eta 0:00:31
epoch [185/200] batch [2/2] time 0.202 (0.602) data 0.000 (0.398) loss 0.1536 (0.2067) acc 90.6250 (93.7500) lr 3.1417e-05 eta 0:00:18
epoch [186/200] batch [1/2] time 1.025 (1.025) data 0.818 (0.818) loss 0.2491 (0.2491) acc 90.6250 (90.6250) lr 3.1417e-05 eta 0:00:29
epoch [186/200] batch [2/2] time 0.211 (0.618) data 0.000 (0.409) loss 0.1945 (0.2218) acc 93.7500 (92.1875) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [1/2] time 1.058 (1.058) data 0.849 (0.849) loss 0.1926 (0.1926) acc 93.7500 (93.7500) lr 2.7630e-05 eta 0:00:28
epoch [187/200] batch [2/2] time 0.213 (0.635) data 0.000 (0.425) loss 0.1637 (0.1782) acc 93.7500 (93.7500) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [1/2] time 0.988 (0.988) data 0.797 (0.797) loss 0.2551 (0.2551) acc 90.6250 (90.6250) lr 2.4083e-05 eta 0:00:24
epoch [188/200] batch [2/2] time 0.212 (0.600) data 0.000 (0.399) loss 0.2524 (0.2538) acc 93.7500 (92.1875) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [1/2] time 0.985 (0.985) data 0.779 (0.779) loss 0.2573 (0.2573) acc 93.7500 (93.7500) lr 2.0777e-05 eta 0:00:22
epoch [189/200] batch [2/2] time 0.207 (0.596) data 0.000 (0.390) loss 0.3584 (0.3079) acc 90.6250 (92.1875) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [1/2] time 0.980 (0.980) data 0.806 (0.806) loss 0.2446 (0.2446) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [2/2] time 0.196 (0.588) data 0.000 (0.403) loss 0.1580 (0.2013) acc 93.7500 (92.1875) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [1/2] time 1.078 (1.078) data 0.873 (0.873) loss 0.1427 (0.1427) acc 100.0000 (100.0000) lr 1.4891e-05 eta 0:00:20
epoch [191/200] batch [2/2] time 0.212 (0.645) data 0.000 (0.437) loss 0.1481 (0.1454) acc 93.7500 (96.8750) lr 1.2312e-05 eta 0:00:11
epoch [192/200] batch [1/2] time 1.007 (1.007) data 0.803 (0.803) loss 0.2264 (0.2264) acc 96.8750 (96.8750) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [2/2] time 0.206 (0.606) data 0.000 (0.402) loss 0.1281 (0.1772) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [1/2] time 0.993 (0.993) data 0.783 (0.783) loss 0.2336 (0.2336) acc 96.8750 (96.8750) lr 9.9763e-06 eta 0:00:14
epoch [193/200] batch [2/2] time 0.199 (0.596) data 0.000 (0.391) loss 0.1752 (0.2044) acc 96.8750 (96.8750) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [1/2] time 1.006 (1.006) data 0.787 (0.787) loss 0.1733 (0.1733) acc 96.8750 (96.8750) lr 7.8853e-06 eta 0:00:13
epoch [194/200] batch [2/2] time 0.200 (0.603) data 0.000 (0.394) loss 0.1044 (0.1389) acc 100.0000 (98.4375) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [1/2] time 1.000 (1.000) data 0.782 (0.782) loss 0.1532 (0.1532) acc 96.8750 (96.8750) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [2/2] time 0.202 (0.601) data 0.000 (0.391) loss 0.3035 (0.2283) acc 87.5000 (92.1875) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [1/2] time 0.996 (0.996) data 0.781 (0.781) loss 0.1321 (0.1321) acc 96.8750 (96.8750) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [2/2] time 0.207 (0.601) data 0.000 (0.390) loss 0.2456 (0.1888) acc 96.8750 (96.8750) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [1/2] time 1.062 (1.062) data 0.839 (0.839) loss 0.1704 (0.1704) acc 93.7500 (93.7500) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [2/2] time 0.225 (0.644) data 0.000 (0.419) loss 0.3738 (0.2721) acc 90.6250 (92.1875) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [1/2] time 1.001 (1.001) data 0.792 (0.792) loss 0.2410 (0.2410) acc 90.6250 (90.6250) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [2/2] time 0.207 (0.604) data 0.000 (0.396) loss 0.2168 (0.2289) acc 96.8750 (93.7500) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [1/2] time 1.026 (1.026) data 0.811 (0.811) loss 0.3818 (0.3818) acc 84.3750 (84.3750) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [2/2] time 0.211 (0.618) data 0.000 (0.405) loss 0.0988 (0.2403) acc 100.0000 (92.1875) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [1/2] time 1.003 (1.003) data 0.799 (0.799) loss 0.1968 (0.1968) acc 93.7500 (93.7500) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [2/2] time 0.211 (0.607) data 0.000 (0.400) loss 0.0718 (0.1343) acc 100.0000 (96.8750) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/eurosat/ALVLM/vit_b16_-1shots/nctx16_cscTrue_ctpend_alentropy_modenone/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,100
* correct: 6,949
* accuracy: 85.8%
* error: 14.2%
* macro_f1: 85.6%
training time for 7-th round: 354.06 seconds
=== Result Overview ===
0: 47.69135802469136
1: 66.58024691358025
2: 73.14814814814815
3: 80.98765432098766
4: 78.03703703703704
5: 82.48148148148148
6: 86.09876543209876
7: 85.79012345679013
=======================
